<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在Centos7上安装Kubernetes]]></title>
    <url>%2Fk8s%2Fk8s-install-on-centos7-20180609.html</url>
    <content type="text"><![CDATA[使用KubeAdm官方工具在Centos7上安装kubernetes. 安装环境OS: Centos7K8s Version: v1.10.4 环境配置更新系统1$ yum update -y 时区1$ timedatectl set-timezone Asia/Shanghai 配置Hosts1234$ cat /etc/hosts10.0.100.2 k8smaster10.0.100.3 k8snode0110.0.100.4 k8snode02 关闭防火墙123$ systemctl disable firewalld$ systemctl stop firewalld$ systemctl status firewalld 关闭内存交换分区12345$ swapoff -a# 永久$ cat /etc/fstab#/dev/mapper/centos-swap swap swap defaults 0 0 禁用SELINUX123$ setenforce 0$ cat /etc/selinux/configSELINUX=disabled 配置网络1234567$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF$ modprobe br_netfilter$ sysctl --system 安装Docker1$ yum install -y docker 配置cgroupdriver为cgroupfs12345$ sed -i "s/native.cgroupdriver=systemd/native.cgroupdriver=cgroupfs/g" /usr/lib/systemd/system/docker.service$ cat /usr/lib/systemd/system/docker.service$ systemctl enable docker &amp;&amp; systemctl start docker$ docker info | grep Cgroup 安装kubeadm kubelet kubectl123456789101112$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF$ yum install -y kubelet kubeadm kubectl$ systemctl enable kubelet &amp;&amp; systemctl start kubelet Note: kubelet在这里启动时不会成功的，它会定时检查重启。后面程序安装后，kubectl会启动成功。 配置kubeadm的cgroup driver为Cgroupfs12345$ sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs --runtime-cgroups=\/systemd\/system.slice --kubelet-cgroups=\/systemd\/system.slice/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf$ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf$ systemctl daemon-reload$ systemctl restart kubelet 查看kubelet日志1$ journalctl -u kubelet -f 下载镜像提前下载Dokcer镜像，如果你机器有科学上网请忽略这一步。我提前将Docker镜像下载到国内私有hub中。下面是下载脚本1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bashARCH=amd64mversion=v1.10.4nversion=1.14.8username=luojipassword=passwordrepo=k8s.gcr.iostore_repo=r.xiaozhou.netstore_repo_path=r.xiaozhou.net/kubernetesimages=($&#123;repo&#125;/kube-apiserver-$&#123;ARCH&#125;:$&#123;mversion&#125; \ $&#123;repo&#125;/kube-controller-manager-$&#123;ARCH&#125;:$&#123;mversion&#125; \ $&#123;repo&#125;/kube-scheduler-$&#123;ARCH&#125;:$&#123;mversion&#125; \ $&#123;repo&#125;/kube-proxy-$&#123;ARCH&#125;:$&#123;mversion&#125; \ $&#123;repo&#125;/etcd-$&#123;ARCH&#125;:3.1.12 \ $&#123;repo&#125;/pause-$&#123;ARCH&#125;:3.1 \ $&#123;repo&#125;/k8s-dns-sidecar-$&#123;ARCH&#125;:$&#123;nversion&#125; \ $&#123;repo&#125;/k8s-dns-kube-dns-$&#123;ARCH&#125;:$&#123;nversion&#125; \ $&#123;repo&#125;/k8s-dns-dnsmasq-nanny-$&#123;ARCH&#125;:$&#123;nversion&#125; \ $&#123;repo&#125;/kubernetes-dashboard-$&#123;ARCH&#125;:v1.8.3 \ quay.io/coreos/flannel:v0.10.0-amd64 \ )docker login -u $username -p $password $store_repofor url in $&#123;images[@]&#125;do sub=$&#123;url%/*&#125; idx=$&#123;#sub&#125; image=$&#123;url:$idx+1&#125; echo -e "download -&gt; $store_repo_path/$image -&gt; $url" docker pull $store_repo_path/$image docker tag $store_repo_path/$image $url docker rmi $store_repo_path/$imagedoneunset ARCH mversion nversion images username password repo store_repo store_repo_path 上传脚本可以在这里找到：https://github.com/gunsluo/k8s-example/tree/master/version 初始化主节点$ kubeadm init –kubernetes-version v1.10.4 –service-cidr 10.96.0.0/16 –pod-network-cidr 10.244.0.0/16 –apiserver-advertise-address 10.0.100.2$ kubeadm init –config kubeadm.yaml –service-cidr 10.96.0.0/16 –pod-network-cidr 10.244.0.0/16$ cat kubeadm.yamlapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.4api: advertiseAddress: ‘10.0.100.2’etcd: extraArgs: ‘listen-peer-urls’: ‘http://127.0.0.1:2380‘ 123$ mkdir -p $HOME/.kube$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config 主节点初始化网络1$ curl -O https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml flanneld启动参数加上–iface=123456command:- /opt/bin/flanneldargs:- --ip-masq- --kube-subnet-mgr- --iface=eth1 1234$ kubectl apply -f kube-flannel.yml$ kubectl get nodes$ kubectl get pods --all-namespaces 加入工作节点1$ kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 安装Dashboard12$ curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml$ kubectl create -f kubernetes-dashboard.yaml $ kubectl apply -f kubernetes-dashboard-admin.yaml1234567891011121314apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 配置代理 将Master节点的/etc/kubernetes/admin.conf拷贝到本机的$HOME/.kube目录下，然后运行kubectl proxy。最后在浏览器中输入：http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 现在可以登录dashboard 配置远程访问dashboard，上面方法需要在本机启动代理。我们可以直接使用IP进行访问，这需要修改kubernetes-dashboard-admin.yaml配置。123456789101112131415kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 31000 selector: k8s-app: kubernetes-dashboard 指定NodePort类型和对外提供端口31000。 使用命令kubectl replace -f kubernetes-dashboard.yaml --force重启服务。浏览器中输入: https://ip:31000/ 即可访问。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
        <tag>centos</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Dnsmasq在Mac上搭建私有网络开发环境]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fdnsmasq-install-for-mac-20180605.html</url>
    <content type="text"><![CDATA[对于经常移动办公的童鞋，网络环境也随之改变，开发过程中部署的本地环境也需要重新配置IP地址等，或者虚拟机（使用桥接方式，桥接好处在于局域网内其它电脑方便访问，虚拟机系统访问外网不需要额外配置）时系统IP改变。我在这样的环境下痛苦针扎了一段时间，决定在本机组建一个私有网络不随连接Internet网络的改变而变动。 先说说我的想法：在电脑上模拟新的一张网卡，用此网卡组建一个网络，此网卡IP使用静态配置。如果想在同一局域网中共享该私有网络要使用dnsmasq，只是本机使用可以不使用dnsmasq。我使用的Mac电脑，由于mac不支持创建虚拟网卡，我只能使用虚拟IP来实现。 环境Mac: 宿主机 (通常使用wifi的网卡)CentOS7 1: 虚拟机(VMware Fusion) (双网卡 桥接模式)CentOS7 2: 虚拟机(VMware Fusion) (双网卡 桥接模式)CentOS7 3: 虚拟机(VMware Fusion) (双网卡 桥接模式) 配置宿主机网络由于Mac不支持创建虚拟网卡，使用虚拟IP。给en0网卡添加虚拟IP12sudo ifconfig en0 alias 10.0.100.1sudo ifconfig en0 -alias 10.0.100.1 linux系统命令：12ifconfig eth0:1 10.0.100.2 netmask 255.255.255.0ip addr del 10.0.100.2/24 dev eth0 配置虚拟机网络12cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth1uuidgen eth1 配置eth1网卡 vi /etc/sysconfig/network-scripts/ifcfg-eth1 (uuid使用uuidgen命令生成)123456789101112131415161718TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=eth1UUID=123a9032-c938-4404-bc95-7103accbdcc3DEVICE=eth1IPADDR=10.0.100.2NETMASK=255.255.255.0GATEWAY=10.0.100.1ONBOOT=yes 其他虚拟机的IP10.0.100.3 10.0.100.4 测试网络宿主机: ping 10.0.100.2虚拟机: ping 10.0.100.1 安装dnsmasq1brew install dnsmasq 配置 vim /usr/local/etc/dnsmasq.conf123strict-orderlisten-address=10.0.100.1,127.0.0.1conf-dir=/usr/local/etc/dnsmasq.d/,*.conf 添加/usr/local/etc/dnsmasq.d/k8s.conf123address=/k8s-master.lan/10.0.100.2address=/k8s-node1.lan/10.0.100.3address=/k8s-node2.lan/10.0.100.4 启动:1sudo brew services start dnsmasq 开机启动1sudo launchctl start homebrew.mxcl.dnsmasq 清除DNS缓存1sudo killall -HUP mDNSResponder 如果使用该dns服务，修改dnsvi /etc/rsyslog.conf1nameserver 10.0.100.1 测试宿主机中修改/etc/rsyslog.confping k8s-master.lan]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>dns</tag>
        <tag>dnsmasq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jitsi开源Web视频会议-安全认证]]></title>
    <url>%2Fvideo%2Fjitsi-auth-20180507.html</url>
    <content type="text"><![CDATA[Jitsi安装教程、之前安装Jitsi后所有人都能访问，当实际情况是需要认证后才能访问。通过配置jitsi提供了认证方式。 Prosody认证配置配置/etc/prosody/conf.d/meet.demo.com.cfg.lua12345678910111213141516171819202122232425262728-- Plugins path gets uncommented during jitsi-meet-tokens package install - that's where token plugin is located--plugin_paths = &#123; "/usr/share/jitsi-meet/prosody-plugins/" &#125;VirtualHost "meet.demo.com" authentication = "internal_plain" ssl = &#123; key = "/var/lib/prosody/meet.demo.com.key"; certificate = "/var/lib/prosody/meet.demo.com.crt"; &#125; modules_enabled = &#123; "bosh"; "pubsub"; &#125;VirtualHost "guest.meet.demo.com" authentication = "anonymous" c2s_require_encryption = falseadmins = &#123; "focus@auth.meet.demo.com" &#125;Component "conference.meet.demo.com" "muc"Component "jitsi-videobridge.meet.demo.com" component_secret = "password1"Component "focus.meet.demo.com" component_secret = "password2"Component "callcontrol.meet.demo.com" component_secret = "password4" Nginx认证配置配置/etc/nginx/sites-enabled/meet.demo.com1234567891011121314151617181920212223242526272829303132333435server &#123; listen 80; server_name meet.demo.com; return 301 https://$host$request_uri;&#125;server &#123; listen 443 ssl; server_name meet.demo.com; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers "EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA256:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EDH+aRSA+AESGCM:EDH+aRSA+SHA256:EDH+aRSA:EECDH:!aNULL:!eNULL:!MEDIUM:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!RC4:!SEED"; ssl_certificate /var/lib/prosody/meet.demo.com.crt; ssl_certificate_key /var/lib/prosody/meet.demo.com.key; root /var/www/jitsi-meet; index index.html index.htm; # error_page 404 /static/404.html; location ~ ^/([a-zA-Z0-9=\?]+)$ &#123; rewrite ^/(.*)$ / break; &#125; location / &#123; ssi on; &#125; # BOSH location /http-bind &#123; proxy_pass http://localhost:5280/http-bind; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; &#125;&#125; videobridge认证配置参数start.sh1234567#!/bin/bashexport JAVA_SYS_PROPS="-Dnet.java.sip.communicator.SC_HOME_DIR_LOCATION=/etc/jitsi -Dnet.java.sip.communicator.SC_HOME_DIR_NAME=videobridge -Dnet.java.sip.communicator.SC_LOG_DIR_LOCATION=/var/log/jitsi/videobridge -Djava.util.logging.config.file=/etc/jitsi/videobridge/logging.properties -Dlog4j.configurationFile=/etc/jitsi/videobridge/log4j2.xml"./jvb.sh --host=localhost --domain=meet.demo.com --port=5347 --secret=password1 &amp;unset JAVA_SYS_PROPS Jicofo认证配置参数start.sh12345678910#!/bin/bash# mvn package -DskipTests -Dassembly.skipAssembly=falseexport JAVA_SYS_PROPS="-Dnet.java.sip.communicator.SC_HOME_DIR_LOCATION=/etc/jitsi -Dnet.java.sip.communicator.SC_HOME_DIR_NAME=jicofo -Dnet.java.sip.communicator.SC_LOG_DIR_LOCATION=/var/log/jitsi/jicofo -Djava.util.logging.config.file=/etc/jitsi/jicofo/logging.properties -Dlog4j.configurationFile=/etc/jitsi/jicofo/log4j2.xml -Dorg.jitsi.jicofo.ALWAYS_TRUST_MODE_ENABLED=true"./jicofo-linux-x64-1.1-SNAPSHOT/jicofo.sh --host=localhost --domain=meet.demo.com --secret=password2 --user_domain=auth.meet.demo.com --user_name=focus --user_password=password3 &amp;unset JAVA_SYS_PROPS Jitsi-meet认证配置config.js配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390var config = &#123; // Configuration // // Alternative location for the configuration. // configLocation: './config.json', // Custom function which given the URL path should return a room name. // getroomnode: function (path) &#123; return 'someprefixpossiblybasedonpath'; &#125;, // Connection // hosts: &#123; // XMPP domain. domain: 'meet.demo.com', // XMPP MUC domain. FIXME: use XEP-0030 to discover it. muc: 'conference.meet.demo.com', // When using authentication, domain for guest users. // anonymousdomain: 'guest.example.com', // Domain for authenticated users. Defaults to &lt;domain&gt;. // authdomain: 'meet.demo.com', // Jirecon recording component domain. // jirecon: 'jirecon.meet.demo.com', // Call control component (Jigasi). call_control: 'callcontrol.meet.demo.com', // Focus component domain. Defaults to focus.&lt;domain&gt;. focus: 'focus.meet.demo.com', bridge: 'jitsi-videobridge.meet.demo.com' &#125;, // BOSH URL. FIXME: use XEP-0156 to discover it. bosh: '//meet.demo.com/http-bind', // The name of client node advertised in XEP-0115 'c' stanza clientNode: 'http://jitsi.org/jitsimeet', // The real JID of focus participant - can be overridden here // focusUserJid: 'focus@auth.meet.demo.com', // Testing / experimental features. // testing: &#123; // Enables experimental simulcast support on Firefox. enableFirefoxSimulcast: false, // P2P test mode disables automatic switching to P2P when there are 2 // participants in the conference. p2pTestMode: false // Enables the test specific features consumed by jitsi-meet-torture // testMode: false &#125;, // Disables ICE/UDP by filtering out local and remote UDP candidates in // signalling. // webrtcIceUdpDisable: false, // Disables ICE/TCP by filtering out local and remote TCP candidates in // signalling. // webrtcIceTcpDisable: false, // Media // // Audio // Disable measuring of audio levels. // disableAudioLevels: false, // Start the conference in audio only mode (no video is being received nor // sent). // startAudioOnly: false, // Every participant after the Nth will start audio muted. // startAudioMuted: 10, // Start calls with audio muted. Unlike the option above, this one is only // applied locally. FIXME: having these 2 options is confusing. // startWithAudioMuted: false, // Video // Sets the preferred resolution (height) for local video. Defaults to 720. // resolution: 720, // w3c spec-compliant video constraints to use for video capture. Currently // used by browsers that return true from lib-jitsi-meet's // util#browser#usesNewGumFlow. The constraints are independency from // this config's resolution value. Defaults to requesting an ideal aspect // ratio of 16:9 with an ideal resolution of 1080p. // constraints: &#123; // video: &#123; // aspectRatio: 16 / 9, // height: &#123; // ideal: 1080, // max: 1080, // min: 240 // &#125; // &#125; // &#125;, // Enable / disable simulcast support. // disableSimulcast: false, // Suspend sending video if bandwidth estimation is too low. This may cause // problems with audio playback. Disabled until these are fixed. disableSuspendVideo: true, // Every participant after the Nth will start video muted. // startVideoMuted: 10, // Start calls with video muted. Unlike the option above, this one is only // applied locally. FIXME: having these 2 options is confusing. // startWithVideoMuted: false, // If set to true, prefer to use the H.264 video codec (if supported). // Note that it's not recommended to do this because simulcast is not // supported when using H.264. For 1-to-1 calls this setting is enabled by // default and can be toggled in the p2p section. // preferH264: true, // If set to true, disable H.264 video codec by stripping it out of the // SDP. // disableH264: false, // Desktop sharing // Enable / disable desktop sharing // disableDesktopSharing: false, // The ID of the jidesha extension for Chrome. desktopSharingChromeExtId: null, // Whether desktop sharing should be disabled on Chrome. desktopSharingChromeDisabled: true, // The media sources to use when using screen sharing with the Chrome // extension. desktopSharingChromeSources: [ 'screen', 'window', 'tab' ], // Required version of Chrome extension desktopSharingChromeMinExtVersion: '0.1', // Whether desktop sharing should be disabled on Firefox. desktopSharingFirefoxDisabled: false, // Optional desktop sharing frame rate options. Default value: min:5, max:5. // desktopSharingFrameRate: &#123; // min: 5, // max: 5 // &#125;, // Try to start calls with screen-sharing instead of camera video. // startScreenSharing: false, // Recording // Whether to enable recording or not. // enableRecording: false, // Type for recording: one of jibri or jirecon. // recordingType: 'jibri', // Misc // Default value for the channel "last N" attribute. -1 for unlimited. channelLastN: -1, // Disables or enables RTX (RFC 4588) (defaults to false). // disableRtx: false, // Disables or enables TCC (the default is in Jicofo and set to true) // (draft-holmer-rmcat-transport-wide-cc-extensions-01). This setting // affects congestion control, it practically enables send-side bandwidth // estimations. // enableTcc: true, // Disables or enables REMB (the default is in Jicofo and set to false) // (draft-alvestrand-rmcat-remb-03). This setting affects congestion // control, it practically enables recv-side bandwidth estimations. When // both TCC and REMB are enabled, TCC takes precedence. When both are // disabled, then bandwidth estimations are disabled. // enableRemb: false, // Defines the minimum number of participants to start a call (the default // is set in Jicofo and set to 2). // minParticipants: 2, // Use XEP-0215 to fetch STUN and TURN servers. // useStunTurn: true, // Enable IPv6 support. // useIPv6: true, // Enables / disables a data communication channel with the Videobridge. // Values can be 'datachannel', 'websocket', true (treat it as // 'datachannel'), undefined (treat it as 'datachannel') and false (don't // open any channel). // openBridgeChannel: true, // UI // // Use display name as XMPP nickname. useNicks: false, // Require users to always specify a display name. // requireDisplayName: true, // Whether to use a welcome page or not. In case it's false a random room // will be joined when no room is specified. enableWelcomePage: true, // Enabling the close page will ignore the welcome page redirection when // a call is hangup. // enableClosePage: false, // Disable hiding of remote thumbnails when in a 1-on-1 conference call. // disable1On1Mode: false, // The minimum value a video's height (or width, whichever is smaller) needs // to be in order to be considered high-definition. minHDHeight: 540, // Default language for the user interface. // defaultLanguage: 'en', // If true all users without a token will be considered guests and all users // with token will be considered non-guests. Only guests will be allowed to // edit their profile. enableUserRolesBasedOnToken: false, // Message to show the users. Example: 'The service will be down for // maintenance at 01:00 AM GMT, // noticeMessage: '', // Stats // // Whether to enable stats collection or not in the TraceablePeerConnection. // This can be useful for debugging purposes (post-processing/analysis of // the webrtc stats) as it is done in the jitsi-meet-torture bandwidth // estimation tests. // gatherStats: false, // To enable sending statistics to callstats.io you must provide the // Application ID and Secret. // callStatsID: '', // callStatsSecret: '', // enables callstatsUsername to be reported as statsId and used // by callstats as repoted remote id // enableStatsID: false // enables sending participants display name to callstats // enableDisplayNameInStats: false // Privacy // // If third party requests are disabled, no other server will be contacted. // This means avatars will be locally generated and callstats integration // will not function. // disableThirdPartyRequests: false, // Peer-To-Peer mode: used (if enabled) when there are just 2 participants. // p2p: &#123; // Enables peer to peer mode. When enabled the system will try to // establish a direct connection when there are exactly 2 participants // in the room. If that succeeds the conference will stop sending data // through the JVB and use the peer to peer connection instead. When a // 3rd participant joins the conference will be moved back to the JVB // connection. enabled: true, // Use XEP-0215 to fetch STUN and TURN servers. // useStunTurn: true, // The STUN servers that will be used in the peer to peer connections stunServers: [ &#123; urls: 'stun:stun.l.google.com:19302' &#125;, &#123; urls: 'stun:stun1.l.google.com:19302' &#125;, &#123; urls: 'stun:stun2.l.google.com:19302' &#125; ], // Sets the ICE transport policy for the p2p connection. At the time // of this writing the list of possible values are 'all' and 'relay', // but that is subject to change in the future. The enum is defined in // the WebRTC standard: // https://www.w3.org/TR/webrtc/#rtcicetransportpolicy-enum. // If not set, the effective value is 'all'. // iceTransportPolicy: 'all', // If set to true, it will prefer to use H.264 for P2P calls (if H.264 // is supported). preferH264: true // If set to true, disable H.264 video codec by stripping it out of the // SDP. // disableH264: false, // How long we're going to wait, before going back to P2P after the 3rd // participant has left the conference (to filter out page reload). // backToP2PDelay: 5 &#125;, // A list of scripts to load as lib-jitsi-meet "analytics handlers". // analyticsScriptUrls: [ // "libs/analytics-ga.js", // google-analytics // "https://example.com/my-custom-analytics.js" // ], // The Google Analytics Tracking ID // googleAnalyticsTrackingId = 'your-tracking-id-here-UA-123456-1', // Information about the jitsi-meet instance we are connecting to, including // the user region as seen by the server. deploymentInfo: &#123; // shard: "shard1", // region: "europe", // userRegion: "asia" &#125; // List of undocumented settings used in jitsi-meet /** alwaysVisibleToolbar autoRecord autoRecordToken debug debugAudioLevels deploymentInfo dialInConfCodeUrl dialInNumbersUrl dialOutAuthUrl dialOutCodesUrl disableRemoteControl displayJids enableLocalVideoFlip etherpad_base externalConnectUrl firefox_fake_device googleApiApplicationClientID iAmRecorder iAmSipGateway peopleSearchQueryTypes peopleSearchUrl requireDisplayName tokenAuthUrl */ // List of undocumented settings used in lib-jitsi-meet /** _peerConnStatusOutOfLastNTimeout _peerConnStatusRtcMuteTimeout abTesting avgRtpStatsN callStatsConfIDNamespace callStatsCustomScriptUrl desktopSharingSources disableAEC disableAGC disableAP disableHPF disableNS enableLipSync enableTalkWhileMuted forceJVB121Ratio hiddenDomain ignoreStartMuted nick startBitrate */&#125;; 修改配置后，重启服务。 浏览器中输入：https://meet.demo.com/]]></content>
      <categories>
        <category>video</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>jitsi</tag>
        <tag>meet</tag>
        <tag>video</tag>
        <tag>conferences</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jitsi开源Web视频会议-安装篇]]></title>
    <url>%2Fvideo%2Fjitsi-install-20180426.html</url>
    <content type="text"><![CDATA[Jitsi是安全、简单和可伸缩的视频会议，您可以作为一个独立的应用程序或嵌入到您的web应用程序中。它是基于Prosody之上实现的，按照官方文档安装会出现一些问题，经过一些实验成功安装，并记录下安装步骤。 安装环境 Env: ubuntu 18.04 User: root 安装Prosody1234echo deb http://packages.prosody.im/debian $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.listwget https://prosody.im/files/prosody-debian-packages.key -O- | sudo apt-key add -apt-get updateapt-get install prosody 配置/etc/prosody/conf.d/meet.demo.com.cfg.lua 1234567891011121314151617181920212223242526272829-- Plugins path gets uncommented during jitsi-meet-tokens package install - that's where token plugin is located--plugin_paths = &#123; "/usr/share/jitsi-meet/prosody-plugins/" &#125;VirtualHost "meet.demo.com" authentication = "anonymous" ssl = &#123; key = "/var/lib/prosody/meet.demo.com.key"; certificate = "/var/lib/prosody/meet.demo.com.crt"; &#125; modules_enabled = &#123; "bosh"; "pubsub"; &#125; c2s_require_encryption = falseVirtualHost "auth.meet.demo.com" ssl = &#123; key = "/var/lib/prosody/auth.meet.demo.com.key"; certificate = "/var/lib/prosody/auth.meet.demo.com.crt"; &#125; authentication = "internal_plain"admins = &#123; "focus@auth.meet.demo.com" &#125;Component "conference.meet.demo.com" "muc"Component "jitsi-videobridge.meet.demo.com" component_secret = "password1"Component "focus.meet.demo.com" component_secret = "password2" 证书生成 123456prosodyctl cert generate meet.demo.comprosodyctl cert generate auth.meet.demo.comoutput: /var/lib/prosody/ln -sf /var/lib/prosody/auth.meet.demo.com.crt /usr/local/share/ca-certificates/auth.meet.demo.com.crtupdate-ca-certificates -f 注册用户 1prosodyctl register focus auth.meet.demo.com password3 重启服务1prosodyctl restart 安装Nginx1apt-get install nginx 配置/etc/nginx/sites-enabled/meet.demo.com 1234567891011121314151617181920212223242526272829303132333435server &#123; listen 80; server_name meet.demo.com; return 301 https://$host$request_uri;&#125;server &#123; listen 443 ssl; server_name meet.demo.com; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers "EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA256:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EDH+aRSA+AESGCM:EDH+aRSA+SHA256:EDH+aRSA:EECDH:!aNULL:!eNULL:!MEDIUM:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!RC4:!SEED"; ssl_certificate /var/lib/prosody/meet.demo.com.crt; ssl_certificate_key /var/lib/prosody/meet.demo.com.key; root /var/www/jitsi-meet; index index.html index.htm; # error_page 404 /static/404.html; location ~ ^/([a-zA-Z0-9=\?]+)$ &#123; rewrite ^/(.*)$ / break; &#125; location / &#123; ssi on; &#125; # BOSH location /http-bind &#123; proxy_pass http://localhost:5280/http-bind; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; &#125;&#125; 重启服务 123nginx -tsystemctl restart nginx 安装Jitsi Videobridge下载jdk1.8(jdk-8u171-linux-x64.tar.gz)，不能使用openjdk和default-jre123456789mkdir -p /usr/share/jdkcd /usr/share/jdktar zxvf jdk-8u171-linux-x64.tar.gzvi ~/.bashrcexport JAVA_HOME=/usr/share/jdk/jdk1.8.0_171export PATH=$JAVA_HOME/bin:$PATHjava -version 下载Jitsi Videobridge 1234567mkdir -p /root/jitsicd /root/jitsiwget https://download.jitsi.org/jitsi-videobridge/linux/jitsi-videobridge-linux-x64-1055.zipunzip jitsi-videobridge-linux-x64-1055.zipmv jitsi-videobridge-linux-x64-1055 videobridgecd videobridge 配置Jitsi Videobridge 添加启动脚本start.sh 123456789#!/bin/bashexport JAVA_SYS_PROPS="-Dnet.java.sip.communicator.SC_HOME_DIR_LOCATION=/etc/jitsi -Dnet.java.sip.communicator.SC_HOME_DIR_NAME=videobridge -Dnet.java.sip.communicator.SC_LOG_DIR_LOCATION=/var/log/jitsi/videobridge -Djava.util.logging.config.file=/etc/jitsi/videobridge/logging.properties -Dlog4j.configurationFile=/etc/jitsi/videobridge/log4j2.xml"./jvb.sh --host=localhost --domain=meet.demo.com --port=5347 --secret=password1 &amp;unset JAVA_SYS_PROPSchmod +x start.sh 配置文件 12mkdir -p /etc/jitsi/videobridgecd /etc/jitsi/videobridge 添加logging.properties 到 /etc/jitsi/videobridge123456789101112131415161718192021222324252627282930313233handlers= java.util.logging.FileHandler#handlers= java.util.logging.ConsoleHandler#handlers= java.util.logging.ConsoleHandler, com.agafua.syslog.SyslogHandlerjava.util.logging.ConsoleHandler.level = ALLjava.util.logging.ConsoleHandler.formatter = net.java.sip.communicator.util.ScLogFormatternet.java.sip.communicator.util.ScLogFormatter.programname=JVB.level=INFOorg.jitsi.videobridge.xmpp.ComponentImpl.level=FINE# All of the INFO level logs from MediaStreamImpl are unnecessary in the context of jitsi-videobridge.org.jitsi.impl.neomedia.MediaStreamImpl.level=WARNING# Syslog(uncomment handler to use)com.agafua.syslog.SyslogHandler.transport = udpcom.agafua.syslog.SyslogHandler.facility = local0com.agafua.syslog.SyslogHandler.port = 514com.agafua.syslog.SyslogHandler.hostname = localhostcom.agafua.syslog.SyslogHandler.formatter = net.java.sip.communicator.util.ScLogFormattercom.agafua.syslog.SyslogHandler.escapeNewlines = false# to disable double timestamps in syslog uncomment next line#net.java.sip.communicator.util.ScLogFormatter.disableTimestamp=truejava.util.logging.FileHandler.level = ALLjava.util.logging.FileHandler.formatter = java.util.logging.SimpleFormatterjava.util.logging.FileHandler.limit=1024000java.util.logging.FileHandler.count=10java.util.logging.FileHandler.pattern=/var/log/jitsi/videobridge/jvb%u.logjava.util.logging.FileHandler.append=true 添加 log4j2.xml 到 /etc/jitsi/videobridge 1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration&gt; &lt;Properties&gt; &lt;Property name="log-path"&gt;/var/log/jitsi/videobridge&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;RollingFile name="RollingFile" fileName="$&#123;log-path&#125;/cs.log" filePattern="$&#123;log-path&#125;/$$&#123;date:yyyy-MM&#125;/cs-%d&#123;MM-dd-yyyy&#125;-%i.log.gz"&gt; &lt;PatternLayout pattern="%d %-5p (%F:%L) - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size="250 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- &lt;Console name="STDOUT" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d %-5p (%F:%L) - %m%n"/&gt; &lt;/Console&gt; --&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!--&lt;Logger name="org.apache.log4j.xml" level="debug"/&gt;--&gt; &lt;Logger name="org.apache.log4j.xml" level="info"/&gt; &lt;Root level="info"&gt; &lt;AppenderRef ref="RollingFile"/&gt; &lt;!-- &lt;AppenderRef ref="STDOUT"/&gt; --&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 添加 sip-communicator.properties 到 /etc/jitsi/videobridge1org.jitsi.impl.neomedia.transform.srtp.SRTPCryptoContext.checkReplay=false 启动服务 123mkdir -p /var/log/jitsi/videobridgecd /root/jitsi/videobridge./start.sh 安装Jitsi Jicofo1234567apt-get install mavencd /root/jitsigit clone https://github.com/jitsi/jicofo.gitcd jicofomvn package -DskipTests -Dassembly.skipAssembly=falseunzip target/jicofo-linux-x64-1.1-SNAPSHOT.zip 配置 12mkdir -p /etc/jitsi/jicofocd /etc/jitsi/jicofo 添加 logging.properties 到 /etc/jitsi/jicofo 1234567891011121314151617181920212223242526272829303132333435363738394041424344handlers= java.util.logging.FileHandler#handlers= java.util.logging.ConsoleHandler#handlers= java.util.logging.ConsoleHandler, com.agafua.syslog.SyslogHandlerjava.util.logging.ConsoleHandler.level = ALLjava.util.logging.ConsoleHandler.formatter = net.java.sip.communicator.util.ScLogFormatternet.java.sip.communicator.util.ScLogFormatter.programname=Jicofo.level=INFOnet.sf.level=SEVEREnet.java.sip.communicator.plugin.reconnectplugin.level=FINEorg.ice4j.level=SEVEREorg.jitsi.impl.neomedia.level=SEVERE# Do not worry about missing stringsnet.java.sip.communicator.service.resources.AbstractResourcesService.level=SEVERE#net.java.sip.communicator.service.protocol.level=ALL# Enable debug packets logging#org.jitsi.impl.protocol.xmpp.level=FINE# Syslog(uncomment handler to use)com.agafua.syslog.SyslogHandler.transport = udpcom.agafua.syslog.SyslogHandler.facility = local0com.agafua.syslog.SyslogHandler.port = 514com.agafua.syslog.SyslogHandler.hostname = localhostcom.agafua.syslog.SyslogHandler.formatter = net.java.sip.communicator.util.ScLogFormattercom.agafua.syslog.SyslogHandler.escapeNewlines = false# to disable double timestamps in syslog uncomment next line#net.java.sip.communicator.util.ScLogFormatter.disableTimestamp=true# uncomment to see how Jicofo talks to the JVB#org.jitsi.impl.protocol.xmpp.colibri.level=ALLjava.util.logging.FileHandler.level = ALLjava.util.logging.FileHandler.formatter = java.util.logging.SimpleFormatterjava.util.logging.FileHandler.limit=1024000java.util.logging.FileHandler.count=10java.util.logging.FileHandler.pattern=/var/log/jitsi/jicofo/jicofo%u.logjava.util.logging.FileHandler.append=true 添加 log4j2.xml 到 /etc/jitsi/jicofo 1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration&gt; &lt;Properties&gt; &lt;Property name="log-path"&gt;/var/log/jitsi/jicofo&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;RollingFile name="RollingFile" fileName="$&#123;log-path&#125;/cs.log" filePattern="$&#123;log-path&#125;/$$&#123;date:yyyy-MM&#125;/cs-%d&#123;MM-dd-yyyy&#125;-%i.log.gz"&gt; &lt;PatternLayout pattern="%d %-5p (%F:%L) - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size="250 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- &lt;Console name="STDOUT" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d %-5p (%F:%L) - %m%n"/&gt; &lt;/Console&gt; --&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!--&lt;Logger name="org.apache.log4j.xml" level="debug"/&gt;--&gt; &lt;Logger name="org.apache.log4j.xml" level="info"/&gt; &lt;Root level="info"&gt; &lt;AppenderRef ref="RollingFile"/&gt; &lt;!-- &lt;AppenderRef ref="STDOUT"/&gt; --&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 添加 sip-communicator.properties 到 /etc/jitsi/jicofo (empty file) 启动脚本cd /root/jitsi/jicofo 添加 start.sh 到 /root/jitsi/jicofo 12345678910#!/bin/bash# mvn package -DskipTests -Dassembly.skipAssembly=falseexport JAVA_SYS_PROPS="-Dnet.java.sip.communicator.SC_HOME_DIR_LOCATION=/etc/jitsi -Dnet.java.sip.communicator.SC_HOME_DIR_NAME=jicofo -Dnet.java.sip.communicator.SC_LOG_DIR_LOCATION=/var/log/jitsi/jicofo -Djava.util.logging.config.file=/etc/jitsi/jicofo/logging.properties -Dlog4j.configurationFile=/etc/jitsi/jicofo/log4j2.xml"./jicofo-linux-x64-1.1-SNAPSHOT/jicofo.sh --host=localhost --domain=meet.demo.com --secret=password2 --user_domain=auth.meet.demo.com --user_name=focus --user_password=password3 &amp;unset JAVA_SYS_PROPS 启动服务 1234chmod +x start.shmkdir -p /var/log/jitsi/jicofo/./start.sh 安装Jitsi Meet编译 123456cd /root/jitsigit clone https://github.com/jitsi/jitsi-meet.gitnpm installmake 配置 12cp -fr jitsi-meet /var/www/cd /var/www/jitsi-meet 修改config.js123456789101112131415var config = &#123; hosts: &#123; domain: 'meet.demo.com', muc: 'conference.meet.demo.com', bridge: 'jitsi-videobridge.meet.demo.com', focus: 'focus.meet.demo.com' &#125;, useNicks: false, bosh: '//meet.demo.com/http-bind', // FIXME: use xep-0156 for that //chromeExtensionId: 'diibjkoicjeejcmhdnailmkgecihlobk', // Id of desktop streamer Chrome extension //minChromeExtVersion: '0.1' // Required version of Chrome extension&#125;;check /etc/nginx/sites-enabled/meet.demo.com rootroot /var/www/jitsi-meet; 重启服务1systemctl restart nginx 浏览器中输入：https://meet.demo.com/]]></content>
      <categories>
        <category>video</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>jitsi</tag>
        <tag>meet</tag>
        <tag>video</tag>
        <tag>conferences</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑苹果hackintosh 安装macos high sierra系统]]></title>
    <url>%2F%E7%94%B5%E8%84%91%E7%A1%AC%E4%BB%B6%2Fhackintosh-to-high-sierra-20180212.html</url>
    <content type="text"><![CDATA[macos high sierra更新一段时间了，以前的hackintosh系统已经很久没有更新了，趁过年更新下系统。系统硬件参考：&lt;&lt;安装Mac Window10 双系统，为发烧而生&gt;&gt; 准备工作 mac电脑 下载Install macOS High Sierra 16G U盘 下载uniBeast、MultiBeast、clover configurator软件 下载显卡驱动WebDriver-387.10.10.10.25.156(根据自己的显卡) 启动盘格式化U盘 双击UniBeast并安装 将下载的软件拷贝到U盘 将U盘插入你的PC。 Bios设置 cpu支持VT-d，禁用它。 支持CFG-Lock，禁用它。 支持Secure Boot Mode，禁用它。 支持IO Serial Port，禁用它。 Set OS 类型 Other OS Set XHCI Handoff 选项为Enable。 设置第一启动为U盘 （或者重启按F12选择启动项） 保存并退出 安装macOS选择Boot OS X Install from Install macOS high Sierra进入安装mac 系统，这里就不截图了，安装过程中可能会重启几次。 系统补丁使用MultiBeast安装补丁 显卡驱动 修复声卡参看：https://hackintosher.com/guides/get-hackintosh-audio-working/ 打开终端: diskutil list diskutil mount /dev/disk0s1 拷贝kext到EFI-&gt;kexts-&gt;other目录下 使用clover configurator配置 关闭clover configurator，保存config.plist. 然后重启电脑。ok，你可以听音乐了。]]></content>
      <categories>
        <category>电脑硬件</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>high sierra</tag>
        <tag>hackintosh</tag>
        <tag>黑苹果</tag>
        <tag>安装</tag>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GraphQL查询语言实践-实现BoltMQ的Console]]></title>
    <url>%2Fgraphql%2Fgraphql-golang-fulfill-20180206.html</url>
    <content type="text"><![CDATA[上篇文章《GraphQL查询语言学习笔记》学习了GraphQL的语法和介绍Golang的库。最近再使用Golang编写一款分布式消息队列BoltMQ，我将GraphQL用到BoltMQ的web管理UI上，这样更进一步了解GraphQL。 GraphQL 设计篇有了将GraphQL应用到BoltMQ的web UI上的需求，那如何来做呢。我总结了下： 业务梳理 GraphQL API设计 GraphQL Schema设计 GraphQL 第三方库的选择 GraphQL 客户端与服务端的实现 GraphQL API设计API的设计比较重要，需要开发人员充分理解业务，在业务的基础抽象有查询Graph。例如console是BolotMQ的集群管理UI。需求： 管理多个BoltMQ集群 查询集群的节点信息 查询集群的统计信息 查询集群的topic信息 查询集群的消息信息 查询集群的订阅组信息 查询集群的消费进度 查询集群的在线消费进程意思列举了几个功能点，首先将集群这个概念抽象出来，用户在选定集群的情况下才会做下面的查询操作，所有集群就可以作为第一层，然后再往下梳理。graphql就是将业务抽象成图（树）的形式的。 以下是对console的查询操作的API设计，当然你需要理解BoltMQ的一些知识。你可查看BoltMQ了解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118query clusters($name: String, $like: String, $group: String, $msgId: String!) &#123; clusters(name: $name) &#123; name stats &#123; producerNums consumerNums brokerNums namesrvNums topicNums outTotalTodayNums outTotalYestNums inTotalTodayNums inTotalYestNums &#125; nodes &#123; namesrvAddrs brokerNodes &#123; role addr version desc outTps inTps outTotalTodayNums outTotalYestNums inTotalTodayNums inTotalYestNums &#125; &#125; topics(like: $like) &#123; topic type isSystem store &#123; brokerName queueId maxOffset minOffset lastUpdateTime &#125; route &#123; queues &#123; brokerName writeQueueNums readQueueNums perm sysFlag &#125; brokers &#123; brokerName brokerAddrs &#123; brokerId addr &#125; &#125; &#125; groups consumeConn &#123; describe conns &#123; consumeGroup clientId clientAddr language version consumeTps consumeFromWhere consumeType diff messageModel &#125; &#125; consumeProgress(group: $group) &#123; consumeGroup tps diff total progress &#123; brokerOffset consumeOffset diff brokerName queueId &#125; &#125; &#125; &#125; msg(msgId: $msgId) &#123; info &#123; msgId topic flag body queueId storeSize queueOffset sysFlag bornTimestamp bornHost storeTimestamp storeHost commitLogOffset bodyCRC reconsumeTimes preparedTransactionOffset properties &#123; key val &#125; &#125; tracks &#123; code type consumeGroup desc &#125; &#125;&#125; GraphQL Schema 设计结合GraphQL API设计类型系统Schema，这里的设计偏后端一些，和API是相辅相成的。在API基础上再次确定返回值的类型以及结构的优化。当然API和Schema的设计可以同时做，也可以分开进行。 console的类型系统123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347# boltmq contole graphql schemaschema &#123; query: Query mutation: Mutation&#125;# The query type, represents all of the entry points into our object graphtype Query &#123; clusters(name: String): [Cluster]! msg(name: String, msgId: String!): Message&#125;# A Cluster from the boltmq servertype Cluster &#123; # The name of cluster name: String! # The stats info of cluster stats: ClusterStats! # The node info of cluster nodes: ClusterNode! # The topics of cluster topics(like: String): [Topic]!&#125;# A ClusterStats info of boltmq clustertype ClusterStats &#123; # The producer nums of cluster producerNums: Int! # The consumer nums of cluster consumerNums: Int! # The broker nums of cluster brokerNums: Int! # The name server nums of cluster namesrvNums: Int! # The topic nums of cluster topicNums: Int! # The cluster consumer msg total number today outTotalTodayNums: Int! # The cluster consumer msg total number yest outTotalYestNums: Int! # The cluster producer msg total number yest inTotalTodayNums: Int! # The cluster producer msg total number today inTotalYestNums: Int!&#125;# A Cluster node info of boltmq clustertype ClusterNode &#123; # The namesrv addr list fo cluster namesrvAddrs: [String!]! # The broker node list fo cluster brokerNodes: [BrokerNode]!&#125;# A Boker node info of boltmq clustertype BrokerNode &#123; # The borker role role: Int! # The borker addr addr: String! # The borker server version version: String! # The borker server describe desc: String! # The borker server current out tps outTps: Float! # The borker server current in tps inTps: Float! # The cluster consumer msg total number today outTotalTodayNums: Int! # The cluster consumer msg total number yest outTotalYestNums: Int! # The cluster producer msg total number yest inTotalTodayNums: Int! # The cluster producer msg total number today inTotalYestNums: Int!&#125;# A topic info of boltmq clustertype Topic &#123; # The topic name topic: String! # The topic type type: Int! # The topic type isSystem: Boolean! # The topic store store: TopicStore! # The topic route route: TopicRoute! # The consume group groups: [String!]! # The consume connection consumeConn: ConsumeConn! consumeProgress(group: String): [ConsumeProgress]!&#125;# topic typeenum TopicType &#123; # normal topic NORMAL_TOPIC # retry topic RETRY_TOPIC # deadline queue topic DLQ_TOPIC&#125;# A topic stroe info of boltmq clustertype TopicStore &#123; # The broker name brokerName: String! # The queue id queueId: Int! # The max offset maxOffset: Int! # The min offset minOffset: Int! # The last update time lastUpdateTime: String!&#125;# A topic route info of boltmq clustertype TopicRoute &#123; # The route data of queue queues: [QueueData]! # The route data of broker brokers: [BrokerData]!&#125;# A queue route data of topictype QueueData &#123; # The broker name brokerName: String! # The write queue nums writeQueueNums: Int! # The read queue nums readQueueNums: Int! # The permissions of topic on broker perm: Int! # The permissions of topic on broker sysFlag: Int!&#125;# A broker route data of topictype BrokerData &#123; # The broker name brokerName: String! # The broker addrs brokerAddrs: [BrokerAddr]!&#125;# A broker addr of topic routetype BrokerAddr &#123; # The broker id brokerId: Int! # The broker addr addr: String!&#125;# consume connectiontype ConsumeConn &#123; # The describe describe: String! # The connection conns: [Connection]!&#125;# connection infotype Connection &#123; # The consume group name consumeGroup: String! # The client id clientId: String! # The client addr clientAddr: String! # The language language: String! # The version version: String! # The consume tps consumeTps: Float! # The consume from where consumeFromWhere: String! # The consume type consumeType: Int! # The message diff total diff: Int! # The message model messageModel: Int!&#125;# consume typeenum ConsumeType &#123; # actively consume CONSUME_ACTIVELY # passively consume CONSUME_PASSIVELY&#125;# message modelenum MessageModel &#123; # broadcasting BROADCASTING # clustering CLUSTERING&#125;# consume progresstype ConsumeProgress &#123; # The consume group name consumeGroup: String! # The consume tps tps: Float! # The consume diff diff: Int! # The total total: Int! # The progress data list progress: [ConsumeProgressData]!&#125;# consume progress datatype ConsumeProgressData &#123; # The broker offset brokerOffset: Int! # The broker offset consumeOffset: Int! # The consume diff diff: Int! # The broker name brokerName: String! # The queue id queueId: Int!&#125;# messagetype Message &#123; # The message base info info: MessageInfo! # The message track list tracks: [MessageTrack]!&#125;# message infotype MessageInfo &#123; # The message id msgId: String! # The topic name topic: String! # The message flag flag: Int! # The message body body: String! # The queue id queueId: Int! # The store size storeSize: Int! # The queue offset queueOffset: Int! # The message sys flag sysFlag: Int! # The born timestamp bornTimestamp: String! # The born host bornHost: String! # The store timestamp storeTimestamp: String! # The store host storeHost: String! # The commitlog offset commitLogOffset: Int! # The message body crc bodyCRC: Int! # The reconsume times reconsumeTimes: Int! # The reconsume times preparedTransactionOffset: Int! # The properties properties: [Property!]!&#125;# property, replace maptype Property &#123; key: String! val: String!&#125;# message tracktype MessageTrack &#123; # The track code, 0: success, non-0: failed code: Int! # track type type: Int! # consume group name consumeGroup: String! # error describe desc: String!&#125;# track typeenum TrackType &#123; # subscribed and consumed SUBSCRIBEDANDCONSUMED # subscribed but filterd SUBSCRIBEDBUTFILTERD # subscribed but pull SUBSCRIBEDBUTPULL # subscribed and not consume yet SUBSCRIBEDBUTNOTCONSUMEYET # unknow exeption UNKNOWEXEPTION # not subscribed and not consumed NOTSUBSCRIBEDANDNOTCONSUMED # consume groupId not online CONSUMEGROUPIDNOTONLINE&#125;# The mutation type, represents all updates we can make to our datatype Mutation &#123; create2UpdateTopic(name: String!, topic: TopicInput!): TopicResponse deleteTopic(name: String!, topic: String!): TopicResponse&#125;# The input object sent when cluster is creating a new topicinput TopicInput &#123; # topic topic: String! # The read queue nums, optional readQueueNums: Int! # The write queue nums, optional writeQueueNums: Int! # The order topic, optional order: Boolean! # The unit topic, optional unit: Boolean!&#125;# Represents a topic for a clusterinterface Response &#123; code: Int! desc: String!&#125;type TopicResponse implements Response &#123; code: Int! desc: String!&#125; GraphQL 代码实现console使用的neelance/graphql-go库，代码查看。使用的缺点和注意事项： neelance使用反射实现 graphql类型匹配严格，并缺少对于的int,int64等基础类型。 接口的实现不优雅，我提了issue，等待改进。 context无法向下传递 封装统一认证 请求header中取得jwtToken进行验证。123456789101112131415161718192021222324252627282930313233type userClaims struct &#123; user jwt.StandardClaims&#125;type authenticator struct &#123;&#125;func (auth *authenticator) Chain(w http.ResponseWriter, r *http.Request, ctx *Context) bool &#123; // extract jwt jwtToken := r.Header.Get("Authorization") // parse tokentoken token, err := jwt.ParseWithClaims(jwtToken, &amp;userClaims&#123;&#125;, func(token *jwt.Token) (interface&#123;&#125;, error) &#123; if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok &#123; return nil, fmt.Errorf("Unexpected signing method") &#125; return jwtSecret, nil &#125;) if err != nil &#123; http.Error(w, "not authorized", http.StatusUnauthorized) return false &#125; claims, ok := token.Claims.(*userClaims) if !ok || !token.Valid &#123; http.Error(w, "not authorized", http.StatusUnauthorized) return false &#125; ctx.ctx = context.WithValue(r.Context(), userAuthKey, claims.user) return true&#125; 登录后生成token并返回。12345678910111213141516171819202122232425262728293031323334353637383940414243444546type loginHandler struct &#123;&#125;func (h *loginHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; decoder := json.NewDecoder(r.Body) userParam := struct &#123; Username string `json:"username"` Password string `json:"password"` &#125;&#123;&#125; err := decoder.Decode(&amp;userParam) if err != nil &#123; http.Error(w, err.Error(), http.StatusInternalServerError) return &#125; defer r.Body.Close() if userParam.Username != "admin" || userParam.Password != "admin" &#123; http.Error(w, "invalid login", http.StatusUnauthorized) return &#125; //generate token expire := time.Now().Add(time.Hour * 1).Unix() // Create the Claims claims := userClaims&#123; user: user&#123; UserID: 1, UserName: userParam.Username, IsAdmin: true, &#125;, StandardClaims: jwt.StandardClaims&#123; ExpiresAt: expire, Issuer: "login", &#125;, &#125; token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) signedToken, _ := token.SignedString(jwtSecret) //output token tokenResponse := struct &#123; Token string `json:"token"` &#125;&#123;signedToken&#125; json.NewEncoder(w).Encode(tokenResponse)&#125; GraphQL接口接入认证1srv.mux.Handle(pattern, join(&amp;relay.Handler&#123;Schema: schema&#125;, &amp;authenticator&#123;&#125;)) 登录接入1srv.mux.Handle(pattern, &amp;loginHandler&#123;&#125;) 以上4点就完成认证功能，详细代码查看console-server 还需功能 dataloader 对比graphql-go库，graphql-go更佳灵活，当抽象程度差一些。根据自己情况自行选择。]]></content>
      <categories>
        <category>graphql</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>graphql</tag>
        <tag>graphiql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack O版 安装部署及问题解决]]></title>
    <url>%2Fopenstack%2Fopentstack-o-deploy-20180131.html</url>
    <content type="text"><![CDATA[OpenStack项目是由Rackspace和NASA（美国国家航空航天局）共同发起的。它是一个开源软件，它提供了一个部署云的平台。为虚拟计算或存储服务的公有/私有云，提供可扩展的、灵活的云计算。是一个旨在为公共及私有云的建设与管理提供软件的开源项目。OpenStack目前版本Pike并在持续更新中，本文选用Ocata版本进行部署，并记录部署过程中遇到的问题。 环境准备操作系统Centos7.2.1151 mini(国内mirrors没有，官方下载) 服务节点本文使用三个节点部署stand-one方式，按照官方文档进行部署，高可用部署之后文章会跟进。 controller节点 computer节点 block storage节点 网络说明 Manage network 管理网络，OpenStack各个模块之间的交互，连接数据库，连接Message Queue都是通过这个网络来进行。 Private network 私有网络，虚拟机之间的数据传输通过这个网络来进行，虚拟机要连接虚拟路由都是通过这个网络来进行。 External network 外部网络，无论是用户调用OpenStack的API，还是创建出来的虚拟机要访问外网，或者外网要ssh到虚拟机，都用这个网络。 OpenStack将三个网络进行隔离，一方面是安全，在虚拟机里面，干扰的都仅仅是Private Network，都不可能访问到我的数据库。一方面是流量分离，Manage Network的流量不是很大的，而且一般都会比较优雅的使用，而Prviate Network和External Network就需要有流量控制策略。 节点 Manage network Private network External network controller 10.50.1.11 10.50.1.11 10.112.1.116 computer 10.50.1.10 10.50.1.11 10.112.1.117(临时) block storage 10.50.1.12 10.50.1.12 10.112.1.118(临时) 注意：非高可用环境采用Manage Private同网方式，computer和block storage节点的External network的外网用于安装环境的。 配置节点别名 修改/etc/hostname的主机名称，修改每个节点的/etc/hosts12310.50.1.11 controller10.50.1.10 computer10.50.1.12 blokstorage Yum下载源 123mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum makecache 安装流程请参看官方Ocata版本安装向导进行安装。请一步一步进行安装，官方文档已经非常详尽了，这里就不在重复写了，我会我部署过程中的问题中列举出来并记录。 安装问题及解决方案编码unknown locale: UTF-8在进行同步数据库命令时，有时会出现unknown locale: UTF-8错误并同步失败。终端中输入以下命令后再执行同步命令即可:12export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8 连接不上rabbitmq的问题其它节点连接不上rabbitmq并提示127.0.0.1:5672(/var/log/下的日志)，文档中已经明确配置了transport_url = rabbit://openstack:RABBIT_PASS@controller，这里的rabbitmq的主机是controller而不是127.0.0.1。问题在于配置中还有rabbit_host参数指定，配置rabbit_host=controller重启服务即可。 chrony时间同步问题参看各个节点的时区是否一致，可使用timedatectl命令进行查看与修改。如果chrony没有能同步时间，请使用命令修改系统时间后再systemctl restart chrony。如果时间没有同步，会导致controller检查block storage的cinder-volume服务一直为down状态。controller节点使用命令openstack volume service list查看。 设置时区1timedatectl set-timezone Asia/Shanghai 重启机器后无法访问web的问题重启后mysql、rabbitmq服务可能没有启动，需要检查并启动。启动后先需要重启http和memcached. 123systemctl start mariadb.servicesystemctl start rabbitmq-server.servicesystemctl restart httpd.service memcached.service]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>虚拟化</tag>
        <tag>openstack</tag>
        <tag>云计算</tag>
        <tag>centos7.2</tag>
        <tag>ocata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GraphQL查询语言学习笔记]]></title>
    <url>%2Fgraphql%2Fgraphql-golang-20180123.html</url>
    <content type="text"><![CDATA[RSETful API接口想必大家都比较熟悉，GraphQL设计用于替代RESTful API接口，但目前不是所有情况都使用替代，他们各有优势。GraphQL 是一个用于 API 的查询语言，是一个使用基于类型系统来执行查询的服务端运行时（类型系统由你的数据定义）。GraphQL 并没有和任何特定数据库或者存储引擎绑定，而是依靠你现有的代码和数据支撑。 在 2015 React 欧洲大会上，Lee Byron 介绍了 Facebook 的 GraphQL ，包含 GraphQL 背后的故事，查询语句的示例，还有核心的概念。GraphQL 非常易懂，直接看查询语句就能知道查询出来的数据是什么样的。你可以把 GraphQL 的查询语句想成是没有值，只有属性的对象，返回的结果就是对应的属性还有对应值的对象。 GraphQL 基础篇GraphQL特性是用户可以自定义查询对象字段，使用图（树）的形式表示业务对象，从而定义查询示例。Facebook定义GraphQL规范，GraphQL语法方面的知识可以参考: 英文 http://facebook.github.io/graphql/October2016/ https://github.com/facebook/graphql 中文 http://spec.graphql.cn//#sec-Overview- 官方提供了新手课程: 英文 http://graphql.org/learn https://www.howtographql.com/ 中文 http://graphql.cn/learn/ GraphQL包括: 类型系统（Type System） 类型语言（Type Language） 对象类型和字段（Object Types and Fields）等查询语言的定义和专用名词。如果你还是新手，请认真阅读GraphQL规范和新手课程。你已经理解了GraphQL是什么，那么可以接下来就是如何使用Golang编写GraphQL Server，如何编写schema，如何运行以及调试。 GraphQL Server首先使用golang语言实现官方示例，以了解GraphQL Server如何编写。当然GraphQL中有客户端和服务端。 客户端库： Relay (github)：Facebook 的框架，用于构建与 GraphQL 后端交流的 React 应用。 Apollo Client (github)：一个强大的 JavaScript GraphQL 客户端，设计用于与 React、React Native、Angular 2 或者原生 JavaScript 一同工作。 graphql: 一个使用 Go 编写的 GraphQL 客户端实现。 服务端库： graphql-go： 支持查询解析器，但不支持GraphQL SDL解析。 graphql-relay-go： 支持react-relay，一般配合graphql-go使用。 neelance/graphql-go： 支持查询解析器和GraphQL SDL解析。(缺少自动生成定义SDL代码工具) 接下来分别使用graphql-go和neelance/graphql-go来实现一些简单示例。 graphql-go库实现graphql-go支持查询解析器，不支持GraphQL SDL解析。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package mainimport ( "encoding/json" "fmt" "github.com/graphql-go/graphql")type user struct &#123; ID string `json:"id"` Name string `json:"name"`&#125;var data map[string]user = map[string]user&#123; "1": user&#123; ID: "1", Name: "Dan", &#125;, "2": user&#123; ID: "2", Name: "Lee", &#125;, "3": user&#123; ID: "3", Name: "Nick", &#125;,&#125;// Create User object type with fields "id" and "name" by using GraphQLObjectTypeConfig:// - Name: name of object type// - Fields: a map of fields by using GraphQLFields// Setup type of field use GraphQLFieldConfigvar userType = graphql.NewObject( graphql.ObjectConfig&#123; Name: "User", Fields: graphql.Fields&#123; "id": &amp;graphql.Field&#123; Type: graphql.String, &#125;, "name": &amp;graphql.Field&#123; Type: graphql.String, &#125;, &#125;, &#125;,)// Create Query object type with fields "user" has type [userType] by using GraphQLObjectTypeConfig:// - Name: name of object type// - Fields: a map of fields by using GraphQLFields// Setup type of field use GraphQLFieldConfig to define:// - Type: type of field// - Args: arguments to query with current field// - Resolve: function to query data using params from [Args] and return value with current typevar queryType = graphql.NewObject( graphql.ObjectConfig&#123; Name: "Query", Fields: graphql.Fields&#123; "user": &amp;graphql.Field&#123; Type: userType, Args: graphql.FieldConfigArgument&#123; "id": &amp;graphql.ArgumentConfig&#123; Type: graphql.String, &#125;, &#125;, Resolve: func(p graphql.ResolveParams) (interface&#123;&#125;, error) &#123; idQuery, isOK := p.Args["id"].(string) if isOK &#123; if v, exist := data[idQuery]; exist &#123; return v, nil &#125; return nil, nil &#125; return nil, nil &#125;, &#125;, &#125;, &#125;)var schema, _ = graphql.NewSchema( graphql.SchemaConfig&#123; Query: queryType, &#125;,)func executeQuery(query string, schema graphql.Schema, vars map[string]interface&#123;&#125;) \*graphql.Result &#123; result := graphql.Do(graphql.Params&#123; Schema: schema, RequestString: query, VariableValues: vars, &#125;) if len(result.Errors) &gt; 0 &#123; fmt.Printf("wrong result, unexpected errors: %v", result.Errors) &#125; return result&#125;func main() &#123; query := `query userinfo($uid: String = "1") &#123; user(id: $uid)&#123; id name &#125; &#125; ` vars := map[string]interface&#123;&#125;&#123;"uid": "3"&#125; r := executeQuery(query, schema, vars) rJSON, _ := json.Marshal(r) fmt.Printf("%s \n", rJSON)&#125; 上面示例是查询用户信息，作为第一个示例比较适合。示例中代码schema, _ = graphql.NewSchema定义了查询schema, 使用executeQuery查询结果并打印。schema中定义了查询入口Query: queryType，queryType中的Resolve处理请求。 点击源码，运行go run main.go查看结果。 提供GraphQL Server http服务 构建http服务只需简单几行代码(源码)：12345678910func main() &#123; http.HandleFunc("/graphql", func(w http.ResponseWriter, r *http.Request) &#123; result := executeQuery(r.URL.Query().Get("query"), schema, nil) json.NewEncoder(w).Encode(result) &#125;) fmt.Println("Now server is running on port 8080") fmt.Println("Test with Get : curl -g 'http://localhost:8080/graphql?query=&#123;user(id:\"1\")&#123;name&#125;&#125;'") http.ListenAndServe(":8080", nil)&#125; 终端输入curl -g &#39;http://localhost:8080/graphql?query={user(id:\&quot;1\&quot;){name}}&#39;查看结果吧。 使用graphql-go/handler包提供http服务 使用graphql-go/handler包(源码)：123456789101112func main() &#123; h := handler.New(&amp;handler.Config&#123; Schema: &amp;schema, Pretty: true, GraphiQL: true, &#125;) http.Handle("/graphql", h) fmt.Println("Now server is running on port 8080") fmt.Println("Test with Get : curl -g 'http://localhost:8080/graphql?query=&#123;user(id:\"1\")&#123;name&#125;&#125;'") http.ListenAndServe(":8080", nil)&#125; 使用graphql-go/handler包后不需要executeQuery函数，并提供了额外的配置项。 neelance/graphql-go库实现neelance/graphql-go支持查询解析器和GraphQL SDL解析。这里使用官方的starwars作为示例,查看starwars示例 实现服务端按下面步骤实现 编写GraphQL SDL 源码 编写GraphQL SDL实现 源码 编写GraphQL Server实现 源码 1234567891011121314151617var schema *graphql.Schemafunc init() &#123; schema = graphql.MustParseSchema(starwars.Schema, &amp;starwars.Resolver&#123;&#125;)&#125;func main() &#123; http.Handle("/", http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) &#123; w.Write(page) &#125;)) http.Handle("/query", &amp;relay.Handler&#123;Schema: schema&#125;) fmt.Println("Now server is running on port 8080.") fmt.Println("Test with Get : http://localhost:8080") log.Fatal(http.ListenAndServe(":8080", nil))&#125; init()函数中初始化schema，main()中监听8080端口出来请求并响应。starwars包下的schema定义和逻辑实现可查看源码。 graphql-go-tools工具neelance的实现需要两部分schema的定义与Reolver的实现。graphql-go-tools包是我自己写的将定义的schema自动生成Reolver代码模板。目前只实现了将graphql文件转换为schema string，后续整理后持续实现。示例代码：graphql-go-tools Dataloader缓存dataloader是用于缓存数据的包，GraphQL是支持多层嵌套结构的（图关系，树结构），缓存数据可以减少请求次数提高性能。 dataloader GraphQL工具chrome extension GraphQL Network Apollo Client Developer Tools ChromeiQL mac tools GraphQL Ide GraphQL PalyGround GraphiQL 推荐使用GraphQL Ide，它有类似postman的collection功能，方便保存请求接口。也可以在程序debug模式中启用内置GraphiQL。]]></content>
      <categories>
        <category>graphql</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>graphql</tag>
        <tag>graphiql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cassandra数据库Golang Driver]]></title>
    <url>%2Fnosql%2Fcassandra-golang-driver-20171219.html</url>
    <content type="text"><![CDATA[搭建cassandra集群后，项目使用Golang语言对cassandra进行操作。需要Golang Driver，有gocql，gocql只提供了curd的操作方法，但不支持创建、删除keyspace。查询后有两种方法可以实现： gocql + gocqltable，纯Golang实现，gocql持续更新中。 golang-driver，需要C/C++ driver支持。 考虑后使用第一方法 下载代码包12go get -u github.com/gocql/gocqlgo get -u github.com/kristoiv/gocqltable 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package mainimport ( "fmt" "log" "time" "github.com/gocql/gocql" "github.com/kristoiv/gocqltable")func main() &#123; // Generic initialization of gocql c := gocql.NewCluster("10.112.68.186", "10.112.68.192") s, err := c.CreateSession() if err != nil &#123; log.Fatalln("Unable to open up a session with the Cassandra database (err=" + err.Error() + ")") &#125; // Tell gocqltable to use this session object as the default for new objects gocqltable.SetDefaultSession(s) fmt.Println("Gocql session setup complete") // Now we're ready to create our first keyspace. We start by getting a keyspace object keyspace := gocqltable.NewKeyspace("gocqltable_test") // Now lets create that in the database using the simple strategy and durable writes (true) err = keyspace.Create(map[string]interface&#123;&#125;&#123; "class": "SimpleStrategy", "replication_factor": 1, &#125;, true) if err != nil &#123; // If something went wrong we print the error and quit. log.Fatalln(err) &#125; fmt.Println("Keyspace created") // Now that we have a very own keyspace to play with, lets create our first table. // First we need a Row-object to base the table on. It will later be passed to the table wrapper // to be used for returning row-objects as the answer to fetch requests. type User struct &#123; Email string // Our primary key Password string Active bool Created time.Time &#125; // Let's define and instantiate a table object for our user table userTable := struct &#123; gocqltable.Table &#125;&#123; keyspace.NewTable( "users", // The table name []string&#123;"email"&#125;, // Row keys nil, // Range keys User&#123;&#125;, // We pass an instance of the user struct that will be used as a type template during fetches. ), &#125; // Lets create this table in our cassandra database err = userTable.Create() if err != nil &#123; log.Fatalln(err) &#125; fmt.Println("") fmt.Println("Table created: users") // Now that we have a keyspace with a table in it: lets make a few rows! Notice that this is the base example, it uses CQL (not ORM) // for database interactions such as INSERT/SELECT/UPDATE/DELETE. err = userTable.Query("INSERT INTO gocqltable_test.users (email, password, active, created) VALUES (?, ?, ?, ?)", "1@example.com", "123456", true, time.Now().UTC()).Exec() if err != nil &#123; log.Fatalln(err) &#125; fmt.Println("User inserted: 1@example.com") err = userTable.Query("INSERT INTO gocqltable_test.users (email, password, active, created) VALUES (?, ?, ?, ?)", "2@example.com", "123456", true, time.Now().UTC()).Exec() if err != nil &#123; log.Fatalln(err) &#125; fmt.Println("User inserted: 2@example.com") err = userTable.Query("INSERT INTO gocqltable_test.users (email, password, active, created) VALUES (?, ?, ?, ?)", "3@example.com", "123456", true, time.Now().UTC()).Exec() if err != nil &#123; log.Fatalln(err) &#125; fmt.Println("User inserted: 3@example.com") // With our database filled up with users, lets query it and print out the results. iter := userTable.Query("SELECT * FROM gocqltable_test.users").Fetch() fmt.Println("") fmt.Println("Fetched all from users:") for row := range iter.Range() &#123; user := row.(*User) // Our row variable is a pointer to "interface&#123;&#125;", and here we type assert it to a pointer to "User" fmt.Println("User:", user) // Let's just print that &#125; if err := iter.Close(); err != nil &#123; log.Fatalln(err) &#125; // You can also fetch a single row, obviously row, err := userTable.Query(`SELECT * FROM gocqltable_test.users WHERE email = ? LIMIT 1`, "2@example.com").FetchRow() if err != nil &#123; log.Fatalln(err) &#125; user := row.(*User) fmt.Println("") fmt.Println("Fetched single row by email: ", user) // Lets clean up after ourselves by dropping the keyspace. keyspace.Drop() fmt.Println("") fmt.Println("Keyspace dropped")&#125;]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库Cassandra运维工具nodetool连接远程节点]]></title>
    <url>%2Fnosql%2Fcassandra-nodetool-20171214.html</url>
    <content type="text"><![CDATA[cassandra集群安装成功后使用nodetool命令连接本机节点正常，连接其他节点报错：connectexception &#39;connection refused (connection refused)&#39;，查看nodetool连接的7199端口，监听0.0.0.0。那为什么会连接不上呢？原因在于：cassandra启动是有两种模式，local和remote模式。可以在conf/cassandra-env.sh中查看和修改。 配置cassandra默认使用local模式，配置如下：12345678910111213141516171819202122232425262728293031if [ "x$LOCAL_JMX" = "x" ]; then LOCAL_JMX=yesfi# Specifies the default port over which Cassandra will be available for# JMX connections.# For security reasons, you should not expose this port to the internet. Firewall it if needed.JMX_PORT="7199"if [ "$LOCAL_JMX" = "yes" ]; then JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT" JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false"else JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.remote.port=$JMX_PORT" # if ssl is enabled the same port cannot be used for both jmx and rmi so either # pick another value for this property or comment out to use a random port (though see CASSANDRA-7087 for origins) JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT" # turn on JMX authentication. See below for further options JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=true" # jmx ssl options #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl=true" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.need.client.auth=true" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.protocols=&lt;enabled-protocols&gt;" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.cipher.suites=&lt;enabled-cipher-suites&gt;" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStore=/path/to/keystore" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStorePassword=&lt;keystore-password&gt;" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStore=/path/to/truststore" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStorePassword=&lt;truststore-password&gt;"fi 如果想使用remote模式，添加LOCAL_JMX=false和修改-Dcom.sun.management.jmxremote.authenticate=false，当然你可以开启加密方式。配置如下：1234567891011121314151617181920212223242526272829303132LOCAL_JMX=falseif [ "x$LOCAL_JMX" = "x" ]; then LOCAL_JMX=yesfi# Specifies the default port over which Cassandra will be available for# JMX connections.# For security reasons, you should not expose this port to the internet. Firewall it if needed.JMX_PORT="7199"if [ "$LOCAL_JMX" = "yes" ]; then JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT" JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false"else JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.remote.port=$JMX_PORT" # if ssl is enabled the same port cannot be used for both jmx and rmi so either # pick another value for this property or comment out to use a random port (though see CASSANDRA-7087 for origins) JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT" # turn on JMX authentication. See below for further options JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false" # jmx ssl options #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl=true" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.need.client.auth=true" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.protocols=&lt;enabled-protocols&gt;" #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.cipher.suites=&lt;enabled-cipher-suites&gt;" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStore=/path/to/keystore" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStorePassword=&lt;keystore-password&gt;" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStore=/path/to/truststore" #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStorePassword=&lt;truststore-password&gt;"fi 修改配置后，重启节点。一切ok]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>nodetool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库Cassandra监控MX4J]]></title>
    <url>%2Fnosql%2Fcassandra-mx4j-20171213.html</url>
    <content type="text"><![CDATA[cassandra监控有许多方案，DataStax的Opscenter是比较耗的监控解决方案。可惜Opscenter6.0版本后已经不在支持开源cassandra版本。Opscenter5.x版本支持到cassandra2.1。最好可以使用cassandra自身支持的MX4J进行监控。 下载MX4J下载页面：http://mx4j.sourceforge.net/ 解压后将mx4j-tools.jar拷贝到cassandra的安装目录的lib文件夹下12unzip mx4j-3.0.2.zipcp mx4j-3.0.2/lib/mx4j-tools.jar apache-cassandra-3.11.1/lib/ 配置mx4j编辑vim conf/cassandra-env.sh，去除以下注释的配置项12MX4J_ADDRESS="-Dmx4jaddress=10.112.68.186"MX4J_PORT="-Dmx4jport=8081" 然后重启cassandra1./bin/cassandra 登录mx4j浏览器中输入：http://10.112.68.186:8081]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库Cassandra监控]]></title>
    <url>%2Fnosql%2Fcassandra-monitor-20171206.html</url>
    <content type="text"><![CDATA[使用packetbeat、elasticsearch、logstash、kibana为cassandra搭建监控系统。 创建用户123groupadd add esuseradd -g es espasswd es ES安装下载123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.zipunzip elasticsearch-6.0.0.zip./bin/elasticsearch-plugin install x-pack 修改配置vim config/elasticsearch.yml(未使用集群)1network.host: 0.0.0.0 设置内置用户密码(需要启动)1234bin/x-pack/setup-passwords interactiveelastic：elastickibana：kibanalogstash_system：logstash_system 生成随机密码，建议生产环境使用1bin/x-pack/setup-passwords auto 启动：1./bin/elasticsearch -d 验证浏览器中http://10.112.68.192:9200/，输入用户名密码(elastic：elastic)登录，检查es安装是否成功。 kibana安装下载123wget https://artifacts.elastic.co/downloads/kibana/kibana-6.0.0-linux-x86_64.tar.gztar zxvf kibana-6.0.0-linux-x86_64.tar.gzbin/kibana-plugin install x-pack 配置vim config/kibana.yml1234567server.port: 5601server.host: "10.112.68.192"server.name: "es-kibana"elasticsearch.url: "http://localhost:9200"ibana.index: ".kibana"elasticsearch.username: "elastic"elasticsearch.password: "elastic" 启动1nohup ./bin/kibana &amp; 浏览器中输入http://10.112.68.192:5601/ 输入用户名密码登录(kibana：kibana，kibana权限不够请使用elastic用户设置)，检查kibana安装是否成功。 Packetbeat安装Packetbeat 是 Elastic 开源的网络流量实时监控工具，目前支持了一些流行的应用软件，如MongoDB、Redis、MySQL、Cassandra等。 下载12wget https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-6.0.0-linux-x86_64.tar.gzsudo yum install libpcap 配置vim packetbeat.yml(其他数据收集关掉，只开启cassandra)12345678910111213packetbeat.interfaces.device: any- type: cassandra ports: [9042]setup.dashboards.enabled: truesetup.kibana: host: "10.112.68.192:5601" username: "kibana" password: "kibana"output.elasticsearch: # Array of hosts to connect to. hosts: ["10.112.68.192:9200"] username: "elastic" password: "elastic" 启动1nohup sudo ./packetbeat -e -v &amp; 验证编写JAVA测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144package com.demo.simple;import com.datastax.driver.core.Cluster;import com.datastax.driver.core.ResultSet;import com.datastax.driver.core.Row;import com.datastax.driver.core.Session;/** * Created by luoji on 04/12/2017. */public class CreateAndPopulateKeyspace &#123; static String[] CONTACT_POINTS = &#123;"10.112.68.186","10.112.68.192"&#125;; static int PORT = 9042; public static void main(String[] args) &#123; CreateAndPopulateKeyspace client = new CreateAndPopulateKeyspace(); try &#123; client.connect(CONTACT_POINTS, PORT); client.createSchema(); client.loadData(); client.querySchema(); &#125; finally &#123; client.close(); &#125; &#125; private Cluster cluster; private Session session; /** * Initiates a connection to the cluster * specified by the given contact point. * * @param contactPoints the contact points to use. * @param port the port to use. */ public void connect(String[] contactPoints, int port) &#123; cluster = Cluster.builder() .addContactPoints(contactPoints).withPort(port) .build(); System.out.printf("Connected to cluster: %s%n", cluster.getMetadata().getClusterName()); session = cluster.connect(); &#125; /** * Creates the schema (keyspace) and tables * for this example. */ public void createSchema() &#123; session.execute("CREATE KEYSPACE IF NOT EXISTS simplex WITH replication " + "= &#123;'class':'SimpleStrategy', 'replication_factor':1&#125;;"); session.execute( "CREATE TABLE IF NOT EXISTS simplex.songs (" + "id uuid PRIMARY KEY," + "title text," + "album text," + "artist text," + "tags set&lt;text&gt;," + "data blob" + ");"); session.execute( "CREATE TABLE IF NOT EXISTS simplex.playlists (" + "id uuid," + "title text," + "album text, " + "artist text," + "song_id uuid," + "PRIMARY KEY (id, title, album, artist)" + ");"); &#125; /** * Inserts data into the tables. */ public void loadData() &#123; session.execute( "INSERT INTO simplex.songs (id, title, album, artist, tags) " + "VALUES (" + "756716f7-2e54-4715-9f00-91dcbea6cf50," + "'La Petite Tonkinoise'," + "'Bye Bye Blackbird'," + "'Joséphine Baker'," + "&#123;'jazz', '2013'&#125;)" + ";"); session.execute( "INSERT INTO simplex.playlists (id, song_id, title, album, artist) " + "VALUES (" + "2cc9ccb7-6221-4ccb-8387-f22b6a1b354d," + "756716f7-2e54-4715-9f00-91dcbea6cf50," + "'La Petite Tonkinoise'," + "'Bye Bye Blackbird'," + "'Joséphine Baker'" + ");"); &#125; /** * Queries and displays data. */ public void querySchema() &#123; ResultSet results = session.execute( "SELECT * FROM simplex.playlists " + "WHERE id = 2cc9ccb7-6221-4ccb-8387-f22b6a1b354d;"); System.out.printf("%-30s\t%-20s\t%-20s%n", "title", "album", "artist"); System.out.println("-------------------------------+-----------------------+--------------------"); for (Row row : results) &#123; System.out.printf("%-30s\t%-20s\t%-20s%n", row.getString("title"), row.getString("album"), row.getString("artist")); &#125; &#125; /** * Closes the session and the cluster. */ public void close() &#123; if (session == null) &#123; return; &#125; session.close(); cluster.close(); &#125;&#125; 浏览器中查看，可以看到刚才对cassandra所做的操作。]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库Cassandra数据模型]]></title>
    <url>%2Fnosql%2Fcassandra-data-model-20171205.html</url>
    <content type="text"><![CDATA[使用NoSQL存储数据和关系型数据库不一样，关系型数据库是行进行组织数据的。取而代之，应该把它想象成事一个有序的map结构。考虑它是一个map中嵌入另一个map：外部map的key为row key，内部map的key为column key，两个map的key都是有序的。 区分频次大的查询和频次小的查询，有些查询可能只被查询几千次，其它可能被查询数十亿次； 还要考虑哪些查询对数据延迟是敏感的。确保你的模型优先满足查询频次大的查询和重要查询。 反范式化来提升查询性能 示例是关于电子商务系统的一个功能，一个user可以喜欢多个item，同时一个item可以被多个user所喜爱，在关系型数据库中这个关系是通过many-to-many实现的。 用户表 UserID Name Email u1 jerrylou jerrylou@gmail.com u2 gunsluo gunsluo@gmail.com Item表 ItemID Title Desc i1 mac pro mac book i2 ipad mac tablet 关系表 ID UserID ItemID Timestamp 1 u1 i1 1512099720 2 u1 i2 1512109720 3 u2 i1 1512119720 关联表查询 通过user id获取user 通过item id获取item 获取指定user喜欢的所有item 查看指定item被那些user所喜爱 按照关系数据库模型设计数据模型 用户表 Row Key(UserID)Column Family Name Emailu1 jerrylou jerrylou@gmail.com u2 gunsluo gunsluo@gmail.com Item表 Row Key(ItemID) Column Family Title Desc i1 mac pro mac book i2 ipad mac tablet 关系表 Row Key(ID) Column Family UserID ItemID Timestamp 1 u1 i1 1512099720 2 u1 i2 1512109720 3 u2 i1 1512119720 这个模型支持通过user id查询user和通过item id查询item。但无法简单查询某个user喜爱的所有item或者某个item被那些user所喜爱。 范式化实体，并将它们反范式化到自定义索引无法查询的原因是按关系表的进行设计。实体用户表和Item表设计同上。关系表修改为： User_By_Item表（CF） Row Key(ItemID) Column Family u1 u2 i1 jerrylou gunsluo i2 jerrylou Item_By_User表（CF） Row Key(UserID) Column Family i1 i2 u1 mac pro ipad u2 mac pro 通过所给item id，获取具体item信息（title, desc等等），并一同查询喜欢这个item的user name(反范式) 通过所给的user id，获取具体user信息，并一同查询user喜欢的所有item titile(反范式) 使用composite column之前忽略了timestamp，使用timestamp和userid【或ItemID】合并为一个composite column key，这样就可以按时间进行排序了。 User_By_Item表（CF） Row Key(ItemID) Column Family 1512099720|u1 1512109720|u1 1512119720|u2 i1 jerrylou gunsluo i2 gunsluo Item_By_User表（CF） Row Key(UserID) Column Family 1512099720|i1 1512109720|i2 1512119720|i1 u1 mac pro ipad u2 mac pro 按照具体的查询需求设计数据模型。]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>cassandra</tag>
        <tag>data model</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库Cassandra集群部署]]></title>
    <url>%2Fnosql%2Fcassandra-install-20171204.html</url>
    <content type="text"><![CDATA[在Linux系统部署NoSQL数据库Cassandra集群。 系统环境 OS ：CentOS Linux release 7.3.1611 (Core) CPU：Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz 8核 内存：16G JDK: 1.8u151 Python: 2.7.5 Cassandra: 3.11.1 服务器 描述 10.112.68.186 节点1 10.112.68.191 节点2 10.112.68.192 节点3 环境准备JDK安装下载地址: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 解压放到/usr/java/jdk1.8.0_151/目录 添加用户123groupadd cassandrauseradd -g cassandra cassandrapasswd cassandra 切换用户su cassandra，配置环境变量vim ~/.bashrc12345678JAVA_HOME=/usr/java/jdk1.8.0_151/JRE_HOME=/usr/java/jdk1.8.0_151/jrePATH=.:$JAVA_HOME/bin:$JRE_HOME/bin:$PATHCLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport JAVA_HOME JRE_HOME PATH CLASSPATHCASSANDRA_HOME=$HOME/apache-cassandra-3.11.1export CASSANDRA_HOME 部署下载123wget http://mirrors.shuosc.org/apache/cassandra/3.11.1/apache-cassandra-3.11.1-bin.tar.gztar -xvf apache-cassandra-3.11.1-bin.tar.gzcd apache-cassandra-3.11.1 配置vim conf/cassandra.yaml1234cluster_name: 'JCPT Test Cluster'- seeds: "10.112.68.186,10.112.68.192"listen_address: 10.112.68.186rpc_address: 10.112.68.186 不同节点listen_address、rpc_address不同，cluster_name和seeds是相同的。 JVM配置: conf/cassandra-env.sh（JVM_OPTS） 日志配置：conf/logback.xml 建立数据和日志的存储目录（生产环境数据和日志放在不同分期）123456mkdir datamkdir data/datamkdir data/commitlogmkdir data/saved_cachesmkdir data/hintsmkdir logs 启动12./bin/cassandra -f (前端启动)./bin/cassandra 测试12bin/cqlshSELECT cluster_name, listen_address FROM system.local; 参考：http://cassandra.apache.org/doc/latest/configuration/cassandra_config_file.html]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL数据库对比]]></title>
    <url>%2Fnosql%2Fnosql-compare-20171106.html</url>
    <content type="text"><![CDATA[本文主要对主流NoSQL进行对比，从各个方面进行对比，意在不同场景下选择正确的 NoSql 数据库。本文主要对比HBase、Cassandra、MongoDB、LevelDB、DoltDB。 集群 数据一致性 事务 扩展能力 HBase 支持 强一致性 不支持 动态扩容 Cassandra 支持 可调节 轻量级 虚拟节点,数据迁移 LevelDB 不支持 BoltDB 不支持 MongoDB Replica Set，Master-Slave 强一致性 支持 命名扩容 存储模型 NoSQL类型 语言 描述 公司 HBase HDFS column-oriented JAVA 实时数据查询 google Cassandra CommitLog Memtable SSTable column-oriented JAVA CQL支持 facebook LevelDB Log Memtable SSTable key-value 多写少读 内嵌数据库 开源 BoltDB 单文件 key-value Golang 内嵌数据库 开源 MongoDB 文件 document-oriented C/C++ 文档存储 开源 HBase数据库在HRegionServer宕机恢复需要时间长。]]></content>
      <categories>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>cassandra</tag>
        <tag>nosql</tag>
        <tag>hbase</tag>
        <tag>leveldb</tag>
        <tag>boltdb</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7搭建shadowsocksrrs privoxy, 支持Chacha20加密]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fcentos-shadowsocksrrs-privoxy-20170821.html</url>
    <content type="text"><![CDATA[shadowsocksrrs和shadowsocks-libev是shadowsocks之后的两分支。window和mac os系统上使用shadowsocks-libev的图形客户端进行翻墙，它安装和配置简单，这里就不在介绍。本文主要对linux服务器如何翻墙进行介绍，下面以Centos7为例。 安装shadowsocksrrsPip 是 Python 的包管理工具，这里我们用 pip 安装 shadowsocks。 使用 yum install -y pip 安装，官方一个最小化的 CentOS，没有这个包，可以手动安装。 12curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"python get-pip.py 安装客户端 12pip install --upgrade pippip install shadowsocks 配置客户端 12mkdir -p /etc/shadowsocksvim config.json 配置如下12345678&#123; "server":"server_ip", "server_port":4004, "local_port":8016, "password":"password", "timeout":600, "method":"aes-256-cfb"&#125; 启动客户端 1sslocal -c /etc/shadowsocks/config.json 使用systmed 新建文件/etc/systemd/system/ssclient.service1vim /etc/systemd/system/ssclient.service 文件中内容123456789[Unit]Description=Shadowsocks Client[Service]TimeoutStartSec=0ExecStart=/usr/bin/sslocal -c /etc/shadowsocks/config.json[Install]WantedBy=multi-user.target 启动客户端 1systemctl start ssclient 客户端状态1systemctl status ssclient 开启启动客户端 1systemctl enable ssclient 好了，shadowsocksrrs客户端安装完成，由于pip安装的客户端不支持chacha20等加密方式，要支持这些加密方式，需要下载源码。 支持chacha20加密方式安装libsodium 1234567wget https://download.libsodium.org/libsodium/releases/LATEST.tar.gztar zxvf LATEST.tar.gzcd libsodium-1.0.13./configuremake -j8 &amp;&amp; make installecho /usr/local/lib &gt; /etc/ld.so.conf.d/usr_local_lib.confldconfig 下载源码由于shadowsocksrrs作者别请去喝茶，代码已经删除。自己备份了一份。 12git clone https://github.com/gunsluo/shadowsocksrrs.gitcd shadowsocksrrs/shadowsocks/ 可以看到目录下有local.py文件，它就是客户端入口函数文件。 修改配置文件 1vim /etc/shadowsocks/config.json 配置如下123456789101112&#123; "dns_ipv6": false, "server": "server_ip", "server_port": 6003, "local_address": "127.0.0.1", "local_port": 8086, "password": "password", "timeout": 600, "method": "chacha20", "protocol": "auth_aes128_md5", "obfs": "tls1.2_ticket_auth"&#125; 客户端加密配置必须和服务器端相同，根据自己的需求做响应修改。 启动客户端 在下载的源码目录下：1python ./local.py -c /etc/shadowsocks/config.json 编写启动脚步ssclient123#!/bin/bash/root/shadowsocksrrs/shadowsocks/local.py -c /etc/shadowsocks/japan.json -d $1 mv ssclient /usr/bin/ 启动客户端1ssclient start 测试安装shadowsocksrrs测试命令：curl --socks5 127.0.0.1:8086 http://httpbin.org/ip 返回vpn服务器ip，说明安装成功。123&#123; "origin": "x.x.x.x"&#125; 安装privoxy1yum install privoxy 编辑 vim /etc/privoxy/config，确保下面两项。 12listen-address 127.0.0.1:8016forward-socks5t / 127.0.0.1:8086 . 启动privoxy1/usr/sbin/privoxy --user privoxy /etc/privoxy/config 配置终端使用代理的命令在~/.zshrc 或者~/.bashrc中加入：1234567function setproxy() &#123; export &#123;http,https,ftp&#125;_proxy="http://127.0.0.1:8016"&#125;function unsetproxy() &#123; unset &#123;http,https,ftp&#125;_proxy&#125; 重新登录终端，setproxy开始代理，unsetproxy关闭代理。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>shadowsocksrrs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IM即时通讯系统业务流程]]></title>
    <url>%2Fim%2Fim-system-workflow-20170705.html</url>
    <content type="text"><![CDATA[上篇文件讲述了IM系统的整体架构(查看)，对细节的业务流程没有详细的说明。这篇文章主要梳理消息的流转过程，也就是用户A发送消息给客户B，消息是如何到达用户B的。主要流程有消息发送，消息推送，心跳消息 首先对上文中没提到的概念进行说明： Session 用户在一定时间的有效会话服务。 Room 用户对用户发送的消息(个人消息、群消息、讨论组消息)，其实就是一个发布订阅过程，发布者将消息发送到room，订阅者从room中取走消息，对于这样的上下文叫room，有些IM系统中将个人消息的上下文叫channel，这里我统一叫room，只不过个人消息是一个订阅者的room。 Msg Queue 未读消息队列 hearbeat 心跳服务器，之前包括在了logic中。 caller 消息推送触发者，可能是hearbeat，可能是logic。 sequence 消息序列号生成器 消息发送流程用户A发送消息hello给用户B为例。 用户A使用终端登录客户端，用户A登录需要验证用户身份（用户权限体系，这里不多说）。登录成功服务器会返回用户唯一标识uid给客户端，同时返回用户A的好友列表（用户B当然就在这个列表中了），之后开始发送心跳消息（这个流程下面讲） 用户A使用客户端发送hello消息。客户端组装消息报文msg request，消息体包括：uid、消息内容、发送消息时间、发送设备ID、Room ID等。将msg request发送到transfer接入服务，然后等待ack响应。 tranfer接收到客户端发送的信息后，使用token(uid)去logic中得到连接描述key。 logic服务收到消息会做几件事情，A. 返回token(uid)的key(uid+roomid) B. 通知router更新用户状态(uid)，C. 通知router更新用户接入信息，uid、transferid、更新时间 tranfer接收到logic返回key，以key保存连接，带上transferid将消息分发到logic服务，等待logic服务处理。 logic服务收到消息会做几件事情，D. 排重，根据uid、消息发送时间、设备ID在缓存中查下是否存在。 logic服务收到消息会做几件事情，E. 生成msgid，以uid + transferid + roomid取个最新的msgid，自增+1或者用snake算法得到自增的msgid。 logic服务收到消息会做几件事情，F. 保存消息 G. 响应ack ack报文: uid、Room ID、msgid。 transfer收到ack后，把ack响应返回给客户端。 客户端收到ack报文，更新本地信箱，信箱中有msgid的消息即为消息发送成功。 到此用户A的消息就发送出去了，这好像个用户B没有上面关系呀，接下来说消息推送流程。注意 上面流程中没有说明Room ID是如何生成和分配的，通常是按照具体业务场景进行生成和分配，如按用户A的好友列表进行维护。 消息推送流程用户B如何收到消息呢，首先用户B已经登录客户端（这里不讨论离线消息）。 logic服务获取Room ID下的所有用户。(维护用户A好友列表的服务可以提供) logic服务去router中用户是否在线，并获取在线用户的transferid。 logic服务更新transferid将消息推送到对应的tranfer。 logic服务将此消息加入到未读消息队列Msg Queue。 tranfer收到消息根据uid找到对应连接将其发送到客户端。 客户端接收到消息，将消息保存到本地信箱，按msgid排序展示给用户B。 客户B终于收到了消息了，真的一定收到了吗？ transfer推送了消息，没有确认机制保证消息一定到达，这过程可能出现网络问题，也可能客户B异常。所以我们还需要一种机制保证客户的正确的在线状态和消息拉取确认机制。 心跳消息流程心跳消息是客户端登录成功后发起的，心跳时长根据不同的网络环境设置不同值。 用户A使用客户端发送心跳消息。客户端组装消息报文hearbeat request，消息体包括：uid、网络类型、发送消息时间、发送设备ID、所有Room ID的最后的msgid(确保机制)。将hearbeat request发送到transfer接入服务，然后等待ack响应。 tranfer接收到客户端发送的心跳，带上transferid将消息分发到hearbeat服务，等待hearbeat服务处理。 hearbeat服务收到消息会做几件事情，A. 通知router更新用户状态(uid)，B. 通知router更新用户接入信息，uid、transferid、更新时间 hearbeat服务收到消息会做几件事情，C. Room ID在sequence查询最新的msgid D. 查询的msgid不等于上报的msgid，发送消息通知logic服务启动消息推送流程(uid、Room ID、上报的msgid) logic服务收到消息，去未读消息队列中取所有大于上报msgid的消息，在到数据存储层取得消息的全部内容，将消息发送到transfer。 tranfer收到消息根据uid找到对应连接将其发送到客户端。 客户端接收到消息，将消息保存到本地信箱，按msgid排序展示给用户B。 hearbeat服务收到消息会做几件事情，E. 响应hearbeat ack，如果需要调整心跳时长，可以将心跳时长加入到ack响应中。 tranfer收到消息根据uid找到对应连接将其发送到客户端。 客户端接收到消息，更新自己的心跳时长。 hearbeat心跳时间间隔根据不同的网络环境，心跳发送成功次数，信息发送次数等因素进行动态调整。 长连接维护长连接的TCP服务器与客户端通讯: client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。下面是用Golang编写的tcp服务端与客户端，客户端每隔一段时间发送消息到服务器，服务器响应回复。代码conn, err = lis.AcceptTCP();中的conn是需要维护的连接，transfer中确保uid与conn的关系，可以使用map[string]*net.TCPConn。 服务端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package mainimport ( "bufio" "log" "net" "time")var ( Debug = true maxInt = 1&lt;&lt;31 - 1)func main() &#123; InitTCP([]string&#123;"0.0.0.0:9999"&#125;) select &#123;&#125;&#125;func InitTCP(addrs []string) (err error) &#123; var ( bind string listener *net.TCPListener addr *net.TCPAddr ) for _, bind = range addrs &#123; if addr, err = net.ResolveTCPAddr("tcp4", bind); err != nil &#123; log.Printf("net.ResolveTCPAddr(\"tcp4\", \"%s\") error(%v)", bind, err) return &#125; if listener, err = net.ListenTCP("tcp4", addr); err != nil &#123; log.Printf("net.ListenTCP(\"tcp4\", \"%s\") error(%v)", bind, err) return &#125; if Debug &#123; log.Printf("start tcp listen: \"%s\"", bind) &#125; go acceptTCP(listener) &#125; return&#125;func acceptTCP(lis *net.TCPListener) &#123; var ( conn *net.TCPConn err error r int ) for &#123; if conn, err = lis.AcceptTCP(); err != nil &#123; // if listener close then return log.Printf("listener.Accept(\"%s\") error(%v)", lis.Addr().String(), err) return &#125; if err = conn.SetKeepAlive(false); err != nil &#123; log.Printf("conn.SetKeepAlive() error(%v)", err) return &#125; if err = conn.SetReadBuffer(256); err != nil &#123; log.Printf("conn.SetReadBuffer() error(%v)", err) return &#125; if err = conn.SetWriteBuffer(2048); err != nil &#123; log.Printf("conn.SetWriteBuffer() error(%v)", err) return &#125; go serveTCP(conn, r) if r++; r == maxInt &#123; r = 0 &#125; &#125;&#125;func serveTCP(conn *net.TCPConn, r int) &#123; var ( // ip addr lAddr = conn.LocalAddr().String() rAddr = conn.RemoteAddr().String() rbuf = make([]byte, 1024) ) if Debug &#123; log.Printf("start tcp serve \"%s\" with \"%s\"", lAddr, rAddr) &#125; reader := bufio.NewReader(conn) for &#123; n, err := reader.Read(rbuf) //message, err := reader.ReadString('\n') if err != nil &#123; return &#125; message := rbuf[0:n] log.Printf("read message (%s)", string(message)) reply := time.Now().String() conn.Write([]byte(reply)) &#125;&#125; 客户端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( "bufio" "fmt" "net" "time")var quitSemaphore chan boolfunc main() &#123; var tcpAddr *net.TCPAddr tcpAddr, _ = net.ResolveTCPAddr("tcp", "192.168.0.9:9999") conn, _ := net.DialTCP("tcp", nil, tcpAddr) defer conn.Close() fmt.Println("connected!") go onMessageRecived(conn) b := []byte("time") conn.Write(b) &lt;-quitSemaphore&#125;func onMessageRecived(conn *net.TCPConn) &#123; var rbuf = make([]byte, 1024) reader := bufio.NewReader(conn) value := 5 incr := 10 for &#123; n, err := reader.Read(rbuf) if err != nil &#123; quitSemaphore &lt;- true break &#125; msg := rbuf[0:n] fmt.Println(string(msg)) time.Sleep(time.Duration(value) * time.Second) conn.Write([]byte(msg)) value += incr &#125;&#125; 总结重新梳理了消息流转流程，基本清楚了消息的轮转，对Room ID的生成和分配还需进行研究。]]></content>
      <categories>
        <category>im</category>
      </categories>
      <tags>
        <tag>im</tag>
        <tag>即时通讯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IM即时通讯系统架构设计]]></title>
    <url>%2Fim%2Fim-system-architecture-20170628.html</url>
    <content type="text"><![CDATA[即时通信（Instant Messaging，简称IM）是一种通过网络进行实时通信的系统，允许两人或多人使用网络即时的传递文字消息、文件、语音与视频交流。微信、QQ基本占领的IM即时通讯系统的半个江湖，易信、钉钉、飞信、旺旺、咚咚、陌陌也各有市场。查询了相关资料后设计了这篇高并发IM通讯系统，不当之处，请指正。(最终架构图在总结章节) IM不就是聊天工具么? 两个用户（Client），一个Server转发聊天内容就可以完成聊天功能。最早的聊天室就是这样设计开发的。问题在于用户量不断上涨，一台服务器无法应对成千上万的用户请求。如何接入这么多用户，做到高并发？这就是今天要解决的问题，首先应该对系统进行分层，细化功能点确定每层需要完成的功能。IM系统的三层架构(不包括client)： 接入层: 用户的连接接入(协议是tcp、udp、http、socketio等，是短链接、长链接。)，接受客户端发送的消息，推送消息到客户端。 逻辑层: 接收接入层接入的消息，对消息进行校验，去重，验证，过滤等业务操作，存储消息内容，本层服务是无状态的，容易扩展。 存储层: 处理逻辑层注册的消息，主要维护消息发送者客户端状态功能，保存发送者接入路由信息（也就是从哪个机器接入，最新的msgid是多少等信息）。本层服务有状态的，设计时应该关注本层的扩展性。 我给这三层对应服务的命名：接入层服务叫tranfer（客户端连接管理，接受转发推送消息模块），逻辑层服务叫logic(业务逻辑模块)，存储层服务router（用户在线状态，用户接入路由信息保存等） 消息轮转流程我接着了解下IM系统的消息轮转流程。首先客户发送信息流程 客户端连接上tranfer，发送消息到tranfer服务，这个消息请求中包括用户UID、消息内容、发送消息时间、发送设备ID等。然后等待ack响应。 tranfer接收到客户端发送的信息后，将消息转发到logic服务。 logic为消息生成msgid(通常由uid,deviceid,发送时间hash再加上自增id得到，这样msgid就是有序的)，logic通过msgid查询消息是否已经发送进行排重，重复响应tranfer消息重复发送。 logic将消息注册到router，更新发送消息用户的在线状态，保存消息的来源（来自哪个tranfer，相当于路由表），把消息来源存储到cache/db。完成后，router给logic服务ack响应。 logic收到ack后，把ack响应给tranfer服务。 tranfer将此带msgid的响应ack返回给客户端，客户端收到ack，即可确定消息发送成功。 客户端发送消息成功，接着就是另一边的客户端接收消息，也就是消息投递。这里需要说明的是客户A发送消息到客户B（群B），他们之间就建立了会话，我们叫它sessoin(有些叫channel，room【群】)。 logic从router服务获取session上发送到接收者的用户列表。 logic从route服务查找出用户列表中所有在线用户的tranfer地址。 logic将消息发送到对应的tranfer地址。 tranfer收到消息并将其发送到客户端。 最终客户端接收到消息内容。 客户端接收消息OK了。但是有个问题，如果用户在线却一直不发送信息，怎么判断他在线呢？所以客户端应该定时发送心跳包给tranfer，以告知transfer客户端的存在，这就是hearbeat。 客户端发送hearbeat到tranfer，hearbeat包括用户UID、发送消息时间、发送设备ID外，还用户所在的所有session和在session中的最新msgid。 tranfer转发hearbeat到logic logic到router更新发送消息用户的在线状态，保存消息的来源（来自哪个tranfer，相当于路由表）。 logic到router查询session的最新msgid，如果上报msgid不等于查询msgid，启动下发消息流程。 logic下发未读消息到客户端。 hearbeat心跳时间间隔根据不同的网络环境，心跳发送成功次数，信息发送次数等因素进行动态调整。 了解了IM的消息轮转流程和分层结构，可以给出一个粗略设计图。 图中为IM系统进行了分层，每层服务有多点（还没有负载均衡方案，当可以同时提供多点服务），对于数据存储可以使用cache/db两层模式，cache提供读取性能，db用户持久化。在推送下发消息使用消息队列，可以做到解耦、异步、高并发。 高可用图中三层中的服务节点扩展为多节点，不同的客户端连接不同的节点以达到接入能力的提升。接入能力提升，但每个节点还存在着单点问题。如何解决单点问题，达到高可用? 接入层的tranfer服务是客户端的接入口，使用长链接时，服务要保持链接不中断。tranfer是无状态，但要保持本次连接状态（本次连接断开可以连接其它tranfer）。 对于不分区域的接入情况，可以使用负载均衡器将用户请求分配到不同的tranfer上，如果有一台服务宕机，负载均衡器将该节点移除。这样会出现该节点所有连接会断开一次，客户端重连，再由负载均衡器分配到其他节点上。缺点是宕机会加大其他机器的负荷。（负载均衡的软实现有lvs+keepalived，nginx upstream，haproxy + pacemaker，硬负载有f5、array。根据业务情况选择实现)。 对于不分区域的接入情况，对于单点宕机时可不可以不增加其它节点负荷。答案是YES，使用transfer服务主备方案，主transfer服务宕机，备接替工作。由于这里使用tcp长连接，主节点的连接还是会中断一次，还好客户端有重连机制，重连连接会到备节点上。transfer的主备切换如何实现呢？有两种方式，第一种：使用vrrp协议 + 接口监视，第二种：使用下面会将到的monitor监控通知。 主备方案缺点很明显，有一半的备节点处于空闲状态，资源浪费严重，优点是主备方案实施相对其它要简单些。在前期（业务快速增长期）使用，暂时不考虑节约成本问题。 分区域接入场景，对于来自不同地区的用户，地区之间的网络差异明显。对于这样情况，应该按区域部署的transfer服务，不同IDC部署的transfer服务。那问题来了，客户端如何选择接入哪个(几个)transfer呢？通常增加查询transfer地址列表接口，通过客户端ip的到用户区域，返回transfer地址列表，客户端再尝试在地址列表中选择最优地址。 通过以上策略就能是接入层transfer高可用，接下来分析逻辑层。接入层、逻辑层、存储层的通讯通常采用RPC。逻辑层logic服务是无状态，每个节点都可以处理来自transfer的消息。 最简单的方式就是使用nginx upstream做转发达到负载的功能，logic接收nginx转发消息。需要注意的是nginx本身也需要搭建集群解决单点问题。 增加moniter服务监控transfer、logic、router的健康情况，transfer、logic、router启动会定时向monitor发送心跳hearbeat，当monitor监控到会logic有节点出现故障，则推送新logic地址列表到transfer，transfer更新本地logic地址列表。transfer使用新的logic地址发送消息。这里存在logic宕机，transfer未及时更新logic列表任然将数据发送到故障节点的问题，所以transfer必须有重试机制和重试队列，重试策略与moniter的心跳发送频率有关，具体策略也按实际策略而定。moniter从设计上是无状态，集群的，集群中每个节点都有相同配置。优点从业务角度做负载均衡，可以自定义易扩展，缺点是增加了服务的耦合性。需要强调的无论如何都需要moniter监控服务，通知transfer更新地址列表接口增加了耦合性。 剩下存储层高可用问题，router服务是存储用户的在线状态，服务本身就是有状态的。 logic通过一致性hash算法将消息散列到不同的router，当有节点故障时，散列该节点的新消息转发到其他节点，当老消息中处理的消息就无法处理。并且节点回复正常是，还需要做数据迁移。（可作为前期版本过度） 基于上面的问题，可以使用router主备策略，logic转发的消息同时发送到主备，当只有主工作，当主宕机后，moniter（或logic）感知到router宕机，提升备为主。同时告警节点故障，恢复节点是，从运行节点中恢复数据。优点是解决了高可用，最大的缺点是一致性hash使扩容需要迁移数据。 修改后的架构图。 服务扩容随着用户的增长，现有机器无法承载是就需要进行服务扩容，怎样才能快捷方便的进行服务扩容呢？能否做到auto-scaling？上面讲到接入层上层使用的负载均衡器，当transfer增加节点服务时，启动新节点后，将节点信息配置到负载均衡器并加载。假如负载策略是平均分配，新接入用户就会分配到新的transfer，直到每个节点接入量平衡。当然也可以按其它策略进行分配，例如A机器配置高可以承载更多的接入量，就可以调整分配策略分配更多接入到A。 对于逻辑层而言，增加logic节点，logic注册到monitor，monitor更新本地logic地址列表，通知transfer服务logic地址已改变，最后transfer调整发送策略完成扩展。 存储层服务扩容，比较麻烦，由于之前使用了一致性hash算法，增加删除节点都需要进行数据迁移，扩展能力相对差些。一致性hash算法是根本问题所在，那我们使用逻辑层的monitor + 主备方式可以吗？monitor方式的问题是请求随机发送到router服务上，而不是用户不变时都请求同一服务，这样会大大降低cache的命中率，降低系统性能。从业务场景出发，router是维护用户状态的，用户数量是有限的，是可以提前预告在线和压力的。微信架构中对于相关设计是单点容灾策略，他整个系统又按用户uid范围进行分Set，每个Set都是一个完整的、独立的子系统。分Set设计目的是为了做灾难隔离，一个Set出现故障只会影响该Set内的用户，而不会影响到其它用户。他使用仲裁节点（类似monitor）判断节点是否正常，再配合嵌入式路由表，将宕机节点请求转移到其它节点。嵌入式路由表是核心，这里就不说原理了，它就是通过维护client(我们这里的logic)与查询服务(我们这里的router)的路由信息一致，其实就是通过在每个报文中带上路由信息，在配合仲裁节点可动态修改路由表，以达到节点的增加删除切换。 结论：使用一致性hash算法和预估压力就能满足打部分要求，使用这种方案做好节点的数据迁移（其实不用迁移，cache重新命中就可以）。 后期可以考虑嵌入式路由表节约成本和auto-scaling. 统计、监控与配置为了时刻知道线上服务的状态，做到提前预警，整个系统还应该有监控。其次，业务数据的统计量也很重要，知道我们下一步该业务中心，以及可能产出的瓶颈。通过这些数据，可能需要去修改服务配置、启动新服务等，配置管理与下发同样重要。 监控：agent，监控服务所属服务器的状态（CPU、MEM、IO等），核心进程状态（logic、router等），通常核心进程状态监控不应用到生产环境，开启会影响性能。如有必要才开启 统计：statis，统计点数据上报，业务数据和非业务数据都可以。 配置: etcd，配置下发功能。 序列号生成器在消息轮转流程中提到msgid的生成，要求是msgid是线性递增的。序列号生成器需要再系统中独立存在运行。IM中通常按用户来自增长的，可以使用redis的inrc来简单实现。 富文本消息对于图片、声音、视频的消息，在IM系统中，客户端本地上传富文本到文件服务器，文件服务器返回url，客户端再将url作为普通消息发送（文本类型要标识），接收端收到消息后，再去服务器上下载。 文件服务器可以选择第三方服务，自建建议使用ceph对象存储服务。 网络传输协议IM系统传输使用UDP、TCP、基于TCP的http这几种协议中的一种或几种。 UDP协议实时性更好，但是如何处理安全可靠的传输并且处理不同客户端之间的消息交互是个难题； TCP协议安全可靠的，如何保证单机服务器高并发量，如何做到灵活，扩展的架构。业界选择TCP居多，建议选择TCP。 数据传输格式对传输的数据应该进行压缩，安全性处理。基于这些要求，选择probuffer是再合适不过了(建议)。当然json格式也是可以的选择，json没有压缩报文，但清晰明了。 跨区域网络问题目前想到的只能走IDC机房专线。 总结下图是对IM系统认知的总结，总体来说我对im系统还不太了解，还需要继续研究学。 参考项目 gopush-cluster goim actor.im]]></content>
      <categories>
        <category>im</category>
      </categories>
      <tags>
        <tag>im</tag>
        <tag>即时通讯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP BBR拥塞控制算法，为你的网络提速！]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2Ftcp-bbr-20170509.html</url>
    <content type="text"><![CDATA[TCP BBR是谷歌出品的TCP拥塞控制算法。BBR目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR可以起到单边加速TCP连接的效果。替代锐速再合适不过，毕竟免费。 Google提交到Linux主线并发表在ACM queue期刊上的TCP-BBR拥塞控制算法。继承了Google“先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR已经再YouTube服务器和Google跨数据中心的内部广域网(B4)上部署。由此可见出该算法的前途。 TCP-BBR的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR解决了两个问题： 再有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的buffer占用率，从而降低延迟。非常适合慢速接入网络的用户。 项目地址:https://github.com/google/bbr 安装BBRBBR是内嵌在Linux内核中的，目前Linux Kernel 4.9已加入了该算法，所以安装新版本内核开启BBR即可享用。 Debian/Ubuntu下面简单讲述如何在Debian/Ubuntu 64bit系统中升级kernel开启TCP BBR拥塞控制算法。 下载最新内核最新内核查看这里：http://kernel.ubuntu.com/~kernel-ppa/mainline/1234$ cd ~;mkdir linux49; cd linux49$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9/linux-headers-4.9.0-040900-generic_4.9.0-040900.201612111631_amd64.deb$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9/linux-image-4.9.0-040900-generic_4.9.0-040900.201612111631_amd64.deb$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9/linux-headers-4.9.0-040900_4.9.0-040900.201612111631_all.deb 开始安装 1$ dpkg -i *.deb 以上用于64位系统，其它可以自行下载Index of /~kernel-ppa/mainline/v4.9 对应版本。 删除其余内核(非必需) 12$ dpkg -l|grep linux-image $ apt-get remove linux-image-[Tab补全] #删旧内核，在这里，就是把第一个删掉 更新grub系统引导文件并重启 1$ update-grub 重启系统并查看内核 12$ reboot$ uname -a Centos/RHEL通过使用ELRepo源的方式在CentOS中安装最新版kernel。 CentOS 6 下载内核并安装 123$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org$ rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm$ yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel -y 查看内核是否安装成功 1$ rpm -qa | grep kernel 更新grub系统引导文件并重启 12$ sed -i 's:default=.*:default=0:g' /etc/grub.conf$ reboot CentOS 7 下载内核并安装 123$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org$ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm$ yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel -y 查看内核是否安装成功 1$ rpm -qa | grep kernel 更新grub系统引导文件并重启 1$ egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \' #删除其余内核(非必需) $ grub2-set-default 0 #default 0表示第一个内核设置为默认运行, 选择最新内核就对了 $ reboot Google TCP BBR一键安装脚本适用于Centos6 32位和64位 12$ wget --no-check-certificate https://github.com/52fancy/GooGle-BBR/raw/master/BBR.sh &amp;&amp; sh BBR.sh适用于Centos 6/7 仅适用64位） 1$ wget -O- http://soft.wellphp.com/scripts/install_bbr_centos.sh | bash 开启BBR安装内核后从刚安装的内核启动，然后执行 12$ echo "net.core.default_qdisc=fq" &gt;&gt; /etc/sysctl.conf$ echo "net.ipv4.tcp_congestion_control=bbr" &gt;&gt; /etc/sysctl.conf 保存生效 1$ sysctl -p 验证是否安装成功 执行以下命令，如果结果中有bbr则证明你的内核已开启bbr。 12345$ sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = bbr cubic reno$ lsmod | grep bbrtcp_bbr 20480 0 关闭BBR123$ sed -i '/net\.core\.default_qdisc=fq/d' /etc/sysctl.conf$ sed -i '/net\.ipv4\.tcp_congestion_control=bbr/d' /etc/sysctl.conf$ sysctl -p 执行完上面的代码，使用reboot重启后才能关闭bbr，重启后再用下面的查看bbr状态代码，查看是否关闭了。 1$ lsmod | grep bbr 如果结果中没有bbr, 则证明你的内核已关闭bbr]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tcp</tag>
        <tag>bbr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建以太坊开发环境]]></title>
    <url>%2Fblockchain%2Fethereum-dev-env-20170505.html</url>
    <content type="text"><![CDATA[以太坊（Ethereum）目标是打造成一个运行智能合约的去中心化平台（Platform for Smart Contract），平台上的应用按程序设定运行，不存在停机、审查、欺诈、第三方人为干预的可能。以太坊平台由 Golang、C++、Python 等多种编程语言实现。当然，为了打造这个平台，以太坊提供了一条公开的区块链，并制定了面向智能合约的一套编程语言。智能合约开发者可以在其上使用官方提供的工具来开发支持以太坊区块链协议的应用（即所谓的 DAPP）。 源码编译当前以太坊版本是1.6x，下载源码开始搭建环境吧。 12git clone https://github.com/ethereum/go-ethereum.gitmake geth 编译geth在./build/bin目录下 solc编译器12345678git clone https://github.com/ethereum/solidity.gitgit submodule update --init --recursive./scripts/install_deps.shmkdir buildcd buildcmake .. &amp;&amp; make 编译solc在./build/solc目录下 启动以太坊创建查看用户12geth --datadir data account newgeth --datadir data account list 更新解锁用户1geth --datadir data account update 启动1geth --datadir data console 自能合约编写hello world示例12345678910111213141516171819202122232425contract mortal &#123; /* Define variable owner of the type address*/ address owner; /* this function is executed at initialization and sets the owner of the contract */ function mortal() &#123; owner = msg.sender; &#125; /* Function to recover the funds on the contract */ function kill() &#123; if (msg.sender == owner) suicide(owner); &#125;&#125;contract greeter is mortal &#123; /* define variable greeting of the type string */ string greeting; /* this runs when the contract is executed */ function greeter(string _greeting) public &#123; greeting = _greeting; &#125; /* main function */ function greet() constant returns (string) &#123; return greeting; &#125;&#125; 编译12345solc --optimize --combined-json abi,bin,interface helloworld.solor echo "var testOutput=`solc --optimize --combined-json abi,bin,interface helloworld.sol`" &gt; test.js 登录console，解锁用户。使用之前创建用户的密码1personal.unlockAccount(address, "password") 未完待续…]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Html中支持数学公式，MathJax]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fmathjax-intro-20170502.html</url>
    <content type="text"><![CDATA[MathJax允许你在你的网页中包含公式，无论是使用LaTeX、MathML或者AsciiMath符号，这些公式都会被javascript处理为HTML、SVG或者MathML符号。 这里有三种方法获取MathJax：最简单的方法就是使用分布式网络服务中的MathJax的副本，它位于 cdn.mathjax.org ,但是你也可以下载并安装一个MathJax的副本到你的服务器,或者使用在你本地硬盘的副本（这样是不需要使用网络）。这三种方法接下来的内容中都有详细的描述。这个页面描述了最简单快捷的设置MathJax并在你的页面运行的方法，但是你也许需要阅读更多细节以帮助你为你的网页定制一些设置。 示例话不多说，先看下用MathJax编写的公式效果。 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); When $a \ne 0$, there are two solutions to \(ax^2 + bx + c = 0\) and they are $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$ 接下来看下如下编写Mahtjax。 编写格式MathJax有三种编写格式，如下： TeX和LaTeX格式 MathML格式 AsciiMath格式 TeX和LaTeX编写Tex和LaTex时，需要将下面代码引入html的head或body中。123&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 配置分隔符12345&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config(&#123; tex2jax: &#123;inlineMath: [['$','$'], ['\\(','\\)']]&#125;&#125;);&lt;/script&gt; 默认的公式分隔符是 $$...$$ 和 \[...\] ，还有 \(...\) 常用于段落中的公式。请特别注意， \(...\) 分隔符 不是 默认使用的。美元符号$常常在其他情况下使用，这会导致本文被错误的当做公式解析了。 编写Tex和LaTex公式 12When $a \ne 0$, there are two solutions to \(ax^2 + bx + c = 0\) and they are$$x = &#123;-b \pm \sqrt&#123;b^2-4ac&#125; \over 2a&#125;.$$ MathML同样需要将下面代码引入html的head或body中。123&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 编写MathML公式(html)1234567891011121314151617181920212223242526272829303132&lt;p&gt;When&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt; &lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&amp;#x2260;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;,there are two solutions to&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt; &lt;mi&gt;a&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt; &lt;mo&gt;+&lt;/mo&gt; &lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt; &lt;mo&gt;+&lt;/mo&gt; &lt;mi&gt;c&lt;/mi&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;and they are&lt;math xmlns="http://www.w3.org/1998/Math/MathML" display="block"&gt; &lt;mi&gt;x&lt;/mi&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mrow&gt; &lt;mfrac&gt; &lt;mrow&gt; &lt;mo&gt;&amp;#x2212;&lt;/mo&gt; &lt;mi&gt;b&lt;/mi&gt; &lt;mo&gt;&amp;#x00B1;&lt;/mo&gt; &lt;msqrt&gt; &lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt; &lt;mo&gt;&amp;#x2212;&lt;/mo&gt; &lt;mn&gt;4&lt;/mn&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt; &lt;/msqrt&gt; &lt;/mrow&gt; &lt;mrow&gt; &lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;a&lt;/mi&gt; &lt;/mrow&gt; &lt;/mfrac&gt; &lt;/mrow&gt; &lt;mtext&gt;.&lt;/mtext&gt;&lt;/math&gt;&lt;/p&gt; AsciiMath需要将下面代码引入html的head或body中（与上面的引入不相同）。123&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"&gt;&lt;/script&gt; 编写MathML公式12345&lt;p&gt;When `a != 0`, there are two solutions to `ax^2 + bx + c = 0` andthey are&lt;/p&gt;&lt;p style="text-align:center"&gt; `x = (-b +- sqrt(b^2-4ac))/(2a) .`&lt;/p&gt; 总结AsciiMath相对要简洁些，是MathJax2.0提出的。选择哪种方式根据实际情况而定。更多的编写方法从下面提供MathJax文档查找： MathJax英文文档 MathJax中文文档 推荐一个网站 Detexify， 可以在上面画数学符号，然后它会帮你找到符合的 Tex 代码。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>mathjax</tag>
        <tag>html</tag>
        <tag>数学公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为自己安装轻量级git代码管理仓库Gogs]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fgogs-install-20170428.html</url>
    <content type="text"><![CDATA[Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。使用 Go 语言开发使得 Gogs 能够通过独立的二进制分发，并且支持 Go 语言支持的 所有平台，包括 Linux、Mac OS X、Windows 以及 ARM 平台。 环境要求 操作系统：ubuntu 16.04 LTS (Xenial Xerus) 数据库：MySQL：版本 &gt;= 5.5.3 git（bash）：服务端和客户端均需版本 &gt;= 1.7.1 源码安装 基本依赖Go 语言：版本 &gt;= 1.5(我使用go1.8.1，Go环境设置不再赘述) 下载并编译 123456# 下载并安装依赖$ go get -u github.com/gogits/gogs# 构建主程序$ cd $GOPATH/src/github.com/gogits/gogs$ go build 初始化数据库12DROP DATABASE IF EXISTS gogs;CREATE DATABASE IF NOT EXISTS gogs CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 配置Gogs创建目录12mkdir -p custom/confcp conf/app.ini custom/conf 修改配置 vim custom/conf/app.ini12345678910[server]PROTOCOL = httpDOMAIN = yourdomain ROOT_URL = http://git.yourdomain/HTTP_ADDR = 0.0.0.0HTTP_PORT = 3000[database]PASSWD = root 参考Gogs配置 开机启动修改 vim scripts/systemd/gogs.service1234567891011121314151617181920212223[Unit]Description=GogsAfter=syslog.targetAfter=network.targetAfter=mariadb.service mysqld.service postgresql.service memcached.service redis.service[Service]# Modify these two values and uncomment them if you have# repos with lots of files and get an HTTP error 500 because# of that####LimitMEMLOCK=infinity#LimitNOFILE=65535Type=simpleUser=gitGroup=gitWorkingDirectory=/home/git/gogsExecStart=/home/git/gogs/gogs webRestart=alwaysEnvironment=USER=git HOME=/home/git[Install]WantedBy=multi-user.target 根据情况修改运行用户和gogs的下载（启动目录）。 启动Gogs123systemctl enable gogs.servicesystemctl start gogs.service 登录登录http://git.yourdomain/，现在可以使用自己的git服务了。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gogs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）Raft 一致性算法论文译文]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fraft_algorithm-20170425.html</url>
    <content type="text"><![CDATA[本篇博客为著名的 RAFT 一致性算法论文的中文翻译，论文名为《In search of an Understandable Consensus Algorithm (Extended Version)》(寻找一种易于理解的一致性算法)。 Raft 是一种用来管理日志复制的一致性算法。它和 Paxos 的性能和功能是一样的，但是它和 Paxos 的结构不一样；这使得 Raft 更容易理解并且更易于建立实际的系统。为了提高理解性，Raft 将一致性算法分为了几个部分，例如领导选取（leader selection），日志复制（log replication）和安全性（safety），同时它使用了更强的一致性来减少了必须需要考虑的状态。从用户学习的结果来看，Raft 比 Paxos 更容易学会。Raft 还包括了一种新的机制来使得动态改变集群成员，它使用重叠大多数（overlapping majorities）来保证安全。 引言一致性算法允许一组机器像一个整体一样工作，即使其中的一些机器出了错误也能正常工作。正因为此，他们扮演着建立大规模可靠的软件系统的关键角色。在过去的十年中 Paxos 一直都主导着有关一致性算法的讨论：大多数一致性算法的实现都基于它或者受它影响，并且 Paxos 也成为了教学生关于一致性知识的主要工具。 不幸的是，尽管在降低它的复杂性方面做了许多努力，Paxos 依旧很难理解。并且，Paxos 需要经过复杂的修改才能应用于实际中。这些导致了系统构构建者和学生都十分头疼。 在被 Paxos 折磨之后，我们开始寻找一种在系统构建和教学上更好的新的一致性算法。我们的首要目标是让它易于理解：我们能不能定义一种面向实际系统的一致性算法并且比 Paxos 更容易学习呢？并且，我们希望这种算法能凭直觉就能明白，这对于一个系统构建者来说是十分必要的。对于一个算法，不仅仅是让它工作起来很重要，知道它是如何工作的更重要。 我们工作的结果是一种新的一致性算法，叫做 Raft。在设计 Raft 的过程中我们应用了许多专门的技巧来提升理解性，包括算法分解（分为领导选取（leader selection），日志复制（log replication）和安全性（safety））和减少状态（state space reduction）（相对于 Paxos，Raft 减少了非确定性的程度和服务器互相不一致的方式）。在两所学校的43个学生的研究中发现，Raft 比 Paxos 要更容易理解：在学习了两种算法之后，其中的33个学生回答 Raft 的问题要比回答 Paxos 的问题要好。 Raft 算法和现在一些已经有的算法在一些地方很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication。但是 Raft 有几个新的特性： 强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。 领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。 成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。 我们认为，Raft 在教学方面和实际实现方面比 Paxos 和其他算法更出众。它比其他算法更简单、更容易理解；它能满足一个实际系统的需求；它拥有许多开源的实现并且被许多公司所使用；它的安全特性已经被证明；并且它的效率和其他算法相比也具有竞争力。 这篇论文剩下的部分会讲如下内容：复制状态机（replicated state machine）问题（第2节），讨论 Paxos 的优缺点（第3节），讨论我们用的为了达到提升理解性的方法（第4节），陈述 Raft 一致性算法（第5~8节），评价 Raft 算法（第9节），对相关工作的讨论（第10节）。 复制状态机（Replicated State Machine）一致性算法是在复制状态机的背景下提出来的。在这个方法中，在一组服务器的状态机产生同样的状态的副本因此即使有一些服务器崩溃了这组服务器也还能继续执行。复制状态机在分布式系统中被用于解决许多有关容错的问题。例如，GFS，HDFS还有 RAMCloud 这些大规模的系统都是用一个单独的集群领导者，使用一个单独的复制状态机来进行领导选取和存储配置信息来应对领导者的崩溃。使用复制状态机的例子有 Chubby 和 ZooKeeper。 复制状态机的架构。一致性算法管理来自客户端状态命令的复制日志。状态机处理的日志中的命令的顺序都是一致的，因此会得到相同的执行结果。 如图-1所示，复制状态机是通过复制日志来实现的。每一台服务器保存着一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的，所以每个状态机的状态都是相同的，执行的命令是相同的，最后的执行结果也就是一样的了。 如何保证复制日志一致就是一致性算法的工作了。在一台服务器上，一致性模块接受客户端的命令并且把命令加入到它的日志中。它和其他服务器上的一致性模块进行通信来确保每一个日志最终包含相同序列的请求，即使有一些服务器宕机了。一旦这些命令被正确的复制了，每一个服务器的状态机都会按同样的顺序去执行它们，然后将结果返回给客户端。最终，这些服务器看起来就像一台可靠的状态机。 应用于实际系统的一致性算法一般有以下特性： 确保安全性（从来不会返回一个错误的结果），即使在所有的非拜占庭（Non-Byzantine）情况下，包括网络延迟、分区、丢包、冗余和乱序的情况下。 高可用性，只要集群中的大部分机器都能运行，可以互相通信并且可以和客户端通信，这个集群就可用。因此，一般来说，一个拥有 5 台机器的集群可以容忍其中的 2 台的失败（fail）。服务器停止工作了我们就认为它失败（fail）了，没准一会当它们拥有稳定的存储时就能从中恢复过来，重新加入到集群中。 不依赖时序保证一致性，时钟错误和极端情况下的消息延迟在最坏的情况下才会引起可用性问题。 通常情况下，一条命令能够尽可能快的在大多数节点对一轮远程调用作出相应时完成，一少部分慢的机器不会影响系统的整体性能。 Paxos 算法的不足在过去的10年中，Leslie Lamport 的 Paxos 算法几乎已经成为了一致性算法的代名词：它是授课中最常见的算法，同时也是许多一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，例如一个单一复制日志条目（single replicated log entry）。我们把这个子集叫做单一决策 Paxos（single-decree Paxos）。之后 Paxos通过组合多个这种协议来完成一系列的决策，例如一个日志（multi-Paxos）。Paxos 确保安全性和活跃性（liveness），并且它支持集群成员的变更。它的正确性已经被证明，通常情况下也很高效。 不幸的是，Paxos 有两个致命的缺点。第一个是 Paxos 太难以理解。它的完整的解释晦涩难懂；很少有人能完全理解，只有少数人成功的读懂了它。并且大家做了许多努力来用一些简单的术语来描述它。尽管这些解释都关注于单一决策子集问题，但仍具有挑战性。在 NSDI 2012 会议上的一次非正式调查显示，我们发现大家对 Paxos 都感到不满意，其中甚至包括一些有经验的研究员。我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。 我们假定 Paxos 的晦涩来源于它将单决策子集作为它的基础。单决策（Single-decree）Paxos 是晦涩且微妙的：它被划分为两个没有简单直观解释的阶段，并且难以独立理解。正因为如此，它不能很直观的让我们知道为什么单一决策协议能够工作。为多决策 Paxos 设计的规则又添加了额外的复杂性和精巧性。我们相信多决策问题能够分解为其它更直观的方式。 Paxos 的第二个缺点是它难以在实际环境中实现。其中一个原因是，对于多决策 Paxos （multi-Paxos） ，大家还没有一个一致同意的算法。Lamport 的描述大部分都是有关于单决策 Paxos （single-decree Paxos）；他仅仅描述了实现多决策的可能的方法，缺少许多细节。有许多实现 Paxos 和优化 Paxos 的尝试，但是他们都和 Lamport 的描述有些出入。例如，Chubby 实现的是一个类似 Paxos 的算法，但是在许多情况下的细节没有公开。 另外，Paxos 的结构也是不容易在一个实际系统中进行实现的，这是单决策问题分解带来的又一个问题。例如，从许多日志条目中选出条目然后把它们融合到一个序列化的日志中并没有带来什么好处，它仅仅增加了复杂性。围绕着日志来设计一个系统是更简单、更高效的：新日志按照严格的顺序添加到日志中去。另一个问题是，Paxos 使用对等的点对点的实现作为它的核心（尽管它最终提出了一种弱领导者的形式来优化性能）。这种方法在只有一个决策被制定的情况下才显得有效，但是很少有现实中的系统使用它。如果要做许多的决策，选择一个领导人，由领带人来协调是更简单有效的方法。 因此，在实际的系统应用中和 Paxos 算法都相差很大。所有开始于 Paxos 的实现都会遇到很多问题，然后由此衍生出了许多与 Paxos 有很大不同的架构。这是既费时又容易出错的，并且理解 Paxos 的难度又非常大。Paxos 算法在它正确性的理论证明上是很好的，但是在实现上的价值就远远不足了。来自 Chubby 的实现的一条评论就能够说明： Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。 正因为存在这些问题，我们认为 Paxos 不仅对于系统的构建者来说不友好，同时也不利于教学。鉴于一致性算法对于大规模软件系统的重要性，我们决定试着来设计一种另外的比 Paxos 更好的一致性算法。Raft 就是这样的一个算法。 易于理解的设计设计 Raft 的目标有如下几个： 它必须提供一个完整的、实际的基础来进行系统构建，为的是减少开发者的工作； 它必须在所有情况下都能保证安全可用； 它对于常规操作必须高效； 最重要的目标是：易于理解，它必须使得大多数人能够很容易的理解； 另外，它必须能让开发者有一个直观的认识，这样才能使系统构建者们去对它进行扩展。 在设计 Raft 的过程中，我们不得不在许多种方法中做出选择。当面临这种情况时，我们通常会权衡可理解性：每种方法的可理解性是如何的？（例如，它的状态空间有多复杂？它是不是有很细微的含义？）它的可读性如何？读者能不能轻易地理解这个方法和它的含义？ 我们意识到对这种可理解性的分析具有高度的主观性；尽管如此，我们使用了两种适用的方式。第一种是众所周知的问题分解：我们尽可能将问题分解成为若干个可解决的、可被理解的小问题。例如，在 Raft 中，我们把问题分解成为了领导选取（leader election）、日志复制（log replication）、安全（safety）和成员变化（membership changes）。 我们采用的第二个方法是通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致并且尽可能消除不确定性。特别地，日志之间不允许出现空洞，并且 Raft 限制了限制了日志不一致的可能性。尽管在大多数情况下，我们都都在试图消除不确定性，但是有时候有些情况下，不确定性使得算法更易理解。尤其是，随机化方法使得不确定性增加，但是它减少了状态空间。我们使用随机化来简化了 Raft 中的领导选取算法。 Raft 一致性算法Raft 是一种用来管理第 2 章中提到的复制日志的算法。表-2 为了方便参考是一个算法的总结版本，表-3 列举了算法中的关键性质；表格中的这些元素将会在这一章剩下的部分中分别进行讨论。 状态： 在所有服务器上持久存在的：（在响应远程过程调用 RPC 之前稳定存储的） 名称 描述 currentTerm 服务器最后知道的任期号（从0开始递增） votedFor 在当前任期内收到选票的候选人 id（如果没有就为 null） log[] 日志条目；每个条目包含状态机的要执行命令和从领导人处收到时的任期号 在所有服务器上不稳定存在的： 名称 描述 commitIndex 已知的被提交的最大日志条目的索引值（从0开始递增） lastApplied 被状态机执行的最大日志条目的索引值（从0开始递增） 在领导人服务器上不稳定存在的：（在选举之后初始化的） 名称 描述 nextIndex[] 对于每一个服务器，记录需要发给它的下一个日志条目的索引（初始化为领导人上一条日志的索引值+1） matchIndex[] 对于每一个服务器，记录已经复制到该服务器的日志的最高索引值（从0开始递增） 附加日志远程过程调用 （AppendEntries RPC） 由领导人来调用复制日志（5.3节）；也会用作heartbeat 参数 描述 term 领导人的任期号 leaderId 领导人的 id，为了其他服务器能重定向到客户端 prevLogIndex 最新日志之前的日志的索引值 prevLogTerm 最新日志之前的日志的领导人任期号 entries[] 将要存储的日志条目（表示 heartbeat 时为空，有时会为了效率发送超过一条） leaderCommit 领导人提交的日志条目索引值 返回值 描述 term 当前的任期号，用于领导人更新自己的任期号 success 如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 接受者需要实现： 如果 term &lt; currentTerm返回 false（5.1节） 如果在prevLogIndex处的日志的任期号与prevLogTerm不匹配时，返回 false（5.3节） 如果一条已经存在的日志与新的冲突（index 相同但是任期号 term 不同），则删除已经存在的日志和它之后所有的日志（5.3节） 添加任何在已有的日志中不存在的条目 如果leaderCommit &gt; commitIndex，将commitIndex设置为leaderCommit和最新日志条目索引号中较小的一个 投票请求 RPC（RequestVote RPC） 由候选人发起收集选票（5.2节） 参数 描述 term 候选人的任期号 candidateId 请求投票的候选人 id lastLogIndex 候选人最新日志条目的索引值 lastLogTerm 候选人最新日志条目对应的任期号 返回值 描述 term 目前的任期号，用于候选人更新自己 voteGranted 如果候选人收到选票为 true 接受者需要实现： 如果term &lt; currentTerm返回 false（5.1节） 如果votedFor为空或者与candidateId相同，并且候选人的日志和自己的日志一样新，则给该候选人投票（5.2节 和 5.4节） 服务器需要遵守的规则： 所有服务器： 如果commitIndex &gt; lastApplied，lastApplied自增，将log[lastApplied]应用到状态机（5.3节） 如果 RPC 的请求或者响应中包含一个 term T 大于 currentTerm，则currentTerm赋值为 T，并切换状态为追随者（Follower）（5.1节） 追随者（followers）: 5.2节 响应来自候选人和领导人的 RPC 如果在超过选取领导人时间之前没有收到来自当前领导人的AppendEntries RPC或者没有收到候选人的投票请求，则自己转换状态为候选人 候选人：5.2节 转变为选举人之后开始选举： currentTerm自增 给自己投票 重置选举计时器 向其他服务器发送RequestVote RPC 如果收到了来自大多数服务器的投票：成为领导人 如果收到了来自新领导人的AppendEntries RPC（heartbeat）：转换状态为追随者 如果选举超时：开始新一轮的选举 领导人： 一旦成为领导人：向其他所有服务器发送空的AppendEntries RPC（heartbeat）;在空闲时间重复发送以防止选举超时（5.2节） 如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端（5.3节） 对于一个追随者来说，如果上一次收到的日志索引大于将要收到的日志索引（nextIndex）：通过AppendEntries RPC将 nextIndex 之后的所有日志条目发送出去 如果发送成功：将该追随者的 nextIndex和matchIndex更新 如果由于日志不一致导致AppendEntries RPC失败：nextIndex递减并且重新发送（5.3节） 如果存在一个满足N &gt; commitIndex和matchIndex[i] &gt;= N并且log[N].term == currentTerm的 N，则将commitIndex赋值为 N 性质 描述 选举安全原则（Election Safety） 一个任期（term）内最多允许有一个领导人被选上（5.2节） 领导人只增加原则（Leader Append-Only） 领导人永远不会覆盖或者删除自己的日志，它只会增加条目 日志匹配原则（Log Matching） 如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同（5.3 节） 领导人完全原则（Leader Completeness) 如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中 状态机安全原则（State Machine Safety） 如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目（5.4.3节） Raft 通过首先选出一个领导人来实现一致性，然后给予领导人完全管理复制日志（replicated log）的责任。领导人接收来自客户端的日志条目，并把它们复制到其他的服务器上，领带人还要告诉服务器们什么时候将日志条目应用到它们的状态机是安全的。通过选出领导人能够简化复制日志的管理工作。例如，领导人能够决定将新的日志条目放到哪，而并不需要和其他的服务器商议，数据流被简化成从领导人流向其他服务器。如果领导人宕机或者和其他服务器失去连接，就可以选取下一个领导人。 通过选出领导人，Raft 将一致性问题分解成为三个相对独立的子问题： 领导人选取（Leader election）： 在一个领导人宕机之后必须要选取一个新的领导人（5.2节） 日志复制（Log replication）： 领导人必须从客户端接收日志然后复制到集群中的其他服务器，并且强制要求其他服务器的日志保持和自己相同 安全性（Safety）： Raft 的关键的安全特性是 表-3 中提到的状态机安全原则（State Machine Safety）:如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。5.4节阐述了 Raft 是如何保证这条原则的，解决方案涉及到一个对于选举机制另外的限制，这一部分会在 5.2节 中说明。 在说明了一致性算法之后，本章会讨论有关可用性（availability）的问题和系统中时序（timing）的问题。 Raft 基础一个 Raft 集群包括若干服务器；对于一个典型的 5 服务器集群，该集群能够容忍 2 台机器不能正常工作，而整个系统保持正常。在任意的时间，每一个服务器一定会处于以下三种状态中的一个：领导人、候选人、追随者。在正常情况下，只有一个服务器是领导人，剩下的服务器是追随者。追随者们是被动的：他们不会发送任何请求，只是响应来自领导人和候选人的请求。领导人来处理所有来自客户端的请求（如果一个客户端与追随者进行通信，追随者会将信息发送给领导人）。候选人是用来选取一个新的领导人的，这一部分会在 5.2节 进行阐释。图-4 阐述了这些状态，和它们之间的转换；它们的转换会在下边进行讨论。 图-4：服务器的状态。追随者只响应其他服务器的请求。如果追随者没有收到任何消息，它会成为一个候选人并且开始一次选举。收到大多数服务器投票的候选人会成为新的领导人。领导人在它们宕机之前会一直保持领导人的状态。 图-5：时间被分为一个个的任期（term），每一个任期的开始都是领导人选举。在成功选举之后，一个领导人会在任期内管理整个集群。如果选举失败，该任期就会因为没有领带人而结束。这个转变会在不同的时间的不同服务器上观察到。 如图-5 所示，Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），就像 5.2节 所描述的那样，一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最少要有一个领导人。 不同的服务器可能会在任期内观察到多次不同的状态转换，在某些情况下，一台服务器可能看不到一次选举或者一个完整的任期。任期在 Raft 中充当逻辑时钟的角色，并且它们允许服务器检测过期的信息，比如过时的领导人。每一台服务器都存储着一个当前任期的数字，这个数字会单调的增加。当服务器之间进行通信时，会互相交换当前任期号；如果一台服务器的当前任期号比其它服务器的小，则更新为较大的任期号。如果一个候选人或者领导人意识到它的任期号过时了，它会立刻转换为追随者状态。如果一台服务器收到的请求的任期号是过时的，那么它会拒绝此次请求。 Raft 中的服务器通过远程过程调用（RPC）来通信，基本的 Raft 一致性算法仅需要 2 种 RPC。RequestVote RPC 是候选人在选举过程中触发的（5.2节），AppendEntries RPC 是领导人触发的，为的是复制日志条目和提供一种心跳（heartbeat）机制（5.3节）。第7章加入了第三种 RPC 来在各个服务器之间传输快照（snapshot）。如果服务器没有及时收到 RPC 的响应，它们会重试，并且它们能够并行的发出 RPC 来获得最好的性能。 领导人选取Raft 使用一种心跳机制（heartbeat）来触发领导人的选取。当服务器启动时，它们会初始化为追随者。一太服务器会一直保持追随者的状态只要它们能够收到来自领导人或者候选人的有效 RPC。领导人会向所有追随者周期性发送心跳（heartbeat，不带有任何日志条目的 AppendEntries RPC）来保证它们的领导人地位。如果一个追随者在一个周期内没有收到心跳信息，就叫做选举超时（election timeout）,然后它就会假定没有可用的领导人并且开始一次选举来选出一个新的领导人。 为了开始选举，一个追随者会自增它的当前任期并且转换状态为候选人。然后，它会给自己投票并且给集群中的其他服务器发送 RequestVote RPC。一个候选人会一直处于该状态，直到下列三种情形之一发生： 它赢得了选举； 另一台服务器赢得了选举； 一段时间后没有任何一台服务器赢得了选举 这些情形会在下面的章节中分别讨论。 一个候选人如果在一个任期内收到了来自集群中大多数服务器的投票就会赢得选举。在一个任期内，一台服务器最多能给一个候选人投票，按照先到先服务原则（first-come-first-served）（注意：在 5.4节 针对投票添加了一个额外的限制）。大多数原则使得在一个任期内最多有一个候选人能赢得选举（表-3 中提到的选举安全原则）。一旦有一个候选人赢得了选举，它就会成为领导人。然后它会像其他服务器发送心跳信息来建立自己的领导地位并且组织新的选举。 当一个候选人等待别人的选票时，它有可能会收到来自其他服务器发来的声明其为领导人的 AppendEntries RPC。如果这个领导人的任期（包含在它的 RPC 中）比当前候选人的当前任期要大，则候选人认为该领导人合法，并且转换自己的状态为追随者。如果在这个 RPC 中的任期小于候选人的当前任期，则候选人会拒绝此次 RPC， 继续保持候选人状态。 第三种情形是一个候选人既没有赢得选举也没有输掉选举：如果许多追随者在同一时刻都成为了候选人，选票会被分散，可能没有候选人能获得大多数的选票。当这种情形发生时，每一个候选人都会超时，并且通过自增任期号和发起另一轮 RequestVote RPC 来开始新的选举。然而，如果没有其它手段来分配选票的话，这种情形可能会无限的重复下去。 Raft 使用随机的选举超时时间来确保第三种情形很少发生，并且能够快速解决。为了防止在一开始是选票就被瓜分，选举超时时间是在一个固定的间隔内随机选出来的（例如，150~300ms）。这种机制使得在大多数情况下只有一个服务器会率先超时，它会在其它服务器超时之前赢得选举并且向其它服务器发送心跳信息。同样的机制被用于选票一开始被瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，在超时进行下一次选举之前一直等待。这能够减小在新的选举中一开始选票就被瓜分的可能性。9.3节 展示了这种方法能够快速的选出一个领导人。 选举是一个理解性引导我们设计替代算法的一个例子。最开始时，我们计划使用一种排名系统：给每一个候选人分配一个唯一的排名，用于在竞争的候选人之中选择领导人。如果一个候选人发现了另一个比它排名高的候选人，那么它会回到追随者的状态，这样排名高的候选人会很容易地赢得选举。但是我们发现这种方法在可用性方面有一点问题（一个低排名的服务器在高排名的服务器宕机后，需要等待超时才能再次成为候选人，但是如果它这么做的太快，它能重置选举领带人的过程）。我们对这个算法做了多次调整，但是每次调整后都会出现一些新的问题。最终我们认为随机重试的方法是更明确并且更易于理解的。 日志复制一旦选出了领导人，它就开始接收客户端的请求。每一个客户端请求都包含一条需要被复制状态机（replicated state machine）执行的命令。领导人把这条命令作为新的日志条目加入到它的日志中去，然后并行的向其他服务器发起 AppendEntries RPC ，要求其它服务器复制这个条目。当这个条目被安全的复制之后（下面的部分会详细阐述），领导人会将这个条目应用到它的状态机中并且会向客户端返回执行结果。如果追随者崩溃了或者运行缓慢或者是网络丢包了，领导人会无限的重试 AppendEntries RPC（甚至在它向客户端响应之后）知道所有的追随者最终存储了所有的日志条目。 图-6：日志由有序编号的日志条目组成。每个日志条目包含它被创建时的任期号（每个方块中的数字），并且包含用于状态机执行的命令。如果一个条目能够被状态机安全执行，就被认为可以提交了。日志就像 图-6 所示那样组织的。每个日志条目存储着一条被状态机执行的命令和当这条日志条目被领导人接收时的任期号。日志条目中的任期号用来检测在不同服务器上日志的不一致性，并且能确保 图-3 中的一些特性。每个日志条目也包含一个整数索引来表示它在日志中的位置。 领导人决定什么时候将日志条目应用到状态机是安全的；这种条目被称为可被提交（commited）。Raft 保证可被提交（commited）的日志条目是持久化的并且最终会被所有可用的状态机执行。一旦被领导人创建的条目已经复制到了大多数的服务器上，这个条目就称为可被提交的（例如，图-6中的7号条目）。领导人日志中之前的条目都是可被提交的（commited），包括由之前的领导人创建的条目。5.4节将会讨论当领导人更替之后这条规则的应用问题的细节，并且也讨论了这种提交方式是安全的。领导人跟踪记录它所知道的被提交条目的最大索引值，并且这个索引值会包含在之后的 AppendEntries RPC 中（包括心跳 heartbeat 中），为的是让其他服务器都知道这条条目已经提交。一旦一个追随者知道了一个日志条目已经被提交，它会将该条目应用至本地的状态机（按照日志顺序）。 我们设计了 Raft 日志机制来保证不同服务器上日志的一致性。这样做不仅简化了系统的行为使得它更可预测，并且也是保证安全性不可或缺的一部分。Raft 保证以下特性，并且也保证了 表-3 中的日志匹配原则（Log Matching Property）: 如果在不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。 如果在不同日志中的两个条目有着相同的索引和任期号，则它们之间的所有条目都是完全一样的。 第一条特性源于领导人在一个任期里在给定的一个日志索引位置最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，领导人会把新日志条目紧接着之前的条目的索引位置和任期号都包含在里面。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。 图-7：当最上边的领导人掌权之后，追随者日志可能有以下情况（a~f）。一个格子表示一个日志条目；格子中的数字是它的任期。一个追随者可能会丢失一些条目（a, b）；可能多出来一些未提交的条目（c, d）；或者两种情况都有（e, f）。例如，场景 f 在如下情况下就会发生：如果一台服务器在任期2时是领导人并且往它的日志中添加了一些条目，然后在将它们提交之前就宕机了，之后它很快重启了，成为了任期3的领导人，又往它的日志中添加了一些条目，然后在任期2和任期3中的条目提交之前它又宕机了并且几个任期内都一直处于宕机状态。 在一般情况下，领导人和追随者们的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，领导人的崩溃会导致日志不一致（旧的领导人可能没有完全复制完日志中的所有条目）。这些不一致会导致一系列领导人和追随者崩溃。图-7 阐述了一些追随者可能和新的领导人日志不同的情况。一个追随者可能会丢失掉领导人上的一些条目，也有可能包含一些领导人没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。 在 Raft 算法中，领导人通过强制追随者们复制它的日志来处理日志的不一致。这就意味着，在追随者上的冲突日志会被领导者的日志覆盖。5.4节会说明当添加了一个额外的限制之后这是安全的。 为了使得追随者的日志同自己的一致，领导人需要找到追随者同它的日志一致的地方，然后删除追随者在该位置之后的条目，然后将自己在该位置之后的条目发送给追随者。这些操作都在 AppendEntries RPC 进行一致性检查时完成。领导人给每一个追随者维护了一个nextIndex，它表示领导人将要发送给该追随者的下一条日志条目的索引。当一个领导人开始掌权时，它会将nextIndex初始化为它的最新的日志条目索引数+1（图-7 中的 11）。如果一个追随者的日志和领导者的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，领导人会将nextIndex递减然后重试 AppendEntries RPC。最终nextIndex会达到一个领导人和追随者日志一致的地方。这时，AppendEntries 会返回成功，追随者中冲突的日志条目都被移除了，并且添加所缺少的上了领导人的日志条目。一旦 AppendEntries 返回成功，追随者和领导人的日志就一致了，这样的状态会保持到该任期结束。 如果需要的话，算法还可以进行优化来减少 AppendEntries RPC 失败的次数。例如，当拒绝了一个 AppendEntries 请求，追随者可以记录下冲突日志条目的任期号和自己存储那个任期的最早的索引。通过这些信息，领导人能够直接递减nextIndex跨过那个任期内所有的冲突条目；这样的话，一个冲突的任期需要一次 AppendEntries RPC，而不是每一个冲突条目需要一次 AppendEntries RPC。在实践中，我们怀疑这种优化是否是必要的，因为AppendEntries 一致性检查很少失败并且也不太可能出现大量的日志条目不一致的情况。 通过这种机制，一个领导人在掌权时不需要采取另外特殊的方式来恢复日志的一致性。它只需要使用一些常规的操作，通过响应 AppendEntries 一致性检查的失败能使得日志自动的趋于一致。一个领导人从来不会覆盖或者删除自己的日志（表-3 中的领导人只增加原则）。 这个日志复制机制展示了在第2章中阐述的所希望的一致性特性：Raft 能够接受，复制并且应用新的日志条目只要大部分的服务器是正常的。在通常情况下，一条新的日志条目可以在一轮 RPC 内完成在集群的大多数服务器上的复制；并且一个速度很慢的追随者并不会影响整体的性能。 安全性之前的章节中讨论了 Raft 算法是如何进行领导选取和复制日志的。然而，到目前为止这个机制还不能保证每一个状态机能按照相同的顺序执行同样的指令。例如，当领导人提交了若干日志条目的同时一个追随者可能宕机了，之后它又被选为了领导人然后用新的日志条目覆盖掉了旧的那些，最后，不同的状态机可能执行不同的命令序列。 这一节通过在领带人选取部分加入了一个限制来完善了 Raft 算法。这个限制能够保证对于固定的任期，任何的领导人都拥有之前任期提交的全部日志条目（表-3 中的领导人完全原则）。有了这一限制，日志提交的规则就更清晰了。最后，我们提出了对于领导人完全原则的简单证明并且展示了它是如何修正复制状态机的行为的。 选举限制在所有的以领导人为基础的一致性算法中，领导人最终必须要存储全部已经提交的日志条目。在一些一致性算法中，例如：Viewstamped Replication，即使一开始没有包含全部已提交的条目也可以被选为领导人。这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，这个过程要么在选举过程中完成，要么在选举之后立即开始。不幸的是，这种方式大大增加了复杂性。Raft 使用了一种更简单的方式来保证在新的领导人开始选举的时候在之前任期的所有已提交的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。 Raft 使用投票的方式来阻止没有包含全部日志条目的服务器赢得选举。一个候选人为了赢得选举必须要和集群中的大多数进行通信，这就意味着每一条已经提交的日志条目最少在其中一台服务器上出现。如果候选人的日志至少和大多数服务器上的日志一样新（up-to-date，这个概念会在下边有详细介绍），那么它一定包含有全部的已经提交的日志条目。RequestVote RPC 实现了这个限制：这个 RPC（远程过程调用）包括候选人的日志信息，如果它自己的日志比候选人的日志要新，那么它会拒绝候选人的投票请求。 Raft 通过比较日志中最后一个条目的索引和任期号来决定两个日志哪一个更新。如果两个日志的任期号不同，任期号大的更新；如果任期号相同，更长的日志更新。 提交之前任期的日志条目图-8：如图的时间序列说明了为什么领导人不能通过之前任期的日志条目判断它的提交状态。（a）中的 S1 是领导人并且部分复制了索引2上的日志条目。（b）中 S1 崩溃了；S5 通过 S3，S4 和自己的选票赢得了选举，并且在索引2上接收了另一条日志条目。（c）中 S5 崩溃了，S1 重启了，通过 S2，S3 和自己的选票赢得了选举，并且继续索引2处的复制，这时任期2的日志条目已经在大部分服务器上完成了复制，但是还并没有提交。如果在（d）时刻 S1 崩溃了，S5 会通过 S2，S3，S4 的选票成为领导人，然后用它自己在任期3的日志条目覆盖掉其他服务器的日志条目。然而，如果在崩溃之前，S1 在它的当前任期在大多数服务器上复制了一条日志条目，就像在（e）中那样，那么这条条目就会被提交（S5就不会赢得选举）。在这时，之前的日志条目就会正常被提交。正如 5.3节 中描述的那样，只要一个日志条目被存在了在多数的服务器上，领导人就知道当前任期就可以提交该条目了。如果领导人在提交之前就崩溃了，之后的领导人会试着继续完成对日志的复制。然而，领导人并不能断定存储在大多数服务器上的日志条目一定在之前的任期中被提交了。图-8 说明了一种情况，一条存储在了大多数服务器上的日志条目仍然被新上任的领导人覆盖了。 为了消除 图-8 中描述的问题，Raft 从来不会通过计算复制的数目来提交之前人气的日志条目。只有领导人当前任期的日志条目才能通过计算数目来进行提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配原则（Log Matching Property），之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，通过观察该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。 因为当领导人从之前任期复制日志条目时日志条目保留了它们最开始的任期号，所以这使得 Raft 在提交规则中增加了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要从之前的任期中复制日志条目，它必须要使用当前的新任期号。Raft 的方法使得判断日志更加容易，因为它们全程都保持着同样的任期号。另外，和其它的一致性算法相比，Raft 算法中的新领导人会发送更少的之前任期的日志条目（其他算法必须要发送冗余的日志条目并且在它们被提交之前来重新排序）。 安全性论证图-9：如果 S1（任期 T 的领导人）在它的任期提交了一条日志条目，并且 S5 在之后的任期 U 成为了领导人，那么最少会有一台服务器（S3）接收了这条日志条目并且会给 S5 投票。 给出了完整的 Raft 算法，现在我们能够更精确的论证领导人完全原则（Leader Completeness)（这基于 9.2节 提出的安全性证明）。我们假定领导人完全原则是不成立的，然后推导出矛盾。假定任期 T 的领导人 leaderT在它的任期提交了一个日志条目，但是这条日志条目并没有存储在之后的任期中的领导人上。我们设大于 T 的最小的任期 U 的领导人（leaderU） 没有存储这条日志条目。 在 leaderU 选举时一定没有那条被提交的日志条目（领导人从来不会删除或者覆盖日志条目）。 leaderT 复制了这个条目到集群的大多数的服务器上。因此，只是有一台服务器（投票者）即接收了来自 leaderT 的日志条目并且给 leaderU 投票，就像 图-9 中所示那样。这个投票者是产生矛盾的关键。 投票者必须在给 leaderU 投票之前接收来自 leaderT 的日志条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（它的当前任期会比 T 要大）。 投票者会在它给 leaderU 投票时存储那个条目，因为任何中间的领导人都保有该条目（基于假设），领导人从来不会移除这个条目，并且追随者也只会在和领导人冲突时才会移除日志条目。 投票者给 leaderU 投票了，所以 leaderU 的日志必须和投票者的一样新。这就导致了一个矛盾。 首先，如果投票者和 leaderU 最后一条日志条目的任期号相同，那么 leaderU 的日志一定和投票者的一样长，因此它的日志包含全部投票者的日志条目。这是矛盾的，因为在假设中投票者和 leaderU 包含的已提交条目是不同的。 除此之外， leaderU 的最后一条日志的任期号一定比投票者的大。另外，它也比 T 要大，因为投票者的最后一条日志条目的任期号最小也要是 T（它包含了所有任期 T 提交的日志条目）。创建 leaderU 最后一条日志条目的上一任领导人必须包含已经提交的日志条目（基于假设）。那么，根据日志匹配原则（Log Matching），leaderU 也一定包含那条提交的日志条目，这也是矛盾的。 这时就完成了矛盾推导。因此，所有比任期 T 大的领导人一定包含所有在任期 T 提交的日志条目。 日志匹配原则（Log Matching）保证了未来的领导人也会包含被间接提交的日志条目，就像 图-8 中（d）时刻索引为2的条目。 通过给出了 领导人完全原则（Leader Completeness)，我们能够证明 表-3 中的状态机安全原则（State Machine Safety），状态机安全原则（State Machine Safety）讲的是如果一台服务器将给定索引上的日志条目应用到了它自己的状态机上，其它服务器的同一索引位置不可能应用的是其它条目。在一个服务器应用一条日志条目到它自己的状态机中时，它的日志必须和领导人的日志在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性（Log Completeness Property）保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。 最后，Raft 算法需要服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。 追随者和候选人崩溃截止到目前，我们只讨论了领导人崩溃的问题。追随者和候选人崩溃的问题解决起来要比领导人崩溃要简单得多，这两者崩溃的处理方式是一样的。如果一个追随者或者候选人崩溃了，那么之后的发送给它的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限的重试来处理这些失败；如果崩溃的服务器重启了，RPC 就会成功完成。如果一个服务器在收到了 RPC 之后但是在响应之前崩溃了，那么它会在重启之后再次收到同一个 RPC。因为 Raft 中的 RPC 都是幂等的，因此不会有什么问题。例如，如果一个追随者收到了一个已经包含在它的日志中的 AppendEntries 请求，它会忽视这个新的请求。 时序和可用性我们对于 Raft 的要求之一就是安全性不依赖于时序（timing）：系统不能仅仅因为一些事件发生的比预想的快一些或慢一些就产生错误。然而，可用性（系统可以及时响应客户端的特性）不可避免的要依赖时序。例如，如果消息交换在服务器崩溃时花费更多的时间，候选人不会等待太长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。 领导人选取是 Raft 中对时序要求最关键的地方。Raft 会选出并且保持一个稳定的领导人只有系统满足下列时序要求（timing requirement）： broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF 在这个不等式中，broadcastTime指的是一台服务器并行的向集群中的其他服务器发送 RPC 并且收到它们的响应的平均时间；electionTimeout指的就是在 5.2节 描述的选举超时时间；MTBF指的是单个服务器发生故障的间隔时间的平均数。broadcastTime应该比electionTimeout小一个数量级，为的是使领导人能够持续发送心跳信息（heartbeat）来阻止追随者们开始选举；根据已经给出的随机化选举超时时间方法，这个不等式也使得瓜分选票的情况变成不可能。electionTimeout也要比MTBF小几个数量级，为的是使得系统稳定运行。当领导人崩溃时，整个大约会在electionTimeout的时间内不可用；我们希望这种情况仅占全部时间的很小的一部分。 broadcastTime和MTBF是由系统决定的性质，但是electionTimeout是我们必须做出选择的。Raft 的 RPC 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，这取决于存储的技术。因此，electionTimeout一般在 10ms 到 500ms 之间。大多数的服务器的MTBF都在几个月甚至更长，很容易满足这个时序需求。 集群成员变化截止到目前，我们都假定集群的配置（加入到一致性算法的服务器集合）是固定的。在实际中，我们会经常更改配置，例如，替换掉那些崩溃的机器或者更改复制级别。虽然通过关闭整个集群，升级配置文件，然后重启整个集群也可以解决这个问题，但是这回导致在更改配置的过程中，整个集群不可用。另外，如果存在需要手工操作，那么就会有操作失误的风险。为了避免这些问题，我们决定采用自动改变配置并且把这部分加入到了 Raft 一致性算法中。 为了让配置修改机制能够安全，那么在转换的过程中在任何时间点两个领导人不能再同一个任期被同时选为领导人。不幸的是，服务器集群从旧的配置直接升级到新的配置的任何方法都是不安全的，一次性自动的转换所有服务器是不可能的，所以集群可以在转换的过程中划分成两个单独的组（如 图-10 所示）。 图-10：从一个配置切换到另一个配置是不安全的因为不同的服务器会在不同的时间点进行切换。在这个例子中，集群数量从三台转换成五台。不幸的是，在一个时间点有两个服务器能被选举成为领导人，一个是在使用旧的配置的机器中（Cold）选出的领导人，另一个领导人是通过新的配置（Cnew）选出来的。 为了保证安全性，集群配置的调整必须使用两阶段（two-phase）方法。有许多种实现两阶段方法的实现。例如，一些系统在第一个阶段先把旧的配置设为无效使得它无法处理客户端请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡配置，我们称其为共同一致（joint consensus）；一旦共同一致被提交了，然后系统再切换到新的配置。共同一致是旧的配置和新的配置的组合： 日志条目被复制给集群中新、老配置的所有服务器。 新、老配置的服务器都能成为领导人。 需要分别在两种配置上获得大多数的支持才能达成一致（针对选举和提交） 共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然能够响应服务器请求。 图-11：集群配置变更的时间线。虚线表示的是已经被创建但是还没提交的配置条目，实线表示的是最新提交的配置条目。领导人首先在它的日志中创建 Cold,new配置条目并且将它提交到Cold,new（使用旧配置的大部分服务器和使用新配置的大部分服务器）。然后创建它创建Cnew配置条目并且将它提交到使用新配置的大部分机器上。这样就不存在Cold和Cnew能够分别同时做出决定的时刻。 集群配置在复制日志中用特殊的日志条目来存储和通信；图-11 展示了配置变更的过程。当一个领导人接收到一个改变配置 Cold 为 Cnew 的请求，它会为了共同一致以前面描述的日志条目和副本的形式将配置存储起来（图中的 Cold,new）。一旦一个服务器将新的配置日志条目增加到它的日志中，它就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论它是否已经被提交）。这意味着领导人要使用 Cold,new 的规则来决定日志条目 Cold,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 Cold 配置也可能是 Cold,new 配置，这取决于赢得选举的候选人是否已经接收到了 Cold,new 配置。在任何情况下， Cnew 配置在这一时期都不会单方面的做出决定。 一旦 Cold,new 被提交，那么无论是 Cold 还是 Cnew，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 Cnew 配置的日志条目并复制给集群就是安全的了。另外，每个服务器在收到新的配置的时候就会立即生效。当新的配置在 Cnew 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如 图-11，Cold 和 Cnew 没有任何机会同时做出单方面的决定；这就保证了安全性。 针对重新配置提出了三个问题。第一个问题是一开始的时候新的服务器可能没有任何日志条目。如果它们在这个状态下加入到集群中，那么它们需要一段时间来更新追赶，在这个阶段它们还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群中来（领导人复制日志给他们，但是不把它们考虑到大多数中）。一旦新的服务器追赶上了集群中的其它机器，重新配置可以像上面描述的一样处理。 第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 Cnew 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括自己；它复制日志但是不把它自己看作是大多数之一。当 Cnew 被提交时，会发生领导人过渡，因为这时是新的配置可以独立工作的最早的时间点（总是能够在 Cnew 配置下选出新的领导人）。在此之前，可能只能从 Cold 中选出领导人。 第三个问题是，移除不在 Cnew 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳（heartbeat），所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新的任期号的 RequestVote RPC，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。 为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略 RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个 RequestVote RPC，它不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么它就不会被更大的任期号废除。 日志压缩Raft 产生的日志在持续的正常操作中不断增长，但是在实际的系统中，它不会无限的增长下去。随着日志的不断增长，它会占据越来越多的空间并且花费更多的时间重置。如果没有一个机制使得它能够废弃在日志中不断累积的过时的信息就会引起可用性问题。 快照（snapshot）是最简单的压缩方式。在快照中，全部的当前系统状态都被写入到快照中，存储到持久化的存储中，然后在那个时刻之前的全部日志都可以被丢弃。在 Chubby 和 ZooKeeper 中都使用了快照技术，这一章的剩下的部分会介绍 Raft 中使用的快照技术。 增量压缩（incremental approaches）的方法，例如日志清理（log cleaning）或者日志结构合并树（log-structured merge trees），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以使用和快照相同的接口来实现 LSM tree ，但是日志清除方法就需要修改 Raft 了。 图-12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。图-12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也将一些少量的元数据包含到快照中：最后被包含的索引（last included index）指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期（last included term）指的是该条目的任期号。保留这些数据是为了支持快照前的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 章），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。 尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 章）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给它们。 安装快照 RPC（InstallSnapshot RPC） 在领导人发送快照给跟随者时使用调用。领导人总是按顺序发送。 参数 描述 term 领导人的任期 leaderId 为了追随者能重定向到客户端 lastIncludedIndex 快照中包含的最后日志条目的索引值 offset 分块在快照中的偏移量 data[] 快照块的原始数据 done 如果是最后一块数据则为真 返回值 描述 term currentTerm，用于领导人更新自己 接受者需要实现： 如果term &lt; currentTerm立刻回复 如果是第一个分块（offset 为 0）则创建新的快照 在指定的偏移量写入数据 如果 done为 false，则回复并继续等待之后的数据 保存快照文件，丢弃所有存在的或者部分有着更小索引号的快照 如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保留并且回复 丢弃全部日志 能够使用快照来恢复状态机（并且装载快照中的集群配置） 在这种情况下领导人使用一种叫做安装快照（InstallSnapshot）的新的 RPC 来发送快照给太落后的跟随者；见 表-13。当跟随者通过这种 RPC 接收到快照时，它必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃它所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须是正确的和并且被保留下来。 这种快照的方式背离了 Raft 的强领导人原则（strong leader principle），因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织它们的数据了。 我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。 还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，它就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。 第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制（copy-on-write）的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。 客户端交互这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端是如何发现领导人的和 Raft 是如何支持线性化语义（linearizable semantics）的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。 Raft 中的客户端将所有请求发送给领导人。当客户端启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供它最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。 我们 Raft 的目标是要实现线性化语义（linearizable semantics）（每一次操作立即执行，在它调用和收到回复之间只执行一次）。但是，如上述所说，Raft 是可以多次执行同一条命令的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读（read-only）的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回过期数据(stale data)的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是它还不知道。线性化的读操作必须不能返回过期数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全原则（Leader Completeness Property）保证了领导人一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来进行实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废除了（如果一个更新的领导人被选举出来，它自己的信息就已经过期了）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳（heartbeat）信息来处理这个问题。另外，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时序来保证安全性（它假设时间误差是有界的）。 实现和评价我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。 这一章会从三个方面来评估 Raft 算法：可理解性、正确性和性能。 相关工作已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中： Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰的论文。 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 有着很大的差别。 Paxos 可以应用的性能优化。 Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。 Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。 像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。 和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。 Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。 一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致（joint consensus）的方法因为它对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Raft 没有采用 Lamport 的基于 α 的方法是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较而言，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。和 VR、SMART 比较而言，Raft 的方法同时需要更少的额外机制来实现。 总结算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。 在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。 本文的版权归作者 罗远航 所有，采用 Attribution-NonCommercial 3.0 License。任何人可以进行转载、分享，但不可在未经允许的情况下用于商业用途；转载请注明出处。感谢配合！]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的笔记，区块链概念！]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fblockchain-concept-20170418.html</url>
    <content type="text"><![CDATA[每个人都认为区块链技术（blockchain）会彻底改变企业的交易方式。我们首先介绍一下相关背景。 术语以下术语在此规范的有限范围内定义，以帮助读者清楚准确的了解这里所描述的概念。 交易(Transaction)是区块链上执行功能的一个请求。功能是使用链节点(chainnode)来实现的。 交易者(Transactor)是向客户端应用这样发出交易的实体。 总账(Ledger)是一系列包含交易和当前世界状态(World State)的加密的链接块。 世界状态(World State)是包含交易执行结果的变量集合。 链码(Chaincode)是作为交易的一部分保存在总账上的应用级的代码（如智能合约）。链节点运行的交易可能会改变世界状态。 验证Peer(Validating Peer)是网络中负责达成共识，验证交易并维护总账的一个计算节点。 非验证Peer(Non-validating Peer)是网络上作为代理把交易员连接到附近验证节点的计算节点。非验证Peer只验证交易但不执行它们。它还承载事件流服务和REST服务。 带有权限的总账(Permissioned Ledger)是一个由每个实体或节点都是网络成员所组成的区块链网络。匿名节点是不允许连接的。 隐私(Privacy)是链上的交易者需要隐瞒自己在网络上身份。虽然网络的成员可以查看交易，但是交易在没有得到特殊的权限前不能连接到交易者。 保密(Confidentiality)是交易的内容不能被非利益相关者访问到的功能。 可审计性(Auditability)作为商业用途的区块链需要遵守法规，很容易让监管机构审计交易记录。所以区块链是必须的。 模块介绍Fabric的构架由成员服务（Membership）、区块链服务（Blockchain）和链码服务（Chaincode）三个主要类别构成。这些类别仅仅是Fabric的逻辑结构，并不是在物理上将组件划分成不同的进程、地址空间或者虚拟机。 成员服务（Membership）成员服务为网络提供身份管理，隐私，保密和可审计性的服务。在一个不带权限的区块链中，参与者是不需要被授权的，且所有的节点都可以同样的提交交易并把它们汇集到可接受的块中，如：它们没有角色的区分。成员服务通过公钥基础设施(Public Key Infrastructure (PKI))和去中心化的/共识技术使得不带权限的区块链变成带权限的区块链。在后者中，通过实体注册来获得长时间的，可能根据实体类型生成的身份凭证（登记证书enrollment certificates）。在用户使用过程中，这样的证书允许交易证书颁发机构（Transaction Certificate Authority (TCA)）颁发匿名证书。这样的证书，如交易证书，被用来对提交交易授权。交易证书存储在区块链中，并对审计集群授权，否则交易是不可链接的。 区块链服务区块链服务通过 HTTP/2 上的点对点（peer-to-peer）协议来管理分布式总账。为了提供最高效的哈希算法来维护世界状态的复制，数据结构进行了高度的优化。每个部署中可以插入和配置不同的共识算法（PBFT, Raft, PoW, PoS） 链码服务链码服务提供一个安全的，轻量的沙箱在验证节点上执行链码。环境是一个“锁定的”且安全的包含签过名的安全操作系统镜像和链码语言，Go，Java 和 Node.js 的运行时和 SDK 层。可以根据需要来启用其他语言。 事件验证 peers 和链码可以向在网络上监听并采取行动的应用发送事件。这是一些预定义好的事件集合，链码可以生成客户化的事件。事件会被一个或多个事件适配器消费。之后适配器可能会把事件投递到其他设备，如 Web hooks 或 Kafka。 应用编程接口(API)fabric的主要接口是 REST API，并通过 Swagger 2.0 来改变。API 允许注册用户，区块链查询和发布交易。链码与执行交易的堆间的交互和交易的结果查询会由 API 集合来规范 命令行界面(CLI)CLI包含REST API的一个子集使得开发者能更快的测试链码或查询交易状态。CLI 是通过 Go 语言来实现，并可在多种操作系统上操作。 技术架构 其中最明显、最表层的变化，就是曾经的Validating/Non-Validating Peer没有了，取而代之的是Endorser（背书节点）和 Committer（提交节点）以及从Peer中剥离出的共识模块Orderer。而MemberService（成员管理）、Certificate Authority（证书管理）以及加密算法等模块目前我还没有发现有明显的变动。 扩展模块 Blockchain-explorer是为了便于Hyperledger应用浏览/查询区块信息、交易相关信息、网络信息（名称、状态，关联节点）、智能合约信息（浏览、调用、部署、查询）和其他相关信息而设计的Web应用。 Blockchain-explorer是为了便于Hyperledger应用浏览/查询区块信息、交易相关信息、网络信息（名称、状态，关联节点）、智能合约信息（浏览、调用、部署、查询）和其他相关信息而设计的Web应用。 Cello是一个用于部署区块链服务BSAS（Blockchain-as-a-Service）的工具，它可以帮助用户降低创建、管理和删除区块信息的复杂度。Cello的意义在于，它让区块链的定制化成为了可能，也就是说Cello可以在多种环境下：裸机、虚拟机和其他容器之上，提供有自治的多租户服务。 IrohaHyperledger Iroha 项目由Makoto Takemiya (Soramitsu), Toshiya Cho (Hitachi), Takahiro Inaba (NTT Data), and Mark Smargon (Colu)几个人提出。目前正处于孵化阶段。Iroha项目的目的在于将分布式账本技术便捷的应用于现有的基础项目上。 Sawtooth LakeSawtooth Lake是Intel主导的区块链应用组件，目的在于实现区块链技术的多用途和可扩展性。从物联网到金融，人们已经尝试将分布式账本技术应用在多个领域。Sawtooth Lake架构将兼顾各类不同的需求。Sawtooth Lake 支持有许可和无需许可的不是方式，并引入了新的共识算法：Proof of Elapsed Time (PoET)。PoET可以减少肌群达到共识所消耗的资源。交易逻辑的管理由Transaction Families负责，从共识管理层剥离。这样将大大减少交易逻辑的约束。 交易流程交易从发起到最终完成共识一共需要4个步骤 交易流程交易从发起到最终完成共识一共需要4个步骤 Client想发起一个交易，它首先要把交易的相关信息（propose message）发给它所选择的Endorser节点。这里解释一点，Endorser的选择是有一定范围的，并不是在所有的Endorser里面随意选择，是由交易所属的Chaincode和该Chaincode所定义的Endorsement Policy共同决定的。 Endorser节点收到上述信息后，首先用Client的公钥验证它的签名，然后开始进行模拟交易（不会写到账本里）并核实相关信息（这里省略了一些信息的介绍，太过技术性就略过了），然后根据Endorsement Policy选择是否为该交易背书（Transaction Endorsed），然后把结果发回给提交的Client。 提交交易的Client收集各个Endorser返回的信息，如果得到了“足够”的背书信息后，就说明这个交易通过了Endorsement阶段。其中足够的数量是多少，依然取决于Endorsement Policy是如何规定的；相反如果Client没有收集到足够的信息的话，这个交易会被废止掉，Client可以选择重新发起交易。而那些通过了Endorsement阶段的交易，就会进入到共识阶段。 共识阶段虽然有不同的算法，不过目的都是把有效的交易加入新生成的区块，并通知所有的节点，使他们的账本保持一致性。共识机制要达成的从根本上讲就是一种保证所有节点都认可的一致性状态时序，Fabric v1.0对所有交易消息进行排序并全序广播相关节点。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非root用户不使用sudo执行docker命令]]></title>
    <url>%2Fdocker%2Fdocker-non-root-20170412.html</url>
    <content type="text"><![CDATA[docker daemon须运行在root用户下，非root用户要使用sudo，原因在官方文档写到： The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The docker daemon always runs as the root user. If you don’t want to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read/writable by the docker group. 非root用户不使用sudo执行docker命令 $ sudo groupadd docker创建docker用户组。 $ sudo usermod -aG docker $USER添加用户到docker用户组。 $ sudo systemctl restart docker.service重启docker daemon并重新登录 $ docker run hello-world测试下吧。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>non-root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为下载docker镜像加速，配置国内Registry Mirror]]></title>
    <url>%2Fdocker%2Fdocker-mirror-20170410.html</url>
    <content type="text"><![CDATA[Docker是Docker.Inc公司开源的一个基于轻量级虚拟化技术的容器引擎项目,整个项目基于Go语言开发，并遵从Apache 2.0协议。通过分层镜像标准化和内核虚拟化技术，Docker使得应用开发者和运维工程师可以以统一的方式跨平台发布应用，并且以几乎没有额外开销的情 况下提供资源隔离的应用运行环境。由于众多新颖的特性以及项目本身的开放性，Docker在不到两年的时间里迅速获得诸多IT厂商的参与，其中更是包括 Google、Microsoft、VMware等业界行业领导者。同时，Docker在开发者社区也是一石激起千层浪，许多如我之码农纷纷开始关注、学 习和使用Docker，许多企业，尤其是互联网企业，也在不断加大对Docker的投入，大有掀起一场容器革命之势。 Docker镜像命名解析镜像是Docker最核心的技术之一，也是应用发布的标准格式。这也是Docker里面比较容易令人混淆的一块概念：Registry，Repository, Tag and Image。Registry存储镜像数据，并且提供拉取和上传镜像的功能。Registry中镜像是通过Repository来组织的，而每个Repository又包含了若干个Image。 Registry包含一个或多个Repository Repository包含一个或多个Image Image用GUID表示，有一个或多个Tag与之关联 docker pull在哪里指定Registry呢？假如本地搭建了localhost:5000的私有Registry。拉取镜像的完整命令：1docker pull localhost:5000/ubuntu:latest 加速方案方案一就是搭建或者使用现有的私有Registry，通过定期和Docker Hub同步热门的镜像，私有Registry上保存了一些镜像的副本，然后大家可以通过docker pull private-registry.com/user-name/ubuntu:latest， 从这个私有Registry上拉取镜像。因为这个方案需要定期同步Docker Hub镜像，因此它比较适合于使用的镜像相对稳定，或者都是私有镜像的场景。而且用户需要显式的映射官方镜像名称到私有镜像名称，私有Registry更多被大家应用在企业内部场景。私有Registry部署也很方便，可以直接在Docker Hub上下载Registry镜像。 方案二是使用Registry Mirror，它的原理类似于缓存，如果镜像在Mirror中命中则直接返回给客户端，否则从存放镜像的Registry上拉取并自动缓存在Mirror 中。最酷的是，是否使用Mirror对Docker使用者来讲是透明的，也就是说在配置Mirror以后，大家可以仍然输入docker pull ubuntu来拉取Docker Hub镜像，除了速度变快了，和以前没有任何区别。 配置Registry Mirror使用DaoCloud国内加速器下载，配置： Linux该脚本可以将 –registry-mirror 加入到你的 Docker 配置文件 /etc/default/docker 中。适用于 Ubuntu14.04、Debian、CentOS6 、CentOS7、Fedora、Arch Linux、openSUSE Leap 42.1，其他版本可能有细微不同。有些是修改/etc/docker/daemon.json1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://c1a47088.m.daocloud.io MacOS右键点击桌面顶栏的 docker 图标，选择 Preferences ，在 Daemon 标签（Docker 17.03 之前版本为 Advanced 标签）下的 Registry mirrors 列表中加入下面的镜像地址:1http://c1a47088.m.daocloud.io Window在桌面右下角状态栏中右键 docker 图标，修改在 Docker Daemon 标签页中的 json ，把下面的地址:1http://c1a47088.m.daocloud.io 现在可以愉快的下载镜像了。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mirror</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环形数据库rrd及rrdtool介绍]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Frrd-intro-20170408.html</url>
    <content type="text"><![CDATA[RRD 全称是 Round Robin Database ，即「环型数据库」。顾名思义，它是一种循环使用存储空间的数据库，适用于存储和时间序列相关的数据。RRDTool是RRD的实现工具，可用于存储和展示被监测对象随时间的变化情况。 “Round Robin”是一种存储数据的方式，使用固定大小的空间来存储数据，并有一个指针指向最新的数据的位置。存储数据的数据库的空间看成一个圆，上面有很多刻度，每个刻度上可以存储一个数值，同时有一个从圆心指向最新存储值的指针。随着时间推移，指针会绕着圆心一直移动下去，当它指向下一个刻度后，就可以在那个位置上存储一个新的数值。在一段时间后，当所有的空间都存满了数据，就又从头开始存放。这样整个存储空间的大小就是一个固定的数值。 RRD术语 ds：Data Source (ds) providing input to the database.定义数据源(Data Source)属性，包括数据源名称 ds-name，比如我们可以给监测内存使用率的数据源命名为memory-rate。 dst：Data Source Type (dst).数据源类型(Data Source Type)，常用的有以下4种数据源类型GAUGE、COUNTER、ABSOLUTE、DERIVE。 pdp： Primary Data Point (pdp).原始数据点，例如cpu的监控数据每分钟上报，每分钟上报的的数据为原始数据。 cdp：Consolidated Data Point (cdp) is the long term storage format for data in the rrd database.归档数据点，原始数据通过归档函数计算出的数据。 rra：Round Robin Archive (rra). This is the place where the consolidated data points (cdp) get stored.RRA (Round Robin Archive) 是用来定义RRD数据库归档模型 cf：Consolidation Function (cf).归档函数，表示{AVERAGE | MIN | MAX | LAST}中的一个归档函数。 xff: xfiles factor.xfiles 因子(factor)，表示超过多少比率的有效 PDP 才可以计算出 CDP，通常为0.5。 数据据源类型(Data Source Type) GAUGE实测值，RRD将如实记录，比如温度变化曲线 时间 监控值 存储值 10:00 6℃ 6℃ 11:00 11℃ 11℃ 12:00 14℃ 14℃ 13:00 12℃ 12℃ 14:00 10℃ 10℃ COUNTER计数值，这是一个只增不减的正整数。比如，汽车行驶里程，从汽车第一次上路开始，里程就从0开始不断增长。假设每隔30分钟监测一次汽车里程，当RRD收到COUNTER 类型的数据时，并不会像 GAUGE 类型那样直接存储，而是计算变化率： 时间 监控值 存储值 10:00 12100km Unknown 10:30 12121km 6.11m/s 11:00 12135km 7.78m/s 11:30 12160km 13.89m/s 计算原理： (12121km - 12100km) / (10:30 - 10:00) = 11000m / 1800s = 6.11m/s RRD对于COUNTER 类型的数据源存储的是变化率，对于上述里程表而言就是行驶速度。注：第一个存储值为 UNKNOWN，因为没有更早的数据，所以没有变化可言） ABSOLUTEABSOLUTE 类型存储的也是变化率，假设我们正在微信和好友聊天，每五分钟我们会看一下有没有新消息，如果有的话就立即处理，这样未读提醒就会变为0，然后下一个五分钟后继续看未读新消息数，会得到这样一个监测表： 时间 监控值 存储值 10:00 100条 Unknown 10:05 120条 0.4条/s 10:10 300条 1条/s 10:15 99条 0.33条/s 计算原理：120条 / 300秒 = 0.4条/秒 DERIVEDERIVE类型存储的也是变化率，和 COUNTER 类型不同的是，监测值可以增长也可以下降，例如水库的水位监测： 时间 监控值 存储值 10:00 1000cm Unknown 10:10 1200cm 0.33cm/s 10:20 800cm -0.67cm/s 10:30 1000km 0.33cm/s 计算原理： (1200cm - 1000km) / (10:10 - 10:00) = 200cm / 600s = 0.33cm/s 归档模型RRA:CF:xff:steps:rows 归档函数CF假如每秒一个原始数据上报，如果我们把每小时监测的3600个原始数据点计算一个归档平均值的话。归档函数AVERAGE。 RRD提供的归档方法有4种，除了上述的计算平均值AVERAGE方法外，还有： 12345计算最大值 MAX(d1,d2,d3,...dn) = 最大的那个监测值计算最小值 MIN(d1,d2,d3,...dn) = 最小的那个监测值计算最后值 LAST(d1,d2,d3,...dn) = 最后的那个监测值 xfiles因子xff默认值为0.5，即有大于50%的有效监测值就可以在这些有效值上计算出归档值，否则这段时间内的归档值记为 UNKNOWN。 steps表示多少个PDP计算出一个CDP。例如，在每秒获取一个监测值的实例中，steps = 60 表示每60个原始数据计算一个归档数据，即一分钟一个数据点。 rows表示多少个CDP组成一个RRA。例如，在每秒获取一个监测值的实例中，steps = 60，rows=60，即一小时数据组成一个RRA。 RRD文件结构 时间 监控值 存储值 char cookie[4] RRD\0 RRD文件标志 char version[5] 0003\0 RRD文件版本 double float_cookie 8.642135E130 Magic number unsigned long ds_cnt 定义的DS个数 unsigned long rra_cnt 定义的RRA个数 unsigned long pdp_step pdp时间间隔 unival par[10] 保留，未使用 char ds_nam[DS_NAM_SIZE] DS的名称，DS_NAM_SIZE=20 char dst[DST_SIZE] DS的类型 unival par[10] DS的参数队列(heartbeat:min:max) DS参数cnt次 char cf_nam[CF_NAM_SIZE] CF的名称 unsigned long row_cnt 存储记录的行数 unsigned long pdp_cnt cf函数执行时，需要的pdp个数 unival par[MAX_RRA_PAR_EN] RRA的参数队列 RRA参数cnt次 time_t last_up 最后一次更新的秒数部分 long last_up_usec 最后一次跟新的微秒数部分 char last_ds[LAST_DS_LEN] 最后一次更新后ds的值 unival scratch[10] 最后一次更新后pdp的相关数值 DS参数cnt次 Unival scratch[MAX_CDP_PAR_EN] 最后一次更新后cdp的相关数值 DS参数cnt次*RRA参数cnt次 unsigned long cur_row 指向RRA当前的记录 RRA参数cnt次 RRDTool语法Create语法： 12345rrdtool create filename [–start|-b start time] [–step|-s step] \[DS:ds-name:DST:dst arguments] \[RRA:CF:cf arguments] Update语法： 1rrdtool update filename [timestamp:value timestamp:value timestamp:value]]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>rrd</tag>
        <tag>环形数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解Golang中defer, 注意使用时的坑]]></title>
    <url>%2FGolang%2Fgolang-defer-explain-20170324.html</url>
    <content type="text"><![CDATA[defer和go一样都是Go语言提供的关键字。defer用于资源的释放，会在函数返回之前进行调用。一般采用如下模式：12345f,err := os.Open(filename) if err != nil &#123; panic(err)&#125;defer f.Close() 如果有多个defer表达式，调用顺序类似于栈，越后面的defer表达式越先被调用。不过如果对defer的了解不够深入，使用起来可能会踩到一些坑，尤其是跟带命名的返回参数一起使用时。在讲解defer的实现之前先看一看使用defer容易遇到的问题。 先来看看几个例子。例1：123456func f() (result int) &#123; defer func() &#123; result++ &#125;() return 0&#125; 例2：1234567func f() (r int) &#123; t := 5 defer func() &#123; t = t + 5 &#125;() return t&#125; 例3：123456func f() (r int) &#123; defer func(r int) &#123; r = r + 5 &#125;(r) return 1&#125; 请读者先不要运行代码，在心里跑一遍结果，然后去验证。例1的正确答案不是0，例2的正确答案不是10，如果例3的正确答案不是6…… defer是在return之前执行的。这个在 官方文档中是明确说明了的。要使用defer时不踩坑，最重要的一点就是要明白，return xxx这一条语句并不是一条原子指令! 函数返回的过程是这样的：先给返回值赋值，然后调用defer表达式，最后才是返回到调用函数中。defer表达式可能会在设置函数返回值之后，在返回到调用函数之前，修改返回值，使最终的函数返回值与你想象的不一致。其实使用defer时，用一个简单的转换规则改写一下，就不会迷糊了。改写规则是将return语句拆成两句写，return xxx会被改写成:123返回值 = xxx调用defer函数空的return 先看例1，它可以改写成这样：1234567func f() (result int) &#123; result = 0 //return语句不是一条原子调用，return xxx其实是赋值＋ret指令 func() &#123; //defer被插入到return之前执行，也就是赋返回值和ret指令之间 result++ &#125;() return&#125; 所以这个返回值是1。 再看例2，它可以改写成这样：12345678func f() (r int) &#123; t := 5 r = t //赋值指令 func() &#123; //defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过 t = t + 5 &#125; return //空的return指令&#125; 所以这个的结果是5。最后看例3，它改写后变成：1234567func f() (r int) &#123; r = 1 //给返回值赋值 func(r int) &#123; //这里改的r是传值传进去的r，不会改变要返回的那个r值 r = r + 5 &#125;(r) return //空的return&#125; 所以这个例子的结果是1。defer确实是在return之前调用的。但表现形式上却可能不像。本质原因是return xxx语句并不是一条原子指令，defer被插入到了赋值 与 ret之间，因此可能有机会改变最终的返回值。 defer关键字的实现跟go关键字很类似,不同的是它调用的是runtime.deferproc而不是runtime.newproc。 在defer出现的地方,插入了指令call runtime.deferproc,然后在函数返回之前的地方,插入指令call runtime.deferreturn。]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>linux</tag>
        <tag>defer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链的那些事，你知道和不知道的都在这里！]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fblockchain-post-collection-20170318.html</url>
    <content type="text"><![CDATA[摘要： 区块链（Blockchain）是比特币的一个重要概念，本质上是一个去中心化的数据库，同时作为比特币的底层技术。本文将前一段时间整理的区块链技术文章做成了集合，以供大家学习。 今年以来“区块链”的概念可以说是异常火爆，好像互联网金融峰会上没人谈一谈区块链技术就out了，BAT以及各大银行还有什么金融机构都在开始自己的区块链研究工作，就连IBM最近也成立了自己的区块链研究实验室，但其实区块链到底是什么？大家或许并不清楚，停留在雾里看花的状态。前一段时间为大家整理一个区块链学习系列文章，在这里为大家提供一个文章集合，大家就和我一起走进区块链吧，揭开区块链的神秘面纱吧！ 【区块链之菜鸟入门】系列文章 亲，你淘的区块链到了！ 区块链发展史：从拜占庭将军问题到智慧契约 来来来，这篇科普告诉你“区块链”到底是个啥？ 区块链——颠覆式创新技术 【区块链之技术进阶】系列文章 区块链：通往互联网第二纪元的革命 金融的未来，区块链将用于何方？ 让这篇技术贴告诉你区块链是怎么运行的 扒一扒某乎上面对于区块链的理解（一） 扒一扒某乎上面对于区块链的理解（二） 从技术现实理解区块链：基于SQL模型创建BQL 掰一掰区块链共识机制与分布式一致性算法 Attention please！区块链技术的风险！ 【区块链之技术实战】系列文章 群雄激辩区块链 颠覆银行基础架构的区块链 在金融领域，区块链该咋用呢？ 区块链开源项目合集：Hello，BlockChain！ 有了区块链做公益，再也不用担心我的捐款啦 【区块链与未来】系列文章 区块链只与互联网+金融有关？让法律人和你聊聊 “区块链+”火花四溅的未来 “公共记账簿”？哼，区块链远比你想象的强大的多 区块链技术将重塑我们的世界 其他区块链好文 【中生代】区块链技术分享 蚂蚁金服首席架构师：区块链技术如何促进数字普惠金融 区块链——2016最耀眼的新兴技术之一 【云栖大会】阿里云邮箱发布首个基于区块链的邮箱存证产品 法链背后的秘密 【云栖大会】详解区块链电子存证 分布式账本 从理想到现实， 你不知道的区块链 https://yq.aliyun.com/articles/65264?spm=5176.8091938.0.0.KUZxTb]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang中类型变量在内存中的布局]]></title>
    <url>%2FGolang%2Fgolang-types-in-memroy-20170314.html</url>
    <content type="text"><![CDATA[了解Golang中的各种类型变量在内存中的布局，有利于帮助我们加深理解。另一种更直接的方式是阅读源码，之后阅读rumtime包深入去了解Golang的运行机制。 基础类型 变量i类型int，在内存中是一个32位字长。 变量j类型int32，做了精确的转换。i和j相同内存布局，但类型不同。 变量f类型float，使用32位浮点型值表示。与int32的内部实现不同。 变量bytes类型[5]byte，一个由5个字节组成数组。内存表示是连起来的5个字节。primes是4个int的数组。 结构体与指针1type Point struct &#123; X, Y int &#125; Point{10,20} 表示一个已初始化的Point类型。对它进行取地址表示一个指向刚刚分配和初始化的Point类型的指针。前者在内存中是两个值，而后者是一个指向两个值的指针。结构体的域在内存中是紧挨着排列的。 字符串 字符串在Go语言内存模型中用一个2字长的数据结构表示。它包含一个指向字符串存储数据的指针和一个长度数据。因为string类型是不可变的，对于多字符串共享同一个存储数据是安全的。切分操作 str[i:j] 会得到一个新的2字长结构，一个可能不同的但仍指向同一个字节序列(即上文说的存储数据)的指针和长度数据。这意味着字符串切分可以在不涉及内存分配或复制操作。这使得字符串切分的效率等同于传递下标。（说句题外话，在Java和其他语言里有一个有名的“疑难杂症”：在你分割字符串并保存时，对于源字符串的引用在内存中仍然保存着完整的原始字符串–即使只有一小部分仍被需要，Go也有这个“毛病”。另一方面，我们努力但又失败了的是，让字符串分割操作变得昂贵–包含一次分配和一次复制。在大多数程序中都避免了这么做。） 切片Slice一个slice是一个数组某个部分的引用。在内存中，它是一个包含3个域的结构体：指向slice中第一个元素的指针，slice的长度，以及slice的容量。长度是下标操作的上界，如x[i]中i必须小于长度。容量是分割操作的上界，如x[i:j]中j不能大于容量。 数组的slice并不会实际复制一份数据，它只是创建一个新的数据结构，包含了另外的一个指针，一个长度和一个容量数据。如同分割一个字符串，分割数组也不涉及复制操作：它只是新建了一个结构来放置一个不同的指针，长度和容量。在例子中，对 []int{2,3,5,7,11} 求值操作会创建一个包含五个值的数组，并设置x的属性来描述这个数组。分割表达式 x[1:3] 并不分配更多的数据：它只是写了一个新的slice结构的属性来引用相同的存储数据。在例子中，长度为2–只有y[0]和y[1]是有效的索引，但是容量为4–y[0:4]是一个有效的分割表达式。由于slice是不同于指针的多字长结构，分割操作并不需要分配内存，甚至没有通常被保存在堆中的slice头部。这种表示方法使slice操作和在C中传递指针、长度对一样廉价。Go语言最初使用一个指向以上结构的指针来表示slice，但是这样做意味着每个slice操作都会分配一块新的内存对象。即使使用了快速的分配器，还是给垃圾收集器制造了很多没有必要的工作。移除间接引用及分配操作可以让slice足够廉价，以避免传递显式索引。 make和newGo有两个数据结构创建函数：new和make。两者的区别在学习Go语言的初期是一个常见的混淆点。基本的区别是 new(T) 返回一个 *T ，返回的这个指针可以被隐式地消除引用（图中的黑色箭头）。而 make(T, args) 返回一个普通的T。通常情况下，T内部有一些隐式的指针（图中的灰色箭头）。一句话，new返回一个指向已清零内存的指针，而make返回一个复杂的结构。 零值与nil的语义按照Go语言规范，任何类型在未初始化时都对应一个零值：布尔类型是false，整型是0，字符串是””，而指针，函数，interface，slice，channel和map的零值都是nil。 string的空值是””，它是不能跟nil比较的。即使是空的string，它的大小也是两个机器字长的。slice也类似，它的空值并不是一个空指针，而是结构体中的指针域为空，空的slice的大小也是三个机器字长的。]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>type</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[适配java fastjson包解析Map的Key类型int的json串]]></title>
    <url>%2FGolang%2Fgolang-ffjson-for-int-key-20170309.html</url>
    <content type="text"><![CDATA[首先说明json是不支持{&quot;Bar&quot;:&quot;test&quot;,&quot;M&quot;:{1:&quot;a&quot;,2:&quot;b&quot;}}key是int的json字符串，官方encoding/json包也不解析这类json串。为什么还要解析这样的josn串呢。java的fastjson对Map解析生产1:&quot;a&quot;,2:&quot;b&quot;的非标准json串。接下来就来实现解析这类非标准的json串吧。 json接口encoding/json包中定义了json.Unmarshaler与json.Marshaler接口来解析json。对结构体实现这两个接口，一切都搞定了。 1234567type Unmarshaler interface &#123; UnmarshalJSON([]byte) error&#125;type Marshaler interface &#123; MarshalJSON() ([]byte, error)&#125; 接口实现我使用Golang ffjson第三方包，自己没有去实现此功能。这里说下思路，UnmarshalJSON接口实现中step解析key没有&quot;判断是否map[int]string类型，否则返回错误。 使用ffjosn先给出示例代码123456package mtype Foo struct &#123; Bar string M map[int]string&#125; 12345678910111213141516171819package mainimport ( "encoding/json" "fmt" "github.com/gunsluo/go-example/ffjson/m")func main() &#123; buf := []byte(`&#123;"Bar":"test","M":&#123;1:"a",2:"b"&#125;&#125;`) nf := new(m.Foo) err := json.Unmarshal(buf, nf) if err != nil &#123; panic(err) &#125; fmt.Println("--&gt;", nf)&#125; 使用json包，报错panic: invalid character &#39;1&#39; looking for beginning of object key string 将json换成ffjson1234567891011121314151617181920package mainimport ( "fmt" "github.com/gunsluo/go-example/ffjson/m" "github.com/pquerna/ffjson/ffjson")func main() &#123; buf := []byte(`&#123;"Bar":"test","M":&#123;1:"a",2:"b"&#125;&#125;`) nf := new(m.Foo) err := ffjson.Unmarshal(buf, nf) if err != nil &#123; panic(err) &#125; fmt.Println("--&gt;", nf)&#125; 直接执行代码会报相同的错误，ffjson检查到Foo没有实现ffjson.unmarshalFaster或重写json.Unmarshaler会使用json来代替解析。 需要执行命令(go get -u github.com/pquerna/ffjson ffjson后会安装到GOPATH/bin目录下)1ffjson foo.go 生成foo_ffjson.go文件，接着查看下该文件。 1234567891011121314151617181920212223242526272829303132func (mj *Foo) MarshalJSON() ([]byte, error) &#123; var buf fflib.Buffer if mj == nil &#123; buf.WriteString("null") return buf.Bytes(), nil &#125; err := mj.MarshalJSONBuf(&amp;buf) if err != nil &#123; return nil, err &#125; return buf.Bytes(), nil&#125;func (mj *Foo) MarshalJSONBuf(buf fflib.EncodingBuffer) error &#123; if mj == nil &#123; buf.WriteString("null") return nil &#125; var err error var obj []byte _ = obj _ = err buf.WriteString(`&#123;"Bar":`) fflib.WriteJsonString(buf, string(mj.Bar)) /* Falling back. type=map[int]string kind=map */ buf.WriteString(`,"M":`) err = buf.Encode(mj.M) if err != nil &#123; return err &#125; buf.WriteByte('&#125;') return nil&#125; 上面截取了foo_ffjson.go的部分内容，它重写了MarshalJSON和MarshalJSONBuf方法。 最后执行代码，解析成功。 说明：非标准的json串是不提倡的，这里给出了其它系统或语言出现这种情况的处理方法。]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang代码调试器delve，必先利其器]]></title>
    <url>%2FGolang%2Fgolang-debugger-delve-20170302.html</url>
    <content type="text"><![CDATA[上篇文章使用gdb作为调试器，本文介绍使用delve代码调试器，记录学习笔记。 安装要求go1.5及以上版本 brew安装1brew install go-delve/delve/delve 源码编译创建证书步骤参看《用Golang代码调试器gdb, 了解interface的存储结构》，这里不再赘述。 下载源码1git clone https://github.com/derekparker/delve.git &amp;&amp; cd delve 编译安装1CERT=dlv-cert make install dlv-cert是创建的证书名 调试delve命令和gdb比较类似，先看下delve的命令1dlv -h 以相同的代码作为示例 1234567891011121314151617181920212223242526package maintype Ner interface &#123; a() b(int) c(string) string&#125;type N intfunc (N) a() &#123;&#125;func (*N) b(int) &#123;&#125;func (*N) c(string) string &#123; return ""&#125;func main() &#123; var n N var ner Ner = &amp;n ner.a()&#125; 编译Go程序的时候无需注意以下两点 传递参数-ldflags “-s”，忽略debug的打印信息 传递-gcflags “-N -l” 参数，这样可以忽略Go内部做的一些优化，聚合变量和函数等优化，这样对于GDB调试来说非常困难，所以在编译的时候加入这两个参数避免这些优化。 编译示例代码go build main.go，目录下生成main可执行文件 dlv exec ./main调试代码或者dlv debug main.go(debug会自己完成编译) 调试代码 高级用法delve支持调试已经运行的golang程序。 示例代码如下：12345678910111213141516171819202122232425package mainimport ( "fmt" "sync" "time")func dostuff(wg *sync.WaitGroup, i int) &#123; fmt.Printf("goroutine id %d\n", i) time.Sleep(20 * time.Second) fmt.Printf("goroutine id %d\n", i) wg.Done()&#125;func main() &#123; var wg sync.WaitGroup workers := 10 wg.Add(workers) for i := 0; i &lt; workers; i++ &#123; go dostuff(&amp;wg, i) &#125; wg.Wait()&#125; 代码创建了10个gorouting，main等待所有gorouting执行完成后退出。 先编译示例代码go build main.go 启动./main进程，查看进程pid delve attach 调试进程，设置断点 delve还支持远程调试哦，使用dlv connect ip:port。好了，今天就写到这里了。 总体来说delve比gdb更适合于调试golang。]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>debugger</tag>
        <tag>delve</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Golang代码调试器gdb, 了解interface的存储结构]]></title>
    <url>%2FGolang%2Fgolang-debugger-gdb-20170302.html</url>
    <content type="text"><![CDATA[Golang语言的接口实现是隐式声明，目标类只要实现接口定义的所有方法，就被视为实现了接口。接口使用了一个名为itab的结构存储运行期所需的相关类型信息。现在利用gdb调试器来查看结构存储的具体内容。 调试器gdbgdb7.x版本支持golang语言，首先下载gdb并安装就可以进行调试。下面介绍下mac下如何安装gdb并授权。 安装gdbbrew install gdb mac上安装完成dgb后，dgb默认是没有读取其它进程的权限，需要安装授权证书赋予权限。 打开Keychain Access，创建新证书。没有截取图片的按默认选择下一步。 创建证书后，Get info中修改授权 授权gdb，好了现在可以使用gdb调试我们的golang代码了。123sudo killall taskgatedcodesign -fs gdb-cert /usr/local/bin/gdblaunchctl load /System/Library/LaunchDaemons/com.apple.taskgated.plist Golang接口的存储结构12345678910type iface struct &#123; tab *itab //类型信息 data unsafe.Pointer //实际对象指针&#125;type itab struct &#123; inter *interfacetype //接口类型 _type *_type //实际对象类型 fun [1]uintptr //实际对象方法地址&#125; 接口示例1234567891011121314151617181920212223242526package maintype Ner interface &#123; a() b(int) c(string) string&#125;type N intfunc (N) a() &#123;&#125;func (*N) b(int) &#123;&#125;func (*N) c(string) string &#123; return ""&#125;func main() &#123; var n N var ner Ner = &amp;n ner.a()&#125; 编译编译Go程序的时候需要注意以下几点 传递参数-ldflags “-s”，忽略debug的打印信息 传递-gcflags “-N -l” 参数，这样可以忽略Go内部做的一些优化，聚合变量和函数等优化，这样对于GDB调试来说非常困难，所以在编译的时候加入这两个参数避免这些优化。 编译示例代码go build -gcflags &quot;-N -l&quot; -ldflags &quot;-s&quot; main.go，目录下生成main可执行文件 gdb调试1gdb main 如图，gdb打印出了接口的结构。你也去试试吧。 关于gdb引入官方gdb说明 GDB does not understand Go programs well. The stack management, threading, and runtime contain aspects that differ enough from the execution model GDB expects that they can confuse the debugger, even when the program is compiled with gccgo. As a consequence, although GDB can be useful in some situations, it is not a reliable debugger for Go programs, particularly heavily concurrent ones. Moreover, it is not a priority for the Go project to address these issues, which are difficult. In short, the instructions below should be taken only as a guide to how to use GDB when it works, not as a guarantee of success.In time, a more Go-centric debugging architecture may be required. golang官网说gdb能解决大部情况的调试，在对大并发的情况有可能达不到期望。下一篇文章将介绍delve调试器]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>mac</tag>
        <tag>debugger</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16 tls安装自己的Bit下载工具Deluge]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fdeluge-install-ubuntu-20170218.html</url>
    <content type="text"><![CDATA[常用的bit下载软件：uTorrent(Windows用户推荐)、 Azureus、 BitTornado、 KTorrent、 RTorrent、 Transmission、 Deluge。捣鼓了下载机，安装了系统ubuntu16 tls，现在就选择deluge作为bit下载软件。这里记录下安装流程： 安装步骤deluge客户端安装1234sudo add-apt-repository ppa:deluge-team/ppasudo apt updatesudo apt install deluge 安装完成可以在Unity Dash或者application menu中找到Deluge BitTorrent Client。 deluge客户端安装1234sudo add-apt-repository ppa:deluge-team/ppasudo apt updatesudo apt install deluge deluge web安装12345sudo add-apt-repository ppa:deluge-team/ppasudo apt-get updatesudo apt-get install deluge-webui deluged 添加deluge运行用户1sudo adduser --system --gecos "Deluge Service" --disabled-password --group --home /home/deluge deluge --disabled-password 禁止deluge用户登录。 添加用户到deluge用户组1sudo gpasswd -a your-user-name deluge 配置deluge客户端配置sudo vim /etc/systemd/system/deluged.service，拷贝下面内容到文件中：12345678910111213141516171819[Unit]Description=Deluge Bittorrent Client DaemonAfter=network-online.target[Service]Type=simpleUser=delugeGroup=delugeUMask=007ExecStart=/usr/bin/deluged -dRestart=on-failure# Configures the time to wait before service is stopped forcefully.TimeoutStopSec=300[Install]WantedBy=multi-user.target deluge启动命令12345systemctl start delugedsystemctl enable delugedsystemctl status deluged 配置deluge web配置sudo vim /etc/systemd/system/deluged-web.service，拷贝下面内容到文件中：1234567891011121314151617[Unit]Description=Deluge Bittorrent Client Web InterfaceAfter=network-online.target[Service]Type=simpleUser=delugeGroup=delugeUMask=027ExecStart=/usr/bin/deluge-webRestart=on-failure[Install]WantedBy=multi-user.target deluge web启动命令12345systemctl start deluge-websystemctl enable deluge-websystemctl status deluge-web 浏览器登录浏览器中输入：your-server-ip:8112 监听127.0.0.1:58846连接Deluge daemon 修改密码 下载界面 Ok，现在可以使用deluge下载上传文件了。午后随笔记录]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>deluge</tag>
        <tag>torrent</tag>
        <tag>bit</tag>
        <tag>下载工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）分布式一致性哈希环]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fconsistent-hash-intro-20170216.html</url>
    <content type="text"><![CDATA[哈希表的原理与实现一列键值对数据，存储在一个table中，如何通过数据的关键字快速查找相应值呢？不要告诉我一个个拿出来比较key啊，呵呵。 大家都知道，在所有的线性数据结构中，数组的定位速度最快，因为它可通过数组下标直接定位到相应的数组空间，就不需要一个个查找。而哈希表就是利用数组这个能够快速定位数据的结构解决以上的问题的。 具体如何做呢？大家是否有注意到前面说的话：“数组可以通过下标直接定位到相应的空间”，对就是这句，哈希表的做法其实很简单，就是把Key通过一个固定的算法函数，既所谓的哈希函数转换成一个整型数字，然后就将该数字对数组长度进行取余，取余结果就当作数组的下标，将value存储在以该数字为下标的数组空间里，而当使用哈希表进行查询的时候，就是再次使用哈希函数将key转换为对应的数组下标，并定位到该空间获取value，如此一来，就可以充分利用到数组的定位性能进行数据定位。 不知道说到这里，一些不了解的朋友是否大概了解了哈希表的原理，其实就是通过空间换取时间的做法。到这里，可能有的朋友就会问，哈希函数对key进行转换，取余的值一定是唯一的吗？这个当然不能保证，主要是由于hashcode会对数组长度进行取余，因此其结果由于数组长度的限制必然会出现重复，所以就会有“冲突”这一问题，至于解决冲突的办法其实有很多种，比如重复散列的方式，大概就是定位的空间已经存在value且key不同的话就重新进行哈希加一并求模数组元素个数，既 (h(k)+i) mod S , i=1,2,3…… ，直到找到空间为止。还有其他的方式大家如果有兴趣的话可以自己找找资料看看。 Hash表这种数据结构在java中是原生的一个集合对象，在实际中用途极广，主要有这么几个特点： 访问速度快 大小不受限制 按键进行索引，没有重复对象 用字符串(id:string)检索对象(object) 今天整理以前写的一些算法，翻出来一个hash表的实现，就贴出来，自己也温习温习。先看看头文件，也就是数据结构的定义，相当于java中的接口的概念：123456789101112131415#include &lt;stdio.h&gt; #define HASHSIZE 256 //定义hash表中的节点的类型struct nlist&#123; struct nlist *next; char *name; char *defn;&#125;; //定义接口中的函数，也就是对外来说，这个程序可以做什么unsigned hash(char*s);//计算一个串的hash值struct nlist *lookup(char*s);//查找一个value，根据keystruct nlist *install(char*name,char*defn);//插入一个key=value的对象 然后是具体实现：123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;string.h&gt; #include"list.h" staticstructnlist *hashtab[HASHSIZE]; unsigned hash(char*s) //取得hash值&#123; unsigned hashval; for(hashval = 0; *s !='\0';s++) hashval = *s + 31 * hashval; returnhashval % HASHSIZE;&#125; struct nlist *lookup(char*s)&#123; struct nlist *np; for(np = hashtab[hash(s)]; np != NULL; np = np-&gt;next) if(strcmp(s,np-&gt;name) == 0) returnnp; returnNULL;&#125; struct nlist *install(char*name,char*defn)&#123; struct nlist *np; unsigned hashval; if((np = lookup(name)) == NULL)&#123; np = (structnlist *)malloc(sizeof(structnlist)); if(np == NULL || (np-&gt;name = strdup(name)) == NULL) returnNULL; hashval = hash(name); np-&gt;next= hashtab[hashval]; hashtab[hashval] = np; &#125;else free((void*)np-&gt;defn); if((np-&gt;defn = strdup(defn)) == NULL) returnNULL; returnnp;&#125; 很简单，只有两个外部接口， install(key, value),用来插入一个新的节点 lookup(key),根据一个键来进行搜索，并返回节点 代码很简单，主要用到的hash算法跟java中的String的hashcode()方法中用到的算法一样，使用：1234567unsigned hash(char*s)&#123; unsigned hashval; for(hashval = 0; *s !='\0';s++) hashval = *s + 31 * hashval; returnhashval % HASHSIZE;&#125; 一致性hash算法consistent hashing 一致性 hash 算法早在 1997 年就在论文 Consistent hashing and random trees 中被提出，目前在 cache 系统中应用越来越广泛。 基本场景比如你有 N 个 cache 服务器（后面简称 cache ），那么如何将一个对象 object 映射到 N 个 cache 上呢，你很可能会采用类似下面的通用方法计算 object 的 hash 值，然后均匀的映射到到 N 个 cache： hash(object)%N 一切都运行正常，再考虑如下的两种情况： 一个 cache 服务器 m down 掉了（在实际应用中必须要考虑这种情况），这样所有映射到 cache m 的对象都会失效，怎么办，需要把 cache m 从 cache 中移除，这时候 cache 是 N-1 台，映射公式变成了 hash(object)%(N-1) ； 由于访问加重，需要添加 cache ，这时候 cache 是 N+1 台，映射公式变成了 hash(object)%(N+1) ； 1 和 2 意味着什么？这意味着突然之间几乎所有的 cache 都失效了。对于服务器而言，这是一场灾难，洪水般的访问都会直接冲向后台服务器； 再来考虑第三个问题，由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。有什么方法可以改变这个状况呢，这就是 consistent hashing 一致性 hash 算法… hash算法和单调性Hash 算法的一个衡量指标是单调性（ Monotonicity ），定义如下： 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 容易看到，上面的简单 hash 算法 hash(object)%N 难以满足单调性要求。 consistent hashing 算法的原理consistent hashing 是一种 hash 算法，简单的说，在移除 / 添加一个 cache 时，它能够尽可能小的改变已存在 key 映射关系，尽可能的满足单调性的要求。 下面就来按照 5 个步骤简单讲讲 consistent hashing 算法的基本原理。 第一步环形hash 空间考虑通常的 hash 算法都是将 value 映射到一个 32 为的 key 值，也即是 0~2^32-1 次方的数值空间；我们可以将这个空间想象成一个首（ 0 ）尾（ 2^32-1 ）相接的圆环，如下图所示的那样。 第二步把对象映射到hash 空间接下来考虑 4 个对象 object1~object4 ，通过 hash 函数计算出的 hash 值 key 在环上的分布如下图所示。 123hash(object1) = key1;...hash(object4) = key4; 4 个对象的 key 值分布 第三步把cache 映射到hash 空间 Consistent hashing 的基本思想就是将对象和 cache 都映射到同一个 hash 数值空间中，并且使用相同的 hash 算法。假设当前有 A,B 和 C 共 3 台 cache ，那么其映射结果将如图 3 所示，他们在 hash 空间中，以对应的 hash 值排列。 123hash(cache A) = key A;...hash(cache C) = key C; cache 和对象的 key 值分布说到这里，顺便提一下 cache 的 hash 计算，一般的方法可以使用 cache 机器的 IP 地址或者机器名作为 hash 输入。 第四步把对象映射到cache 现在 cache 和对象都已经通过同一个 hash 算法映射到 hash 数值空间中了，接下来要考虑的就是如何将对象映射到 cache 上面了。 在这个环形空间中，如果沿着顺时针方向从对象的 key 值出发，直到遇见一个 cache ，那么就将该对象存储在这个 cache 上，因为对象和 cache 的 hash 值是固定的，因此这个 cache 必然是唯一和确定的。这样不就找到了对象和 cache 的映射方法了吗？！ 依然继续上面的例子（上图），那么根据上面的方法： 对象 object1 将被存储到 cache A 上； object2和 object3 对应到 cache C ； object4 对应到 cache B。 第五步考察cache 的变动 前面讲过，通过 hash 然后求余的方法带来的最大问题就在于不能满足单调性，当 cache 有所变动时， cache 会失效，进而对后台服务器造成巨大的冲击，现在就来分析分析 consistent hashing 算法。 考虑假设 cache B 挂掉了，根据上面讲到的映射方法，这时受影响的将仅是那些沿 cache B 逆时针遍历直到下一个 cache （ cache C ）之间的对象，也即是本来映射到 cache B 上的那些对象。 因此这里仅需要变动对象 object4 ，将其重新映射到 cache C 上即可： Cache B 被移除后的 cache 映射再考虑添加一台新的 cache D 的情况，假设在这个环形 hash 空间中， cache D 被映射在对象 object2 和 object3 之间。这时受影响的将仅是那些沿 cache D 逆时针遍历直到下一个 cache （ cache B ）之间的对象（它们是也本来映射到 cache C 上对象的一部分），将这些对象重新映射到 cache D 上即可。 因此这里仅需要变动对象 object2 ，将其重新映射到 cache D 上： 虚拟节点考量 Hash 算法的另一个指标是平衡性 (Balance) ，定义如下： 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。 hash 算法并不是保证绝对的平衡，如果 cache 较少的话，对象并不能被均匀的映射到 cache 上，比如在上面的例子中，仅部署 cache A 和 cache C 的情况下，在 4 个对象中， cache A 仅存储了 object1 ，而 cache C 则存储了 object2 、 object3 和 object4 ；分布是很不均衡的。 为了解决这种情况， consistent hashing 引入了“虚拟节点”的概念，它可以如下定义： “虚拟节点”（ virtual node ）是实际节点在 hash 空间的复制品（ replica ），一实际个节点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 值排列。 仍以仅部署 cache A 和 cache C 的情况为例，在前面 中我们已经看到， cache 分布并不均匀。现在我们引入虚拟节点，并设置“复制个数”为 2 ，这就意味着一共会存在 4 个“虚拟节点”， cache A1, cache A2 代表了 cache A ； cache C1, cache C2 代表了 cache C ；假设一种比较理想的情况，参见下图 。 此时，对象到“虚拟节点”的映射关系为： 1234objec1-&gt;cache A2objec2-&gt;cache A1objec3-&gt;cache C1objec4-&gt;cache C2 因此对象 object1 和 object2 都被映射到了 cache A 上，而 object3 和 object4 映射到了 cache C 上；平衡性有了很大提高。引入“虚拟节点”后，映射关系就从 { 对象 -&gt; 节点 } 转换到了 { 对象 -&gt; 虚拟节点 } 。查询物体所在 cache 时的映射关系如图 7 所示。 “虚拟节点”的 hash 计算可以采用对应节点的 IP 地址加数字后缀的方式。例如假设 cache A 的 IP 地址为 202.168.14.241 。 引入“虚拟节点”前，计算 cache A 的 hash 值：Hash(“202.168.14.241”);引入“虚拟节点”后，计算“虚拟节”点 cache A1 和 cache A2 的 hash 值： 12Hash("202.168.14.241#1"); // cache A1Hash("202.168.14.241#2"); // cache A2 小结Consistent hashing 的基本原理就是这些，具体的分布性等理论分析应该是很复杂的，不过一般也用不到。 分布式哈希算法我们从浅入深一步一步介绍什么是分布式哈希表。 哈希函数哈希函数是一种计算方法，它可以把一个值A映射到一个特定的范围[begin, end]之内。对于一个值的集合{k1, k2, … , kN}，哈希函数把他们均匀的映射到某个范围之中。这样，通过这些值就可以很快的找到与之对应的映射地址{index1, index2, … , indexN}。对于同一个值，哈希函数要能保证对这个值的运算结果总是相同的。 哈希函数需要经过精心设计才能够达到比较好的效果，但是总是无法达到理想的效果。多个值也许会映射到同样的地址上。这样就会产生冲突，如图中的红线所示。在设计哈希函数时要尽量减少冲突的产生。 最简单的哈希函数就是一个求余运算： hash(A) = A % N。这样就把A这个值映射到了[0~N-1]这样一个范围之中。 哈希表哈希表的核心就是哈希函数hash()。 哈希表是一中数据结构，它把KEY 和 VALUE用某种方式对应起来。使用hash()函数把一个KEY值映射到一个index上，即hash(KEY) = index。这样就可以把一个KEY值同某个index对应起来。然后把与这个KEY值对应的VALUE存储到index所标记的存储空间中。这样，每次想要查找KEY所对应的VALUE值时，只需要做一次hash()运算就可以找到了。 举个例子：图书馆中的书会被某人借走，这样“书名”和“人名”之间就形成了KEY与VALUE的关系。假设现在有三个记录： 简明现代魔法 小明 最后一天 小红 变形记 小红 这就是“书名”和“人名”的对应关系，它表示某人借了某本书。现在我们把这种对应关系用哈希表存储起来，它们的hash()值分别为： hash(简明现代魔法) = 2 hash(最后一天) = 0 hash(变形记) = 1 然后我们就可以在一个表中存储“人名”了： 0 小明 1 小红 2 小红 这三个人名分别存储在0、1和2号存储空间中。当我们想要查找《简明现代魔法》这本书是被谁借走的时候，只要hash()一下这个书名，就可以找到它所对应的index，为2。然后在这个表中就可以找到对应的人名了。在这里，KEY为“书名”， VALUE为“人名”。 当有大量的KEY VALUE对应关系的数据需要存储时，这种方法就非常有效。 分布式哈希表哈希表把所有的东西都存储在一台机器上，当这台机器坏掉了之后，所存储的东西就全部消失了。分布式哈希表可以把一整张哈希表分成若干个不同的部分，分别存储在不同的机器上，这样就降低了数据全部被损坏的风险。 分布式哈希表通常采用一致性哈希函数来对机器和数据进行统一运算。这里先不用深究一致性哈希究竟是什么，只需要知道它是对机器（通常是其IP地址）和数据（通常是其KEY值）进行统一的运算，把他们全都映射到一个地址空间中。假设有一个一致性哈希函数可以把一个值映射到32bit的地址空间中，从0一直到2^32 – 1。我们用一个圆环来表示这个地址空间。 假设有N台机器，那么hash()就会把这N台机器映射到这个环的N个地方。然后我们把整个地址空间进行一下划分，使每台机器控制一个范围的地址空间。这样，当我们向这个系统中添加数据的时候，首先使用hash()函数计算一下这个数据的index，然后找出它所对应的地址在环中属于哪个地址范围，我们就可以把这个数据放到相应的机器上。这样，就把一个哈希表分布到了不同的机器上。如下图所示： 这里蓝色的圆点表示机器，红色的圆点表示某个数据经过hash()计算后所得出的地址。 在这个图中，按照逆时针方向，每个机器占据的地址范围为从本机器开始一直到下一个机器为止。用顺时针方向来看，每个机器所占据的地址范围为这台机器之前的这一段地址空间。图中的虚线表示数据会存储在哪台机器上。 哈希表的工作原理与常用操作 哈希表（Hash Table）的应用近两年才在NOI中出现，作为一种高效的数据结构，它正在竞赛中发挥着越来越重要的作用。 哈希表最大的优点，就是把数据的存储和查找消耗的时间大大降低，几乎可以看成是常数时间；而代价仅仅是消耗比较多的内存。然而在当前可利用内存越来越多的情况下，用空间换时间的做法是值得的。另外，编码比较容易也是它的特点之一。 哈希表又叫做散列表，分为“开散列” 和“闭散列”。考虑到竞赛时多数人通常避免使用动态存储结构，本文中的“哈希表”仅指“闭散列”，关于其他方面读者可参阅其他书籍。 基础操作我们使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数， 也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标）相对应，于是用这个数组单元来存储这个元素。也可以简单的理解为，按照关键字为每一 个元素“分类”，然后将这个元素存储在相应“类”所对应的地方。 但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了“冲突”，换句话说，就是把不同的元素分在了相同的“类”之中。后面我们将看到一种解决“冲突”的简便做法。 总的来说，“直接定址”与“解决冲突”是哈希表的两大特点。 函数构造：构造函数的常用方法（下面为了叙述简洁，设 h(k) 表示关键字为 k 的元素所对应的函数值）： 除余法： 选择一个适当的正整数 p ，令 h(k ) = k mod p ，这里， p 如果选取的是比较大的素数，效果比较好。而且此法非常容易实现，因此是最常用的方法。 数字选择法： 如果关键字的位数比较多，超过长整型范围而无法直接运算，可以选择其中数字分布比较均匀的若干位，所组成的新的值作为关键字或者直接作为函数值。 冲突处理：线性重新散列技术易于实现且可以较好的达到目的。令数组元素个数为 S ，则当 h(k) 已经存储了元素的时候，依次探查 (h(k)+i) mod S , i=1,2,3…… ，直到找到空的存储单元为止（或者从头到尾扫描一圈仍未发现空单元，这就是哈希表已经满了，发生了错误。当然这是可以通过扩大数组范围避免的）。 支持运算：哈希表支持的运算主要有：初始化(makenull)、哈希函数值的运算(h(x))、插入元素(insert)、查找元素(member)。 设插入的元素的关键字为 x ，A 为存储的数组。 初始化比较容易，例如 ：12345678constempty=maxlongint;// 用非常大的整数代表这个位置没有存储元素p=9997;// 表的大小procedure makenull;var i:integer;beginfori:=0 to p-1doA[i]:=empty;End; 哈希函数值的运算根据函数的不同而变化，例如除余法的一个例子：1234function h(x:longint):Integer;beginh:= x mod p;end; 我们注意到，插入和查找首先都需要对这个元素定位，即如果这个元素若存在，它应该存储在什么位置，因此加入一个定位的函数 locate。1234567891011function locate(x:longint):integer;var orig,i:integer;beginorig:=h(x);i:=0;while(i &lt; S)and(A[(orig+i)mod S]&lt;&gt;x)and(A[(orig+i)mod S]&lt;&gt;empty)doinc(i);//当这个循环停下来时，要么找到一个空的存储单元，要么找到这个元//素存储的单元，要么表已经满了locate:=(orig+i) mod S;end; 插入元素：1234567procedure insert(x:longint);var posi:integer;beginposi:=locate(x);//定位函数的返回值ifA[posi]=empty then A[posi]:=xelseerror;//error 即为发生了错误，当然这是可以避免的end; 查找元素是否已经在表中：1234567procedure member(x:longint):boolean;var posi:integer;beginposi:=locate(x);ifA[posi]=x then member:=trueelsemember:=false;end; 这些就是建立在哈希表上的常用基本运算。 当数据规模接近哈希表上界或者下界的时候，哈希表完全不能够体现高效的特点，甚至还不如一般算法。但是如果规模在中央，它高效的特点可以充分体现。试验表明当元素充满哈希表的 90% 的时候，效率就已经开始明显下降。这就给了我们提示：如果确定使用哈希表，应该尽量使数组开大，但对最太大的数组进行操作也比较费时间，需要找到一个平衡点。通常使它的容量至少是题目最大需求的 120% ，效果比较好（这个仅仅是经验，没有严格证明）。 应用举例什么时候适合应用哈希表呢？如果发现解决这个问题时经常要询问：“某个元素是否在已知集合中？”，也就是需要高效的数据存储和查找，则使用哈希表是最好不过的了！那么，在应用哈希表的过程中，值得注意的是什么呢？ 哈希函数的设计很重要。一个不好的哈希函数，就是指造成很多冲突的情况，从前面的例子已经可以看出来，解决冲突会浪费掉大量时间，因此我们的目标就是尽力避免冲突。前面提到，在使用“除余法”的时候，h(k)=k mod p ，p 最好是一个大素数。这就是为了尽力避免冲突。为什么呢？假设 p=1000 ，则哈希函数分类的标准实际上就变成了按照末三位数分类，这样最多1000类，冲突会很多。一般地说，如果 p 的约数越多，那么冲突的几率就越大。 简单的证明：假设 p 是一个有较多约数的数，同时在数据中存在 q 满足 gcd(p,q)=d &gt;1 ，即有 p=ad , q=bd, 则有 q mod p= q – p [q div p] =q – p[b div a] . ① 其中 [b div a ] 的取值范围是不会超过 [0，b] 的正整数。也就是说， [b div a] 的值只有 b+1 种可能，而 p 是一个预先确定的数。因此 ① 式的值就只有 b+1 种可能了。这样，虽然mod 运算之后的余数仍然在 [0，p-1] 内，但是它的取值仅限于 ① 可能取到的那些值。也就是说余数的分布变得不均匀了。容易看出， p 的约数越多，发生这种余数分布不均匀的情况就越频繁，冲突的几率越高。而素数的约数是最少的，因此我们选用大素数。记住“素数是我们的得力助手”。 另一方面，一味的追求低冲突率也不好。理论上，是可以设计出一个几乎完美，几乎没有冲突的函数的。然而，这样做显然不值得，因为这样的函数设计 很浪费时间而且编码一定很复杂，与其花费这么大的精力去设计函数，还不如用一个虽然冲突多一些但是编码简单的函数。因此，函数还需要易于编码，即易于实现。 综上所述，设计一个好的哈希函数是很关键的。而“好”的标准，就是较低的冲突率和易于实现。 另外，使用哈希表并不是记住了前面的基本操作就能以不变应万变的。有的时候，需要按照题目的要求对哈希表的结构作一些改进。往往一些简单的改进就可以带来巨大的方便。 这些只是一般原则，真正遇到试题的时候实际情况千变万化，需要具体问题具体分析才行。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>一致性哈希</tag>
        <tag>hash</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存备份篇（七）]]></title>
    <url>%2Fredis%2Fredis-cache-backup-20170212.html</url>
    <content type="text"><![CDATA[redis的数据备份比较简单，持久化策略有aof与rdb两种方式，rdb对存储数据有压缩，备份最好选择rdb文件进行备份。数据恢复在从rdb文件中提取。 RDB 将数据库的快照（snapshot）以二进制的方式保存到磁盘中。文件小，便于备份恢复 AOF 以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到AOF文件，以此达到记录数据库状态的目的。文件大用于主备数据同步 定时备份linux系统有cron定时任务，编写简单脚本备份rdb文件即可。 备份脚本12345678910111213141516171819202122232425262728293031323334353637#!/bin/shREDIS_DATA_HOME="/root/redis/data/"REDIS_BACKUP_FILES="appendonly-7000.aof dump-7000.rdb"BACKUP_DAYS=7REDIS_BACKUP_FILES_ARR=($REDIS_BACKUP_FILES)CUR_DAY=$(date +%Y%m%d)EXPIRED_DAY=$(date -d "$BACKUP_DAYS days ago" +%Y%m%d)for f in $&#123;REDIS_BACKUP_FILES_ARR[@]&#125;do tempFPath=$(echo "$REDIS_DATA_HOME$f" | awk 'gsub(/^ *| *$/,"")') if [ ! -f "$tempFPath" ]; then echo "$tempFPath not exist!" exit 1 fi tempBPath="$tempFPath.$CUR_DAY" if [ -f "$tempBPath" ]; then echo "backup-file [$tempBPath] exist already." else echo "start backup $tempFPath to $tempBPath." cp $tempFPath $tempBPath fi #delete backup file by expired tempEPath="$tempFPath.$EXPIRED_DAY" if [ -f "$tempEPath" ]; then echo "delete backup-file [$tempEPath]." rm -f $tempEPath #else #echo "backup-file [$tempEPath] not exist." fi doneecho "ok." 定时任务配置vim /etc/crontab123456789101112131415SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed30 3 * * * root shell.sh 重启生效1systemctl restart crond.service 备份恢复备份恢复期间是不能提供服务了，请慎重恢复。 关闭Slave节点关闭salve节点防止主备切换，使用SHUTDOWN命令关闭节点。 关闭Master节点关闭Master节点防止主备切换，使用SHUTDOWN命令关闭节点。 拷贝rdb文件拷贝rdb文件到Master节点数据目录 Master节点关闭AOF持久化设置节点appendonly配置项为no，关闭AOF持久化，并启动节点。 重写AOF文件客户端连接Master节点，执行bgrewriteaof命令重新aof。 Master节点开启AOF持久化设置节点appendonly配置项为yes，关闭AOF持久化，并重启节点。 开启Slave节点开启Salve节点，同步master的数据。 总结 定时备份数据 备份数据恢复请慎重，期间不会中断服务。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存扩容篇（六）]]></title>
    <url>%2Fredis%2Fredis-cache-expand-20170210.html</url>
    <content type="text"><![CDATA[redis cluster集群提供了扩容功能，以解决在线动态扩容和数据迁移等问题。Redis 集群没有并使用传统的一致性哈希来分配数据，而是采用另外一种叫做哈希槽 (hash slot)的方式来分配的。redis cluster 默认分配了 16384 个slot，当我们set一个key 时，会用CRC16算法来取模得到所属的slot，然后将这个key 分到哈希槽区间的节点上，具体算法就是：CRC16(key) % 16384。 注意的是：必须要3个以后的主节点，否则在创建集群时会失败，我们在后续会实践到。 集群节点请参考基于Redis Cluster的分布式缓存部署篇（三） 主机名 主机地址 端口 备注 nodea-7000 10.128.31.104 7000 主机A7000端口节点 nodea-7001 10.128.31.104 7001 主机A7001端口节点 nodeb-7000 10.128.31.108 7000 主机B7000端口节点 nodeb-7001 10.128.31.108 7001 主机B7001端口节点 nodec-7000 10.128.31.109 7000 主机C7000端口节点 nodec-7001 10.128.31.109 7001 主机C7001端口节点 新增集群节点 主机名 主机地址 端口 备注 noded-7000 10.128.31.103 7000 主机D7000端口节点 noded-7001 10.128.31.105 7001 主机E7001端口节点 扩容准备启动新节点启动命令12redis-server ~/redis/etc/redis-7000.confredis-server ~/redis/etc/redis-7001.conf 重新规划槽位 原集群槽位 节点 槽位号 备注 nodea 0-5460 节点A槽位段 nodeb 5461-10922 节点B槽位段 nodec 10923-16383 节点C槽位段 新集群槽位，新增一组节点，按平均分配策略各组节点槽16384/4=4096 节点 槽位号 备注 nodea 0-4095 节点A槽位段 nodeb 4096-8491 节点B槽位段 nodec 8492-12287 节点C槽位段 nodec 12288-16383 节点D槽位段 上面的调整方案槽位移动多，可以将每组节点的1365槽分配给新增节点 集群扩容添加Master节点添加节点，需要知道节点IP和port，通常情况是同时添加两个节点。使用redis-trib.rb add-node分别将两个新结点添加到集群中，按照顺序，前一个节点作为Master，后一个作为其Slave。执行命令 1redis-trib.rb add-node 10.128.31.103:7000 10.128.31.104:7000 查看Mater节点NodeId1redis-cli -h 10.128.31.104 -p 7000 cluster nodes 添加Slave节点集群节点是一主多备，添加master节点，再给Master添加Salve，不能挂起或关闭原有节点。添加从节点的命令 1redis-trib.rb add-node --slave --master-id 2aeade86ccf4027a56df26d275db91cd3c689fe9 迁移槽数据1redis-trib.rb reshard 10.128.31.104:7000 执行命令询问需要迁移的槽位数量，打印一份期望的执行结果，确认后输入4096 输入接收槽位的集群节点ID，也就是10.128.31.103:7000的节点node id 全部节点的迁移槽位，则输入all，最终每个节点迁移迁移一部分槽 新槽位分布 节点 槽位号 备注 nodea 0-4095 节点A槽位段 nodeb 4096-8491 节点B槽位段 nodec 8492-12287 节点C槽位段 nodec 12288-16383 节点D槽位段 总结 迁移过程中允许slot继续有操作，redis保证了迁移过程中slot的正常访问。 集群扩容前后，slot总数保存不变，每个集群中的节点持有总slot的一部分，有效的保证了数据的完整性。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存性能篇（五）]]></title>
    <url>%2Fredis%2Fredis-cache-performance-20170209.html</url>
    <content type="text"><![CDATA[redis良好的性能是做缓存的最佳选择之一，其自带的benchmarks性能测试工具的数据也说明。但Cluster集群，不同业务场景下的性能有什么不同吗？耳听为虚，眼见为实，让我们来测试下吧。 集群节点请参考基于Redis Cluster的分布式缓存部署篇（三） 主机名 主机地址 端口 备注 nodea-7000 10.128.31.104 7000 主机A7000端口节点 nodea-7001 10.128.31.104 7001 主机A7001端口节点 nodeb-7000 10.128.31.108 7000 主机B7000端口节点 nodeb-7001 10.128.31.108 7001 主机B7001端口节点 nodec-7000 10.128.31.109 7000 主机C7000端口节点 nodec-7001 10.128.31.109 7001 主机C7001端口节点 测试脚本java编写的测试程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159package jcache;import jcache.clients.jcachecluster.base.JCacheClient;import jcache.clients.jcachecluster.common.PropertiesConst;import jcache.clients.jcachecluster.factory.CacheFactorySingle;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.UUID;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.atomic.AtomicLong;public class JcacheClusterTPS &#123; private static String HostAndPort = "10.128.31.104:7000;" + "10.128.31.104:7001;" + "10.128.31.105:7000;" + "10.128.31.105:7001;" + "10.128.31.109:7000;" + "10.128.31.109:7001"; public static void main(String[] args) &#123; final JCacheClient cluster = getCluster(); final int nThreads = args.length &gt;= 1 ? Integer.parseInt(args[0]) : 8; final int sendNumOnceTime = args.length &gt;= 2 ? Integer.parseInt(args[1]) : 100; final int keySize = args.length &gt;= 3 ? Integer.parseInt(args[2]) : 10; final int messageSize = args.length &gt;= 4 ? Integer.parseInt(args[3]) : 10; final int times = args.length &gt;= 5 ? Integer.parseInt(args[4]) : 1; final int setOrGet = args.length &gt;= 6 ? Integer.parseInt(args[5]) : 0; final String keyParam = args.length &gt;= 7 ? args[6] : "0"; final AtomicLong atomicSuccessNums = new AtomicLong(0); final List&lt;Long&gt; tpsList = new ArrayList&lt;Long&gt;(); final String msg = buildMessage(messageSize); final List&lt;String&gt; keys = getKeys(keySize, nThreads * sendNumOnceTime, times); doRun(cluster, tpsList, atomicSuccessNums, nThreads, times, sendNumOnceTime, keys, msg, keySize, messageSize, setOrGet, keyParam); &#125; private static JCacheClient getCluster() &#123; Properties properties = new Properties(); properties.put(PropertiesConst.Keys.HOST_AND_PORT, HostAndPort); properties.put(PropertiesConst.Keys.AUTH_KEY, "13F455A8E9DC2BBEBE1BD906C82B3C0A1"); properties.put(PropertiesConst.Keys.NAMESPACE, "weidian-1"); return CacheFactorySingle.createJCacheClient(properties); &#125; private static void doRun(final JCacheClient cluster, final List&lt;Long&gt; tpsList, final AtomicLong atomicSuccessNums, final int nThreads, final int times, final int sendNumOnceTime, final List&lt;String&gt; keys, final String msg, final int keySize, final int messageSize, final int setOrGet, final String keyParam) &#123; final AtomicLong atomicFailNum = new AtomicLong(0); for (int time = 0; time &lt; times; time++) &#123; final Object object = new Object(); synchronized (object) &#123; final int t = time + 1; final ExecutorService exec = Executors.newCachedThreadPool(); final long startCurrentTimeMillis = System.currentTimeMillis(); final CyclicBarrier barrier = new CyclicBarrier(nThreads, new Runnable() &#123; // 设置几个线程为一组,当这一组的几个线程都执行完成后,然后执行住线程的 public void run() &#123; synchronized (object) &#123; long endCurrentTimeMillis = System.currentTimeMillis(); long sendNums = nThreads * sendNumOnceTime; long escapedTimeMillis = endCurrentTimeMillis - startCurrentTimeMillis; long tps = sendNums * 1000 / escapedTimeMillis; String type = "set"; if (setOrGet != 0) &#123; type = "get"; &#125; tpsList.add(tps); System.out.printf("第 %d 次, 发送完成, 用时 : %d ms, " + "线程大小 : %d , " + "key大小 : %d , " + "msg大小 : %d , " + "发送数量 : %d , " + "成功数量 : %d , " + "失败数量 : %d , " + "统计方式 : %s , " + "TPS : %d !!!", t, escapedTimeMillis, nThreads, keySize, messageSize, sendNums, atomicSuccessNums.intValue(), atomicFailNum.intValue(), type, tps); exec.shutdown(); object.notify(); System.out.println(); &#125; &#125; &#125;); for (int i = 0; i &lt; nThreads; i++) &#123; final String finalI = "i" + i; exec.execute(new Runnable() &#123; public void run() &#123; try &#123; for (int j = 0; j &lt; sendNumOnceTime; j++) &#123; if (setOrGet == 0) &#123; String key = "ke" + keyParam + finalI + "j" + j; try &#123; String resp = cluster.set(key, msg); if (!"OK".equals(resp)) &#123; atomicFailNum.incrementAndGet(); &#125; else &#123; atomicSuccessNums.incrementAndGet(); &#125; &#125; catch (Exception e) &#123; atomicFailNum.incrementAndGet(); &#125; &#125; else &#123; cluster.get(keys.get(atomicSuccessNums.intValue())); &#125; &#125; barrier.await(); &#125; catch (Exception e) &#123; try &#123; barrier.await(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; catch (BrokenBarrierException e1) &#123; e1.printStackTrace(); &#125; e.printStackTrace(); &#125; &#125; &#125;); &#125; try &#123; object.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; long sum = 0; for (Long tps : tpsList) &#123; sum += tps; &#125; System.out.printf("全部发送完成, 平均TPS : %d !!!", sum / tpsList.size()); &#125; private static String buildMessage(final int messageSize) &#123; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; messageSize; i += 8) &#123; sb.append("hello baby"); &#125; return sb.toString(); &#125; private static List&lt;String&gt; getKeys(int keySize, int keys, int times) &#123; System.out.println("&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;正在生成Key&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"); List&lt;String&gt; keysList = new ArrayList&lt;String&gt;();// for (int i = 0; i &lt; keys * times; i++) &#123;// keysList.add(getUId(keySize, i));// &#125; System.out.println("&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;生成成功Key&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"); return keysList; &#125; private static String getUId(int keySize, int i) &#123; return String.format("%0" + keySize + "d", i); &#125; private static String getUUId() &#123; return UUID.randomUUID().toString(); &#125;&#125; 用例执行命令1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterDataCorrect 100 10000 10 10 1 0 用例参数说明 第一个参数是线程数量 第二个参数是每个线程执行操作次数 第三个参数是key的大小（字节） 第四个参数是val的大小（字节） 第五个参数是运行次数 第六个参数是操作类型，0标示set，1标示get 场景测试客户端线程数量分别设置Key和Value大小固定为10个字节，写操作1000000次，线程数逐步增加，统计每次发送数据的TPS，如下: 序号 Key（字节） Value（字节） 执行次数 客户端线程数 集群TPS 1 10 10 1000000 5 9704 2 10 10 1000000 10 19534 3 10 10 1000000 20 36406 4 10 10 1000000 50 42634 5 10 10 1000000 100 43050 6 10 10 1000000 200 44086 7 10 10 1000000 500 42902 8 10 10 1000000 1000 43046 9 10 10 1000000 2000 42542 10 10 10 1000000 5000 42858 缓存Value长度分别设置Key固定10个字节，客户端线程数固定100，进行写操作1000000次，缓存的value长度逐步增加，统计每次发送数据的TPS，如下: 序号 Key（字节） Value（字节） 执行次数 客户端线程数 集群TPS 1 10 10 1000000 100 47777 2 10 20 1000000 100 46213 3 10 50 1000000 100 45209 4 10 100 1000000 100 43539 5 10 200 1000000 100 40692 6 10 500 1000000 100 32023 7 10 1000 1000000 100 30835 8 10 2000 1000000 100 15550 9 10 5000 1000000 100 13956 10 10 10000 1000000 100 9608 读取数据性能客户端读取缓存数据，影响TPS的因素主要就是key的长度。因此只需要保持缓存value固定10个字节，设置客户端线程数为100，在逐步增加缓存Key大小的同时，客户端多线程读取当前测试用例的相同key，如下: 序号 Key（字节） Value（字节） 执行次数 客户端线程数 集群TPS 1 10 10 1000000 100 14468 2 20 10 1000000 100 14753 3 30 10 1000000 100 15874 4 40 10 1000000 100 14588 5 50 10 1000000 100 16880 6 60 10 1000000 100 16283 7 100 10 1000000 100 14543 8 200 10 1000000 100 13992 9 500 10 1000000 100 13283 10 1000 10 1000000 100 12062 Swap性能开启设置并发300线程，每个线程执行20972次，每个key设置10字节，每个value设置512字节，如下： Swap性能关闭设置并发300线程，每个线程执行20972次，每个key设置10字节，每个value设置512字节，如下： 持久化策略性能everysec策略平均的TPS是22855。 always策略平均的TPS是11604 no策略平均的TPS是27383。注意：通常情况都不会配置“no”持久化策略。 总结读写性能 在一定范围内，提高客户端线程数可以提高集群写数据的性能，超过范围再提供线程数则会导致集群写数据性能急速下降。 缓存Value长度对redis写数据性能影响很大,它们呈现相反的变化趋势；增加Value长度则集群写数据的TPS下降，因此redis合适缓存小数据。 Swap性能 关闭swap内存，集群整体性能有所提高；在高并发环境下，关闭swap性能会提高，也是以处理能力下降为代价。 开启swap内存，集群的整体内存使用量会增加，当服务器有足够的内存，建议关闭swap以提高集群的效率。 持久化策略 always策略每次持久化，性能最低。建议数据安全性要求很到的业务场景使用，作为缓存建议不使用。 everysec策略每秒持久化，测试数据看是always性能的两倍以上。建议采用everysec持久化策略，即保证了数据稳定性，又兼顾缓存的效率。 no策略由操作系统置换页时持久化，性能最高，例如linux是30秒一次。no策略数据安全性较差，请在合适的业务场景下选择。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存算法篇（四）]]></title>
    <url>%2Fredis%2Fredis-cache-algorithm-20170207.html</url>
    <content type="text"><![CDATA[说明下这里的算法指的是内存淘汰算法。redis作为应用级缓存使用，在内存超过限制时，按照配置的策略，淘汰掉相应的kv，使得内存可以继续留有足够的空间保存新的数据。 算法介绍redis 提供 6种数据淘汰策略： noeviction : 默认，不淘汰 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 以上6中淘汰策略已经能够满足大多数应用场景，如何选择淘汰策略？根据实际业务情况进行选择。 源码分析其缓存管理功能，由redis.c文件中的freeMemoryIfNeeded函数实现。如果maxmemory被设置，则在每次进行命令执行之前，该函数均被调用，用以判断是否有足够内存可用，释放内存或返回错误。如果没有找到足够多的内存，程序主逻辑将会阻止设置了REDIS_COM_DENYOOM flag的命令执行，对其返回command not allowed when used memory &gt; ‘maxmemory’的错误消息。源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141int freeMemoryIfNeeded(void) &#123; size_t mem_used, mem_tofree, mem_freed; int slaves = listLength(server.slaves); /* Remove the size of slaves output buffers and AOF buffer from the * count of used memory. */ // 计算占用内存大小时，并不计算slave output buffer和aof buffer，因此maxmemory应该比实际内存小，为这两个buffer留足空间。 mem_used = zmalloc_used_memory(); if (slaves) &#123; listIter li; listNode *ln; listRewind(server.slaves,&amp;li); while((ln = listNext(&amp;li))) &#123; redisClient *slave = listNodeValue(ln); unsigned long obuf_bytes = getClientOutputBufferMemoryUsage(slave); if (obuf_bytes &gt; mem_used) mem_used = 0; else mem_used -= obuf_bytes; &#125; &#125; if (server.appendonly) &#123; mem_used -= sdslen(server.aofbuf); mem_used -= sdslen(server.bgrewritebuf); &#125; /* Check if we are over the memory limit. */ if (mem_used &lt;= server.maxmemory) return REDIS_OK; if (server.maxmemory_policy == REDIS_MAXMEMORY_NO_EVICTION) return REDIS_ERR; /* We need to free memory, but policy forbids. */ /* Compute how much memory we need to free. */ mem_tofree = mem_used - server.maxmemory; mem_freed = 0; while (mem_freed &lt; mem_tofree) &#123; int j, k, keys_freed = 0; for (j = 0; j &lt; server.dbnum; j++) &#123; long bestval = 0; /* just to prevent warning */ sds bestkey = NULL; struct dictEntry *de; redisDb *db = server.db+j; dict *dict; if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_RANDOM) &#123; dict = server.db[j].dict; &#125; else &#123; dict = server.db[j].expires; &#125; if (dictSize(dict) == 0) continue; /* volatile-random and allkeys-random policy */ if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_RANDOM || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_RANDOM) &#123; de = dictGetRandomKey(dict); bestkey = dictGetEntryKey(de); &#125;//如果是random delete,则从dict中随机选一个key /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; robj *o; de = dictGetRandomKey(dict); thiskey = dictGetEntryKey(de); /* When policy is volatile-lru we need an additonal lookup * to locate the real key, as dict is set to db-&gt;expires. */ if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) de = dictFind(db-&gt;dict, thiskey); //因为dict-&gt;expires维护的数据结构里并没有记录该key的最后访问时间 o = dictGetEntryVal(de); thisval = estimateObjectIdleTime(o); /* Higher idle time is better candidate for deletion */ if (bestkey == NULL || thisval &gt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125;//为了减少运算量,redis的lru算法和expire淘汰算法一样，都是非最优解，lru算法是在相应的dict中，选择maxmemory_samples(默认设置是3)份key，挑选其中lru的，进行淘汰 &#125; /* volatile-ttl */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_TTL) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; de = dictGetRandomKey(dict); thiskey = dictGetEntryKey(de); thisval = (long) dictGetEntryVal(de); /* Expire sooner (minor expire unix timestamp) is better * candidate for deletion */ if (bestkey == NULL || thisval &lt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125;//注意ttl实现和上边一样，都是挑选出maxmemory_samples份进行挑选 &#125; /* Finally remove the selected key. */ if (bestkey) &#123; long long delta; robj *keyobj = createStringObject(bestkey,sdslen(bestkey)); propagateExpire(db,keyobj); //将del命令扩散给slaves /* We compute the amount of memory freed by dbDelete() alone. * It is possible that actually the memory needed to propagate * the DEL in AOF and replication link is greater than the one * we are freeing removing the key, but we can't account for * that otherwise we would never exit the loop. * * AOF and Output buffer memory will be freed eventually so * we only care about memory used by the key space. */ delta = (long long) zmalloc_used_memory(); dbDelete(db,keyobj); delta -= (long long) zmalloc_used_memory(); mem_freed += delta; server.stat_evictedkeys++; decrRefCount(keyobj); keys_freed++; /* When the memory to free starts to be big enough, we may * start spending so much time here that is impossible to * deliver data to the slaves fast enough, so we force the * transmission here inside the loop. */ if (slaves) flushSlavesOutputBuffers(); &#125; &#125;//在所有的db中遍历一遍，然后判断删除的key释放的空间是否足够 if (!keys_freed) return REDIS_ERR; /* nothing to free... */ &#125; return REDIS_OK;&#125; 算法源码解析淘汰算法redis.h的相关定义 123456789101112131415161718/* The actual Redis Object */ #define REDIS_LRU_BITS 24 #define REDIS_LRU_CLOCK_MAX ((1&lt;&lt;REDIS_LRU_BITS)-1) /* Max value of obj-&gt;lru */ #define REDIS_LRU_CLOCK_RESOLUTION 1000 /* LRU clock resolution in ms */ typedef struct redisObject &#123; unsigned type:4; //存放的对象类型 unsigned encoding:4; //内容编码 //与server.lruclock的时间差值 unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; //引用计数算法使用的引用计数器 void *ptr; //数据指针&#125; robj; /* Macro used to obtain the current LRU clock. * If the current resolution is lower than the frequency we refresh the * LRU clock (as it should be in production servers) we return the * precomputed value, otherwise we need to resort to a function call. */ #define LRU_CLOCK() ((1000/server.hz &lt;= REDIS_LRU_CLOCK_RESOLUTION) ? server.lruclock : getLRUClock()) 12345unsigned int getLRUClock(void) &#123; return (mstime()/REDIS_LRU_CLOCK_RESOLUTION) &amp; REDIS_LRU_CLOCK_MAX; &#125; unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ 以最简单的all-keys-lru淘汰策略为例，该策略随机选出16个，通过过期时间对pool内存数据进行淘汰。1234567891011121314151617181920212223242526272829303132333435/* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) &#123; struct evictionPoolEntry *pool = db-&gt;eviction_pool; while(bestkey == NULL) &#123; evictionPoolPopulate(dict, db-&gt;dict, db-&gt;eviction_pool); /* Go backward from best to worst element to evict. */ for (k = REDIS_EVICTION_POOL_SIZE-1; k &gt;= 0; k--) &#123; if (pool[k].key == NULL) continue; de = dictFind(dict,pool[k].key); /* Remove the entry from the pool. */ sdsfree(pool[k].key); /* Shift all elements on its right to left. */ memmove(pool+k,pool+k+1, sizeof(pool[0])*(REDIS_EVICTION_POOL_SIZE-k-1)); /* Clear the element on the right which is empty * since we shifted one position to the left. */ pool[REDIS_EVICTION_POOL_SIZE-1].key = NULL; pool[REDIS_EVICTION_POOL_SIZE-1].idle = 0; /* If the key exists, is our pick. Otherwise it is * a ghost and we need to try the next element. */ if (de) &#123; bestkey = dictGetKey(de); break; &#125; else &#123; /* Ghost... */ continue; &#125; &#125; &#125; &#125; 通过lru淘汰返回lru时间，最后与当前lru比较1234567891011/* Given an object returns the min number of milliseconds the object was never * requested, using an approximated LRU algorithm. */ unsigned long long estimateObjectIdleTime(robj *o) &#123; unsigned long long lruclock = LRU_CLOCK(); if (lruclock &gt;= o-&gt;lru) &#123; return (lruclock - o-&gt;lru) * REDIS_LRU_CLOCK_RESOLUTION; &#125; else &#123; return (lruclock + (REDIS_LRU_CLOCK_MAX - o-&gt;lru)) * REDIS_LRU_CLOCK_RESOLUTION; &#125; &#125; 测试验证要将redis最终应用到生产环境中，稳定性、可靠性测试更为重要，下面将对淘汰算法相关进行测试。对于性能的测试另有文章，请关注之后的性能篇。 说明在基于Redis Cluster的分布式缓存部署篇（三）中设置maxmemory为100M，保证服务不会在高并发情况下宕机。最后设置maxmemory-policy要测试的淘汰策略。 用例代码java编写的测试程序，没有讲究代码结构，请见谅。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159package jcache;import jcache.clients.jcachecluster.base.JCacheClient;import jcache.clients.jcachecluster.common.PropertiesConst;import jcache.clients.jcachecluster.factory.CacheFactorySingle;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.UUID;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.atomic.AtomicLong;public class JcacheClusterTPS &#123; private static String HostAndPort = "10.128.31.104:7000;" + "10.128.31.104:7001;" + "10.128.31.105:7000;" + "10.128.31.105:7001;" + "10.128.31.109:7000;" + "10.128.31.109:7001"; public static void main(String[] args) &#123; final JCacheClient cluster = getCluster(); final int nThreads = args.length &gt;= 1 ? Integer.parseInt(args[0]) : 8; final int sendNumOnceTime = args.length &gt;= 2 ? Integer.parseInt(args[1]) : 100; final int keySize = args.length &gt;= 3 ? Integer.parseInt(args[2]) : 10; final int messageSize = args.length &gt;= 4 ? Integer.parseInt(args[3]) : 10; final int times = args.length &gt;= 5 ? Integer.parseInt(args[4]) : 1; final int setOrGet = args.length &gt;= 6 ? Integer.parseInt(args[5]) : 0; final String keyParam = args.length &gt;= 7 ? args[6] : "0"; final AtomicLong atomicSuccessNums = new AtomicLong(0); final List&lt;Long&gt; tpsList = new ArrayList&lt;Long&gt;(); final String msg = buildMessage(messageSize); final List&lt;String&gt; keys = getKeys(keySize, nThreads * sendNumOnceTime, times); doRun(cluster, tpsList, atomicSuccessNums, nThreads, times, sendNumOnceTime, keys, msg, keySize, messageSize, setOrGet, keyParam); &#125; private static JCacheClient getCluster() &#123; Properties properties = new Properties(); properties.put(PropertiesConst.Keys.HOST_AND_PORT, HostAndPort); properties.put(PropertiesConst.Keys.AUTH_KEY, "13F455A8E9DC2BBEBE1BD906C82B3C0A1"); properties.put(PropertiesConst.Keys.NAMESPACE, "weidian-1"); return CacheFactorySingle.createJCacheClient(properties); &#125; private static void doRun(final JCacheClient cluster, final List&lt;Long&gt; tpsList, final AtomicLong atomicSuccessNums, final int nThreads, final int times, final int sendNumOnceTime, final List&lt;String&gt; keys, final String msg, final int keySize, final int messageSize, final int setOrGet, final String keyParam) &#123; final AtomicLong atomicFailNum = new AtomicLong(0); for (int time = 0; time &lt; times; time++) &#123; final Object object = new Object(); synchronized (object) &#123; final int t = time + 1; final ExecutorService exec = Executors.newCachedThreadPool(); final long startCurrentTimeMillis = System.currentTimeMillis(); final CyclicBarrier barrier = new CyclicBarrier(nThreads, new Runnable() &#123; // 设置几个线程为一组,当这一组的几个线程都执行完成后,然后执行住线程的 public void run() &#123; synchronized (object) &#123; long endCurrentTimeMillis = System.currentTimeMillis(); long sendNums = nThreads * sendNumOnceTime; long escapedTimeMillis = endCurrentTimeMillis - startCurrentTimeMillis; long tps = sendNums * 1000 / escapedTimeMillis; String type = "set"; if (setOrGet != 0) &#123; type = "get"; &#125; tpsList.add(tps); System.out.printf("第 %d 次, 发送完成, 用时 : %d ms, " + "线程大小 : %d , " + "key大小 : %d , " + "msg大小 : %d , " + "发送数量 : %d , " + "成功数量 : %d , " + "失败数量 : %d , " + "统计方式 : %s , " + "TPS : %d !!!", t, escapedTimeMillis, nThreads, keySize, messageSize, sendNums, atomicSuccessNums.intValue(), atomicFailNum.intValue(), type, tps); exec.shutdown(); object.notify(); System.out.println(); &#125; &#125; &#125;); for (int i = 0; i &lt; nThreads; i++) &#123; final String finalI = "i" + i; exec.execute(new Runnable() &#123; public void run() &#123; try &#123; for (int j = 0; j &lt; sendNumOnceTime; j++) &#123; if (setOrGet == 0) &#123; String key = "ke" + keyParam + finalI + "j" + j; try &#123; String resp = cluster.set(key, msg); if (!"OK".equals(resp)) &#123; atomicFailNum.incrementAndGet(); &#125; else &#123; atomicSuccessNums.incrementAndGet(); &#125; &#125; catch (Exception e) &#123; atomicFailNum.incrementAndGet(); &#125; &#125; else &#123; cluster.get(keys.get(atomicSuccessNums.intValue())); &#125; &#125; barrier.await(); &#125; catch (Exception e) &#123; try &#123; barrier.await(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; catch (BrokenBarrierException e1) &#123; e1.printStackTrace(); &#125; e.printStackTrace(); &#125; &#125; &#125;); &#125; try &#123; object.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; long sum = 0; for (Long tps : tpsList) &#123; sum += tps; &#125; System.out.printf("全部发送完成, 平均TPS : %d !!!", sum / tpsList.size()); &#125; private static String buildMessage(final int messageSize) &#123; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; messageSize; i += 8) &#123; sb.append("hello baby"); &#125; return sb.toString(); &#125; private static List&lt;String&gt; getKeys(int keySize, int keys, int times) &#123; System.out.println("&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;正在生成Key&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"); List&lt;String&gt; keysList = new ArrayList&lt;String&gt;();// for (int i = 0; i &lt; keys * times; i++) &#123;// keysList.add(getUId(keySize, i));// &#125; System.out.println("&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;生成成功Key&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"); return keysList; &#125; private static String getUId(int keySize, int i) &#123; return String.format("%0" + keySize + "d", i); &#125; private static String getUUId() &#123; return UUID.randomUUID().toString(); &#125;&#125; 用例执行命令1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterDataCorrect 100 10000 10 10 1 0 用例参数说明 第一个参数是线程数量 第二个参数是每个线程执行操作次数 第三个参数是key的大小（字节） 第四个参数是val的大小（字节） 第五个参数是运行次数 第六个参数是操作类型，0标示set，1标示get noeviction策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果当集群的运行内存超过最大使用内存后，测试程序抛异常。noeviction策略不淘汰数据。 查看redis的key数量 allkeys-lru策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果内存占用到达最大值后会置换之前的数据，所有测试数据能全部跑完 查看redis的key数量 volatile-lru策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果 内存占用到达最大值后，如果没有设置过期的数据仍在添加,程序会抛出异常。 调整测试用例，前100条不设置过期时间，后面全部设置过期时间，程序能正常跑完。 查看redis的key数量 allkeys-random策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果内存占用到达最大值后会置换之前的数据，所有测试数据能全部跑完 查看redis的key数量 volatile-random策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果 内存占用到达最大值后，如果没有设置过期的数据仍在添加,程序会抛出异常。 调整测试用例，前100条不设置过期时间，后面全部设置过期时间，程序能正常跑完。 查看redis的key数量 volatile-ttl策略测试用例每条数据1K，运行200M的数据，也就是测试204800条数据的读取和写入。1java -cp jedis-2.9.0.jar:commons-pool2-2.3.jar: RedisClusterStrategy 1024 204800 测试结果调整测试用例过期时间设置从3600秒开始逐渐增加。内存占用到达最大值后会先移除最近过期的数据，所以程序能全部跑完。 查看redis的key数量 总结 启用内存策略之前，需要设置maxmeory配置项，即节点的最大内存使用值。 考虑到持久化数据的缓存场景，选择自己的淘汰策略，一般情况建议设置为noeviction策略。 根据使用场景，适当调低rewrite-percentage百分比，降低内存此时的最小内存值，以防止在进行rewrite时，使用大量内存导致进程挂死问题。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存部署篇（三）]]></title>
    <url>%2Fredis%2Fredis-cache-deploy-20170125.html</url>
    <content type="text"><![CDATA[了解Redis的配置后就开始搭建集群。redis集群建议最少6个节点，在此之前先明确几个概念。 节点/集群节点 对应集群的逻辑节点，是一个进程。通常说的节点就是集群节点。 主备节点组/主备集群节点组 一组集群节点的集合，包括一个主节点及至少一个备节点。 主机/主机节点 对应一台物理主机或虚拟主机，主机节点上可以有多个集群节点（不建议）。 环境准备集群架构 硬件参数先准备部署redis的硬件，以下是服务器参数。 参数 值 CPU 8 core Intel(R) Xeon(R) CPU E7-4860 @ 2.27GHz 内存 16G 磁盘 128G OS Centos7 节点规划 主机名 IP地址 端口 备注 ma 10.128.31.104 7000 主节点a mb 10.128.31.105 7000 主节点b mc 10.128.31.106 7000 主节点c sa 10.128.31.107 7001 从节点a sb 10.128.31.108 7001 从节点b sc 10.128.31.109 7001 从节点c Redis版本当前redis稳定版本号3.2.6, 因此以3.2.6版本搭建集群。 安装Redis下载Redis在用户根目录下创建目录并下载redis3.2.6 1234mkdir ~/rediscd ~/rediswget http://download.redis.io/releases/redis-3.2.6.tar.gz 编译Redis1234tar –zxvf redis-3.2.6.tar.gzcd redis-3.2.6make 安装Redis1make install 注：在每台主机都需要安装Redis Server。 安装集群工具安装epel源1yum -y install epel-release 安装ruby环境，部署工具是使用ruby编写的。1yum -y install rubygems-devel 更换ruby国内安装源12gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/gem sources –l 安装ruby redis1gem install redis 拷贝部署脚本1cp redis-3.2.6/src/redis-trib.rb /usr/local/bin/ 注：只需在一台主机上安装部署脚本。 集群配置主节点配置修改redis.conf配置文件，修改如下： 123456789101112port 7000bind 0.0.0.0daemonize yeslogfile "logs/node-7000.log"cluster-enabled yescluster-config-file nodes-7000.confcluster-node-timeout 5000cluster-slave-validity-factor 10appendonly yesdir ./datadbfilename dump-7000.rdbappendfilename "appendonly-7000.aof" 从节点配置修改redis.conf配置文件，修改如下： 123456789101112port 7001bind 0.0.0.0daemonize yeslogfile "logs/node-7001.log"cluster-enabled yescluster-config-file nodes-7001.confcluster-node-timeout 5000cluster-slave-validity-factor 10appendonly yesdir ./datadbfilename dump-7001.rdbappendfilename "appendonly-7001.aof" 启动节点在每台主机上启动节点服务 1redis-server redis.conf 主节点加入集群使用部署脚本将节点加入集群。 1redis-trib.rb create --replicas 0 10.128.31.104:7000 10.128.31.105:7000 10.128.31.106:7000 命令说明：redis-trib.rb create --replicas &lt;int&gt; &lt;ip:port&gt; &lt;ip:port&gt; &lt;ip:port&gt; &lt;ip:port&gt;其中replicas后的数字表示配置slave个数；集群新增的节点按照顺序，前面的为master节点，后面的为slave节点。通常节点个数是偶数，便于对等分master和slave节点。如果replicas参数后是0，表示新增的节点全部是master，没有slave节点。 从节点加入集群1redis-trib.rb add-node –slave --master-id 9294dfb4e38ada705ae14d4b2b7cb6178bead23c 10.128.31.107:7001 10.128.31.104:7000 命令说明：redis-trib.rb add-node --slave --master-id &lt;arg&gt; ip:port注意：(9294dfb4e38ada705ae14d4b2b7cb6178bead23c)是主节点的node id 补充说明使用以下命令一次性添加主备节点，缺点在于不能指定主备关系，用脚本自动匹配。 1redis-trib.rb create --replicas 1 10.128.31.104:7000 10.128.31.105:7000 10.128.31.106:7000 10.128.31.107:7001 10.128.31.108:7001 10.128.31.109:7001 验证集群查看集群节点状态1redis-cli -h 10.128.31.104 -p 7000 cluster nodes 查看集群节点slot分布1redis-cli -h 10.128.31.104 -p 7000 cluster slots]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存配置篇（二）]]></title>
    <url>%2Fredis%2Fredis-cache-config-20170125.html</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器。先了解Redis的配置项，通常保存在redis.conf配置文件，如下对redis-3.2.6的配置文件做详细介绍。 配置项 默认值 描述 bind 127.0.0.1 指定节点绑定的IP地址, 要监听所有请求则设置“0.0.0.0”。 protected-mode yes 是否开启保护模式。 如果bind配置项没有限制IP、也未设置访问密码，则建议开启该参数，表示Redis只会本地进行访问，拒绝外部访问。如果bind配置项限制了IP和requirepass设置了访问密码，则建议关闭此配置项。 port 6379 Redis实例默认端口号，可根据实际场景修改。 tcp-backlog 511 高并发环境中设置高速后台日志，避免慢客户端连接问题。 timeout 15 设置客户端连接时的超时时间，单位秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接。如果设置为0 表示关闭此设置。 tcp-keepalive 300 指定用于发送ACKs的时间,单位秒。设置为0表示禁用长连接, 非0值表示开启长连接。同时client端socket也可以通过配置keepalive选项,开启长连接。 需要注意的是关闭连接需要双倍的时间。 daemonize no 设置为yes表示redis是后台运行的进程。 supervised no 是否需要配置supervise进程管理方式，有四个值 no upstart system auto 可根据实际情况调整。 pidfile /var/run/redis.pid 运行Redis存放pid的文件，默认是/var/run/redis.pid。运行多个redis实例则需要指定不同的pid文件和端口。 loglevel notice 日志级别，总共支持四个级别：debug、verbose、 notice、 warning。生产环境推荐notice级别。 logfile ./redis.log 日志文件名，文件名推荐以port区分，如 ./redis/logs/node-7001.log databases 16 最大db簇的个数 save save 900 1save 300 10save 60 10000 数据持久化的频率：在一定时间内执行一定数量的写操作时自动保存快照，其中第一个参数表示多长时间，第二个参数表示执行多少次写操作。save 900 1 是指900秒内至少有1次写操作save 300 10 是指300秒内至少有10次写操作save 60 10000 指60秒内至少有10000次写操作 stop-writes-on-bgsave-error yes 集群后台存储发生错误，是否终止所有客户端write请求。测试中服务器内存不足、永久缓存value过大造成磁盘空间不足、强制停止集群等均会造成后台存储发生错误而导致集群节点crash的情况。注：此配置通常需要根据操作系统内核参数vm.overcommit_memory的配置来优化。 rdbcompression yes 缓存数据持久化到快照时，是否用LZF压缩数据。如果期望提高部分CPU性能，可设置为no关闭此压缩选项。 rdbchecksum yes 是否对rdb文件使用CRC64校验和，设置yes表示每个rdb文件内容末尾都追加CRC校验和。以便于其他第三方校验工具很方便的检测文件的完整性。 dbfilename dump.rdb 设置dump的文件位置。如：dump-7001.rdb dir ./ redis工作目录，该配置项一定是个目录，而不能是文件名。 slave-serve-stale-data yes 当slave与master失去联系，或者复制正在进行的时候，slave可能会有两种表现：1. 如果为yes，slave仍然会应答客户端请求，但返回的数据可能是过时，或者在第一次同步的时候数据可能是空的。2. 如果为no，在你执行除了info he salveof之外的其他命令时，slave 都将返回一个 “SYNC with master in progress” 的错误 slave-read-only yes Slave是否设置为只读，通常不建议修改此项。 repl-diskless-sync no 通常用来设置无硬盘复制功能。 repl-diskless-sync-delay 10 diskless复制的延迟时间，单位秒。一旦复制开始，节点不会再接收新slave的复制请求，直到下一个rdb传输。最好等待一段时间，等更多的slave连上来。 repl-disable-tcp-nodelay no slave与master的连接,是否禁用TCP-nodelay选项。设置”yes”表示禁用,那么socket通讯中数据将会以packet方式发送(packet大小受到socket buffer限制)。可提高socket通讯的效率,但是小数据将会被buffer,不会被立即发送,对于接受者可能存在延迟。 设置“no”表示开启TCP-nodelay选项,任何数据都会被立即发送,及时性较好,但是效率较低。建议设为no，但是在高并发或者主从有大量操作的情况下，设置为yes slave-priority 100 slave的权重值。当master失效后,Sentinel将会从slave列表中找到权重值最低(&gt;0)的slave,并提升为master。如果权重值为0,表示此slave为”观察者”,不参与master选举。 appendonly yes 是否启用AOF文件持久化 appendfilename appendonly.aof 设置aof的文件位置。如：appendonly-7001.aof appendfsync everysec Redis支持三种同步AOF文件的策略:always 表示每次有写操作都进行同步 效率最低， 数据可靠性最高everysec 每秒同步一次 性能折中 no 不主动调用fsync同步到磁盘，完全由操作系统调用，linux是30秒写一次, 性能最高，数据可靠性最低 no-appendfsync-on-rewrite no 在aof rewrite期间,是否对aof新记录的append暂缓使用文件同步策略,主要考虑磁盘IO开支和请求阻塞时间。默认为no,表示”不暂缓”,新的aof记录仍然会被立即同步。 auto-aof-rewrite-percentage 100 Redis会隐式调用BGREWRITEAOF来重写log文件，以缩减文件大小，此配置判断log文件大小的基准值和百分比。 auto-aof-rewrite-min-size 64mb Redis会记录上次重写时的aof大小。假如Redis自启动至今还没有进行过重写，那么启动时aof文件的大小会被作为基准值。这个基准值会和当前的aof大小进行比较。如果当前aof大小超出所设置的增长比例，则会触发重写。如果设置auto-aof-rewrite-percentage为0，则会关闭此重写功能。 aof-load-truncated yes 指Redis在恢复时，会忽略最后一条可能存在问题的指令。即在aof写入时，可能存在指令写错的问题(突然断电，写了一半)，这种情况下，如果设置yes会记录日志并继续，而设置no会直接恢复失败。 lua-time-limit 5000 lua脚本执行的最大时间，单位毫秒。redis会记个log，然后返回error。当一个脚本超过了最大时限。只有SCRIPT KILL和SHUTDOWN NOSAVE可以用。如果为0或负数表示无限执行时间 slowlog-log-slower-than 10000 慢日志记录，单位微妙。如果操作时间超过此值,将会把command信息”记录”起来(内存,非文件)。 其中”操作时间”不包括网络IO开支,只包括请求达到server后进行”内存实施”的时间。 如果设置为”0”，则表示记录全部操作，不建议设置为0。 slowlog-max-len 128 “慢操作日志”保留的最大条数,”记录”将会被队列化,如果超过了此长度,旧记录将会被移除。此类日志只会维持在内存中而不会写入磁盘。slowlog get 5 返回最近的5条慢查询日志slowlog len 返回当前已有慢查询日志的条数slowlog reset 清空当前所有的慢查询日志 latency-monitor-threshold 0 是否开启redis自身延迟监控配置，默认0是关闭监控。 notify-keyspace-events “” 默认空字符串，表示关闭通知 hash-max-ziplist-entries 512 Hash在条目数量较小的时候会使用一种高效的内存数据结构编码，当超过某个临界点就会采用另一种存储方式，该临界点由下面的hash-max-ziplist-value和hash-max-ziplist-entries两个配置决定。 hash-max-ziplist-value 64 list-max-ziplist-size -2 与Hash类似，较小的List会以一种特殊的编码方式来节省空间，只要List不超过设定的上限。 list-compress-depth 0 set-max-intset-entries 512 限制特殊编码的最大上限，特殊编码指的是缓存数据全是64位无符号整型数字构成的字符串。 zset-max-ziplist-entries 128 有序集合也会采用特殊编码来节省空间，只要不超过上限。 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 RedisHyperLogLog是用来做基数统计的算法，HyperLogLog的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定并且很小的。当HyperLogLog用稀疏式表示法时所用内存超过下面的限制，就会转换成稠密式表示，为了更高的内存利用率，官方建议值为3000，不建议修改此配置项。 activerehashing yes 是否开启顶层数据结构的rehash功能,如果内存允许、实时性要求不高,请设置为true开启它。rehash能够很大程度上提高K-V存取的效率。 client-output-buffer-limit normal 0 0 0 普通client，包括monitor client-output-buffer-limit slave 256mb 64mb 60 slave用来同步master数据的client client-output-buffer-limit pubsub 32mb 8mb 60 Pub/Sub模式中订阅了至少一个channel或者模式的client hz 10 Redis server执行后台任务的频率，此值越大表示redis对”间歇性task”的执行次数越频繁(次数/秒)。”间歇性task”包括”过期集合”检测、关闭”空闲超时”的连接等,此值必须大于0且小于500。此值过小就意味着更多的cpu周期消耗,后台task被轮询的次数更频繁。此值过大意味着”内存敏感”性较差。建议采用默认值。 aof-rewrite-incremental-fsync yes 当一个child在重写AOF文件的时候，如果设置为true，那么这个文件会以每次32M数据的来被同步，在大量新增提交到磁盘的场景下是有用的，并且能避免高峰延迟。 requirepass “” 设置客户端连接后进行任何其他指定前需要使用的密码。 maxclients 10000 最大并发连接数，默认为10000，这个跟系统本身的 open-file-limit 有关。 cluster-enabled no 节点开启集群选项配置 cluster-config-file 节点的配置文件，该文件是由集群节点来创建和维护的，不能人工修改。多个实例的节点需要，建议以端口号标记配置文件。 cluster-node-timeout 节点超时时间(单位毫秒)，超过则认为节点宕机。 cluster-slave-validity-factor 10 如果将该项设置为0(单位秒)，不管slave节点和master节点间失联多久都会一直尝试failover（设为正数，失联大于一定时间（factor节点TimeOut），不再进行FailOver） cluster-require-full-coverage yes 默认情况下，当集群检测到某个哈希槽（hash slot）没有被覆盖（没有任何节点为此服务）会停止接受查询服务，如果集群部分宕机最终会导致整个集群不可用，当哈希槽重新被全覆盖的时候会自动变为可用。如果希望哈希槽未被覆盖的集群节点继续接受服务，需要将cluster-require-full-coverage设置为no。 cluster-migration-barrier 1 考虑一种极端情况，集群有一台主Redis和四台从Redis，从Redis全部挂掉，failover机制有可能造成集群只有主Redis而无从Redis的尴尬境况。为了保证集群的名副其实，可以规定，当从Redis少于某个数量时，拒绝执行failover。 配置参照上表，接下来可以搭建集群了。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis Cluster的分布式缓存介绍篇（一）]]></title>
    <url>%2Fredis%2Fredis-cache-intro-20170125.html</url>
    <content type="text"><![CDATA[最近公司需要基于第三方开源项目做分布式缓存，业界有Redis、Memcached、Ehcache等产品。对产品redis做了一些研究，于是决定整理一下该产品的特性及使用场景拿出来分享。 Redis有sentinel哨兵机制监控管理多节点，也有通过代理管理的codis，3.0之后官方推出Cluster的集群方案。它们各有优缺点，sentinel简洁方便，codis管理方便，cluster性能优越。 功能 少量数据存储，高速读写访问。通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。 海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。 特性网络模型Redis使用单线程的IO复用模型，自己封装了一个简单的AeEvent事件处理框架，主要实现了epoll、kqueue和select，对于单纯只有IO操作来说，单线程可以将速度优势发挥到最大，但是Redis也提供了一些简单的计算功能，比如排序、聚合等，对于这些操作，单线程模型实际会严重影响整体吞吐量，CPU计算过程中，整个IO调度都是被阻塞住的。 内存管理Redis使用现场申请内存的方式来存储数据，并且很少使用free-list等方式来优化内存分配，会在一定程度上存在内存碎片，Redis跟据存储命令参数，会把带过期时间的数据单独存放在一起，并把它们称为临时数据，非临时数据是永远不会被剔除的，即便物理内存不够，导致swap也不会剔除任何非临时数据（但会尝试剔除部分临时数据），这点上Redis更适合作为存储而不是cache。 数据一致性问题在一致性问题上，个人感觉redis没有memcached实现的好，Memcached提供了cas命令，可以保证多个并发访问操作同一份数据的一致性问题。 Redis没有提供cas 命令，并不能保证这点，不过Redis提供了事务的功能，可以保证一串命令的原子性，中间不会被任何操作打断。集群中使用tag实现。 支持的KEY类型Redis除key/value之外，还支持list,set,sorted set,hash等众多数据结构，提供了KEYS进行枚举操作，但不能在线上使用，如果需要枚举线上数据，Redis提供了工具可以直接扫描其dump文件，枚举出所有数据，Redis还同时提供了持久化和复制等功能。 客户端支持redis官方提供了丰富的客户端支持，包括了绝大多数编程语言的客户端，比如我此次测试就选择了官方推荐了Java客户端Jedis.里面提供了丰富的接口、方法使得开发人员无需关系内部的数据分片、读取数据的路由等，只需简单的调用即可，非常方便。 数据复制从2.8开始，Slave会周期性（每秒一次）发起一个Ack确认复制流（replication stream）被处理进度。 读写分离redis支持读写分离，而且使用简单，只需在配置文件中把redis读服务器和写服务器进行配置，多个服务器使用逗号分开。 水平动态扩展历时三年之久，终于等来了期待已由的Redis 3.0。新版本主要是实现了Cluster的功能，增删集群节点后会自动的进行数据迁移。实现 Redis 集群在线重配置的核心就是将槽从一个节点移动到另一个节点的能力。因为一个哈希槽实际上就是一些键的集合， 所以 Redis 集群在重哈希（rehash）时真正要做的，就是将一些键从一个节点移动到另一个节点。 数据淘汰策略redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis 提供 6种数据淘汰策略： noeviction : 默认，不淘汰 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 集群 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。 redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node-&gt;slot-&gt;value 这里对Redis做简单的介绍，接下来会对集群的配置、部署、IO、性能、扩容、备份、迁移进行说明和测试。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>redis</tag>
        <tag>cluster</tag>
        <tag>cache</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让你的Github项目持续集成，基于Travis-CI Coveralls]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fhowto-github-travisci-coveralls-20170120.html</url>
    <content type="text"><![CDATA[Travis-CI是国外新兴的开源持续集成构建项目，支持Github项目，使用了小清新的yml语法，通过yml语法来驱动执行。Github项目的支持非常简单，开通Travis后只需编写.travis.yml就能完成持续集成。 Coveralls是一个自动化测试覆盖率的服务，它能提供代码覆盖率并友好的展现。 让我们现在开始吧！ 持续集成Travis CItravis对Github的支持非常好，而且是免费使用。当然它也提供私有的专业服务，有点小贵。 开通Travis浏览器中输入travis网址https://travis-ci.org/。 使用github账号授权登录。添加项目，这里使用我写的Golang示例项目common。 编写.travis.yml在Github项目根目录下添加.travis.yml 1234567891011121314151617181920212223language: gogo: - 1.7.x - masternotifications: email: recipients: - gunsluo@gmail.com on_success: change on_failure: alwaysinstall:before_install: script: - go buildbefore_deploy: deploy: 说明 language指定开发语言 notifications持续集成后邮件通知设置 install依赖安装 before_install依赖安装前执行 script集成脚本 before_deploy部署脚本前执行 deploy部署脚本 其它配置项 官方参考文档 提交.travis.yml提交.travis.yml到Github，自动触发持续集成，到https://travis-ci.org/gunsluo/common查看结果 到这里就用travis完成了持续集成，简单吧。 测试覆盖率Coveralls首先编写自己的单元测试代码，我提供的实例项目已经实现。如何编写单元测试不是这里的重点，先忽略过吧。 开通Coveralls在coveralls官方网站使用github账号登录授权。 添加项目 加密repo_token查看coveralls的repo_token。 repo_token涉及安全不应该提交到.travis.yml，coveralls提供了非对称加密repo_token的方法。 加密命令travis是用ruby编写的。ruby gem的国外下载源很慢，更改安装源，提高下载速度。12gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/gem sources -l 安装加密命令travis1gem install travis 加密1travis encrypt COVERALLS_TOKEN=your_repo_token 修改.travis.yml12345678910111213141516171819202122232425262728language: gogo: - masternotifications: email: recipients: - gunsluo@gmail.com on_success: change on_failure: alwaysinstall: - go get github.com/mattn/goverallsbefore_install: script: - go test -v -covermode=count -coverprofile=coverage.out - $HOME/gopath/bin/goveralls -coverprofile=coverage.out -service=travis-ci -repotoken $COVERALLS_TOKENbefore_deploy: deploy: env: global: secure: "q1OEufKH4mGRxto7qyrBQKmuEXUZl/JKJWEx2zLMNkSVI9xxEvDSFfpiyNlQevq8ZND34xsOH1+jVQMmAttFc5Ry6cDT/Lgb9F6Cc4WSjPVEpW69dYoyGAPvQNPhWQf8M5yYbUfEQqq088TPrvcIqoheZtdjEg4jRGzrOEu2cOubX5xFTBeRowfilzzOsHCfYIofpWFsEug0ffeo5RrgxYxN7w9utpHmIGU0vDaUZ/Ui0P4zxLQ1gG/18GAykj4+QnYVGDip+kOwCa/EevvmS0OZmwIkujgu1Xul3Dm5M9A1LoyCWaa5NKQh8SBqU4XZErhXlTIl5BKnLI6f7sElcFrH0ShFXosbfbgmmpwDB2et5RHYXoFAOE2qsitV3ZD2WvzGOwNtJNxYB//LCroza1JcGZe/PF+5Vkfzb9JwDcxCi2hP6HcA+oCSp3hvLkmQOgbANPa2ALCIi4r+PaGP5LNAfP4Izn9dEqRB/whGja1zYh8xgT+Yoo7useiMlYl0RcajkZ5EDJadR+UzCqqHxoTHiVw0qqO0EmN+N3wKDHYYfMebn9GSpLcWReZJajZxyzQ+6MHVbFrjJqKRgu4T+OePCv57vLp+gYjxDMqVsyIdlRTWzvaGmf+6IHUJMGztEc7QvOUPabNOawSEtquY2s2Q+IWiBrt/KjtofaJOIBA=" 说明 go get github.com/mattn/goveralls goveralls是coveralls对golang的测试覆盖率支持命令 go test -v -covermode=count -coverprofile=coverage.out golang生成测试覆盖率文件 $HOME/gopath/bin/goveralls -coverprofile=coverage.out -service=travis-ci -repotoken $COVERALLS_TOKEN 将测试覆盖率文件提交给coveralls env环境变量设置，travis提供的repo_token安全方式之一 Github项目添加图标travis页面复制图标标签 coveralls页面复制图标标签 将标签写入README.md1[![Build Status](https://travis-ci.org/gunsluo/common.svg?branch=master)](https://travis-ci.org/gunsluo/common) [![Coverage Status](https://coveralls.io/repos/github/gunsluo/common/badge.svg?branch=master)](https://coveralls.io/github/gunsluo/common?branch=master) 提交后检查下效果吧 ok, 项目可以直观看到编译和覆盖率的情况。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>ci</tag>
        <tag>github</tag>
        <tag>travis</tag>
        <tag>coveralls</tag>
        <tag>持续集成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评论系统更换，多说替代Disqus]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fuse-duoshuo-20170112.html</url>
    <content type="text"><![CDATA[对于评论系统我个人比较偏爱Disqus，喜欢Disqus的简洁风格，博客启用至今一直使用它。多说是国内使用最多最便捷的第三方评论系统。为什么要用多说呢？当然是Disqus别强了，虽说翻墙对于程序员来说像吃饭一样简单，但对新手还是有一定的门槛。考虑再三还是决定换用多说，让评论飞扬起来吧。已有的Disqus的评论再见了，相信Disqus会在回来的。 以此来祭奠逝去的Disqus。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于Gitlab Jenkins Docker集群 微服务搭建自动化部署平台]]></title>
    <url>%2Fdocker%2Fdeploy-auto-platform-20161109.html</url>
    <content type="text"><![CDATA[随着公司应用系统的不断增多，原有手工部署流程越来越不满足上线的需求。为了各个系统能快速迭代与测试，减少上线流程时间和人为出错，迫切需要一套自动化部署系统。 目标 快速迭代，方便的功能更新 代码版本管理，方便的管理、审核 快速打包部署与测试，自动化测试 应用集群管理 快速部署线上环境，快速发布、回滚、重启、停止。 为了达到目标提出下面概念 微服务 - micro service 代码仓库 - gitlab 持续集成部署工具 - jenkins / circleci / ThoughtWorks Go 虚拟化技术 - docker集群 (swarm/kubernetes) 微服务一个简单的应用会随着时间推移逐渐变大。在每次的sprint中，开发团队都会面对新“故事”，然后开发许多新代码。我很确信这个代码正是很多开发者经过多年努力开发出来的一个怪物。单体式应用也会降低开发速度。应用越大，启动时间会越长。单体式应用在不同模块发生资源冲突时，扩展将会非常困难。试想下其中一个功能出错导致服务cash，整个应用都无法使用。micro service 就是解决这样的问题。微服务根据业务适当的拆分。微服务具有更敏捷的迭代，更快速方便的上线。 服务之间的是完全解耦的。 服务通讯是重点，制定统一标准。 分布式事务 版本控制 动态扩容 代理微服务设计模式 异步消息传递微服务设计模式 代码仓库Gitlab是一个用Ruby on Rails开发的开源项目管理程序，Git Flow管理开发流程实现代码提交和审核。流程如下： 持续集成部署工具持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。持续交付（Continuous delivery）指的是，频繁地将软件的新版本，交付给质量团队或者用户，以供评审。持续部署（continuous deployment）是持续交付的下一步，指的是代码通过评审以后，自动部署到生产环境。 持续集成它的好处主要有两个： 快速发现错误。每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 防止分支大幅偏离主干。如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 Jenkins是开源的持续集成工具。自动化的build、打包、构建、测试。可定制的众多插件实现持续部署。 Circleci是一个强大的持续集成与部署服务, 支持多种语言。配置简单，需要付费。 ThoughtWorks GoGO是一款开源的持续集成和发布的系统，旨在使软件开发企业和团队在构建-测试-发布软件产品的流程自动化，并且能持续地发布软件产品。技术支持，需要付费。 虚拟化技术docker 集群Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。 基于docker的集群工具有swarm(官方)和kubernetes(google)。 文件系统隔离：每个进程容器运行在完全独立的根文件系统里。 资源隔离：可以使用cgroup为每个进程容器分配不同的系统资源，例如CPU和内存。 网络隔离：每个进程容器运行在自己的网络命名空间里，拥有自己的虚拟接口和IP地址。 写时复制：采用写时复制方式创建根文件系统，这让部署变得极其快捷，并且节省内存和硬盘空间。 日志记录：Docker将会收集和记录每个进程容器的标准流（stdout/stderr/stdin），用于实时检索或批量检索。 变更管理：容器文件系统的变更可以提交到新的映像中，并可重复使用以创建更多的容器。无需使用模板或手动配置。 交互式Shell Swarm架构 kubernetes架构 自动化部署 - gitlab 开发者fork正式repo代码 开发者克隆代码到本地，git branch特性分支开始开发 开发完成开发者创建pull request 将代码提交正式repo dev分支 合并请求触发gitlab webhook或者 jenkins 定时拉取代码后。jenkins开始 编译、打包、单元测试。jenkins插件完成持续部署到测试环境并进行自动化测试。 测试完成开发者创建pull request 将代码提交正式repo master分支 合并请求触发gitlab webhook或者 jenkins 定时拉取代码后。通过发布系统将应用部署到生产环境 自动化部署 - jenkins 合并请求触发gitlab webhook或者 jenkins 定时拉取代码 jenkins 编译。 jenkins 单元测试。 jenkins 打包、构建 jenkins 插件 docker镜像并上传 jenkins 部署脚本（基于doker swarm/kubernetes） 部署成功，自动化测试或通知测试人员测试 测试通过 jenkins 插件 Publish Over SSH Plugin/SSH Agent Plugin Docker Plugin 自动化部署 - 部署流程jenkins部署应用 合并请求触发gitlab webhook或者 jenkins 定时拉取代码 编译代码二次验证 jenkins 部署脚本 生产环境更新应用 docker pull image docker run 重启应用 存在的问题 不支持热部署 不支持回滚、重启、停止 不支持统一管理 发布系统为了解决发布流程中的问题，需要管理应用的发布系统。发布系统要做到热部署，动态扩容发布，定位应用状态，友好用户界面和接口，统一的配置中心。 正确的发布应用的流程如下： 检查应用 禁止业务流量 停止应用 更新应用 启动应用 开启业务流量 总结自动化部署是从提交代码后，实现代码自动编译、打包、测试到线上部署的整套流程，能解决人工部署带来的无法快速上线、应用版本众多无法管理的问题。自动化部署平台不仅是部署应用，它还涉及到开发流程管理、代码托放管理、持续集成、持续部署、版本管理以及系统架构。自动化部署平台具有快速迭代，快速的代码管理、审核，快速的打包部署和测试，快速的发布、回滚、重启、停止应用的特点，同时还支持自由控制的弹性策略。 自动化部署平台场景 标准的敏捷开发流程 完善的自动化测试 统一安装、配置应用 应用定期检测是否运行 支持自由控制的弹性策略 应用部署的安全管理]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>自动化部署</tag>
        <tag>微服务</tag>
        <tag>集群</tag>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack 安装手册]]></title>
    <url>%2Fopenstack%2Fopenstack-install-20161018.html</url>
    <content type="text"><![CDATA[Openstack作为业界著名的开源ISSA，我在这单机部署所有模块。 安装要求 Centos7 mini vmware workstation 内存 4G+ Centos7安装vmware安装Centos7 mini版本，安装步骤就不在赘述了。下载Centos7 mini Centos7环境准备 更新Yum下载源 123mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum makecache 使用国内Yum源加快下载速度 更新Centos 1yum update 使用代理 编辑vi ~/.bashrc 1234567function setproxy() &#123; export &#123;http,https,ftp&#125;_proxy="http://192.168.29.1:16808"&#125;function unsetproxy() &#123; unset &#123;http,https,ftp&#125;_proxy&#125; 为了加快下载国外源使用VPN（没有vpn可以不配置，192.168.206.1:16808是我的代理程序与端口）。在终端中输入setproxy使用代理 安装网络工具 命令 yum install net-tools 关闭防火墙 命令 systemctl disable firewalld.service 关闭Selinux 修改/etc/selinux/config文件中设置SELINUX=disabled，然后重启服务器。 重启系统 命令 reboot 拍摄快照 给安装好的Centos7系统拍摄快照，准备OpenStack的安装。拍摄快照方便后续安装出错可回退。选择虚拟机 &gt; 快照 &gt; 拍摄快照 如图： OpenStack环境Centos7环境准备完毕，接下来准备安装OpenStack环境。 网络环境配置安装openstack需要两张网卡 管理内部网络(management interface) 提供internet网络(provider interface) 如图需要vmware添加网卡。 vmware配置添加网卡，选择虚拟机 &gt; 设置 &gt; 添加 如图： 查看网络 123456789101112131415161718192021222324# ifconifgeno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.206.131 netmask 255.255.255.0 broadcast 192.168.206.255 inet6 fe80::20c:29ff:fe1f:3222 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:1f:32:22 txqueuelen 1000 (Ethernet) RX packets 566 bytes 54421 (53.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 372 bytes 42227 (41.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno33554984: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::20c:29ff:fe1f:322c prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:1f:32:2c txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 10 bytes 1308 (1.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 0 (Local Loopback) RX packets 4 bytes 340 (340.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 4 bytes 340 (340.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eno33554984是新添加网络设备。其中eno16777736是外网网卡，eno33554984是内网管理网卡。 网卡配置eno33554984 12345# cp /etc/sysconfig/network-scripts/ifcfg-eno16777736 /etc/sysconfig/network-scripts/ifcfg-eno33554984# vi /etc/sysconfig/network-scripts/ifcfg-eno33554984...NAME=eno33554984DEVICE=eno33554984 配置内部网络 123456# vi /etc/sysconfig/network-scripts/ifcfg-eno33554984...IPADDR=192.168.70.2NETMASK=255.255.255.0NM_CONTROLLED=noONBOOT=yes 注:192.168.70.2在vmware only-host网段相同，确保与主主机在192.168.70.0/24网段连通性。 配置外部网络 编辑/etc/sysconfig/network-scripts/ifcfg-eno16777736, HWADDR UUID值不修改 1234DEVICE=eno16777736TYPE=EthernetONBOOT="yes"BOOTPROTO="dhcp" 配置主机名 编辑/etc/hostname修改主机名为controller, 编辑/etc/hosts(单机部署) 如下: 12345678910# controller192.168.70.2 controller# compute1192.168.70.2 compute1# block1192.168.70.2 block1# object1192.168.70.2 object1# object2192.168.70.2 object2 注意: 某些发行版在/etc/hosts文件中添加了一个无关的条目，将实际的主机名解析为另一个环回IP地址（例如127.0.1.1）。 您必须注释掉或删除此条目以防止名称解析问题。 不要删除127.0.0.1条目 设置DNS服务器 vi /etc/resolv.conf 添加 nameserver 114.114.114.114 1234# Generated by NetworkManagersearch localdomainnameserver 192.168.29.2nameserver 114.114.114.114 重启激活配置 reboot 验证网络连通性 123456789# ping -c 4 www.baidu.com PING www.a.shifen.com (180.97.33.107) 56(84) bytes of data.64 bytes from 180.97.33.107: icmp_seq=1 ttl=128 time=39.5 ms64 bytes from 180.97.33.107: icmp_seq=2 ttl=128 time=37.8 ms64 bytes from 180.97.33.107: icmp_seq=3 ttl=128 time=39.5 ms64 bytes from 180.97.33.107: icmp_seq=4 ttl=128 time=39.6 ms--- www.a.shifen.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3006msrtt min/avg/max/mdev = 37.885/39.154/39.666/0.734 ms 123456789# ping -c 4 controllerPING controller (192.168.70.2) 56(84) bytes of data64 bytes from controller (192.168.70.2): icmp_seq=1 ttl=64 time=0.037 ms64 bytes from controller (192.168.70.2): icmp_seq=2 ttl=64 time=0.045 ms64 bytes from controller (192.168.70.2): icmp_seq=3 ttl=64 time=0.026 ms64 bytes from controller (192.168.70.2): icmp_seq=4 ttl=64 time=0.037 ms--- controller ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 2999msrtt min/avg/max/mdev = 0.026/0.036/0.045/0.008 ms 安装Network Time Protocol (NTP) 安装包 1# yum install chrony 编辑 /etc/chrony.conf 添加修改删除ntp服务器: 1server NTP_SERVER iburst NTP_SERVER替换为ntp服务器域名或ip，如果没有自己的NTP服务器使用默认。这里vmware安装不用配置ntp。如果添加其他节点则需要 编辑 /etc/chrony.conf 添加ntp客户端网段（单一节点可以不使用）: 1allow 192.168.70/24 开机启动与启动ntp 12# systemctl enable chronyd.service# systemctl start chronyd.service 验证安装的ntp 1234567# chronyc sources210 Number of sources = 3MS Name/IP address Stratum Poll Reach LastRx Last sample===============================================================================^* news.neu.edu.cn 2 6 377 49 +804us[ +545us] +/- 38ms^+ time6.aliyun.com 2 6 337 49 -676us[ -676us] +/- 38ms^+ 202.118.1.130 2 6 377 50 -5469us[-5727us] +/- 44ms OpenStack安装源 CentOS系统中需要额外提供rpm的openstack安装源 1# yum install centos-release-openstack-newton CentOS系统中需要额外提供rdo的openstack安装源 1# yum install https://rdoproject.org/repos/rdo-release.rpm 如果yum下载不了，请手动下载后放置 /etc/yum.repos.d/ 更新系统 1# yum upgrade 安装OpenStack Client 1# yum install python-openstackclient 安装OpenStack Selinux 1# yum install openstack-selinux 安装Mysql 安装mysql 1# yum install mariadb mariadb-server python2-PyMySQL 创建和编辑 /etc/my.cnf.d/openstack.cnf 创建 [mysqld] 设置 bind-address 为 控制节点(controller node)management IP地址 1234567[mysqld]bind-address = 192.168.70.2default-storage-engine = innodbinnodb_file_per_tablemax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 开机启动和启动mysql 12# systemctl enable mariadb.service# systemctl start mariadb.service 设置mysql安全脚本 1# mysql_secure_installation 设置root密码为123456，为了方便后续所有密度都设置为123456。 安装消息队列 安装rabbitmq 1# yum install rabbitmq-server 开机启动和启动rabbitmq 12# systemctl enable rabbitmq-server.service# systemctl start rabbitmq-server.service 添加 openstack 用户 12# rabbitmqctl add_user openstack 123456Creating user "openstack" ... 123456是rabbitmq密码(RABBIT_PASS)。 配置rabbitmq权限 12# rabbitmqctl set_permissions openstack ".*" ".*" ".*"Setting permissions for user "openstack" in vhost "/" ... 安装memcache 安装memcache包 1# yum install memcached python-memcached 开机启动和启动memcache 12# systemctl enable memcached.service# systemctl start memcached.service 到此openstack安装环境准备就绪，接下来就是安装OpenStack组件了。 身份认证组件 - keystoneKeystone套件作為OpenStack中的身份驗證服務(Identity Service)，Keystone執行了以下兩個功能： 認證與授權。 提供可用服務的 API 服務端點目錄資訊。當安裝OpenStack Identity Service套件後，必須將OpenStack的每個服務註冊到Keystone。這樣身份驗證服務才可以認證已經安裝的OpenStack服務套件，並且得知服務在網絡上的位置。Identity Service 提供了 Role-based 的管理概念，並提供傳統的 UserName/Password 及 Token 的認證方式 想要了解OpenStack的身份驗證套件以前，須先理解以下概念： User 使用OpenStack雲端服務的人、系統、服務，在Keystone會以一個數字表示。身份驗證服務會驗證那些產生呼叫的使用者傳來的請求，使用者登入後會被賦予token來存取資源，使用者可以直接被分配到特定的 Tenant 與Behave，如果這些是被包含在Tenant中的。Credentials 使用者身份的確認資料，諸如：使用者名稱、密碼、API金鑰，或者是一個有身份的服務提供授權token。 Authentication 確認使用者身份的流程，OpenStack身份驗證服務會確認傳送過來的請求，即驗證由使用者提供的憑證。這些憑證通常是使用者名稱、密碼、API金鑰等。當使用者憑證被驗證過後，OpenStack身份驗證服務會給該使用者一個token，透過該token即可請求OpenStack其他服務。 Token 一個以字母與數字混合的字串，用來讓使用者存取OpenStack的API與資源，token可以隨時清除，且本身就有一定時間限制。在近幾版本中，OpenStack身份驗證服務支援了基於token的驗證，這也表示未來會支持更多協定，主要目的是集成服務，且不希望成為一個完整的身份驗證儲存與管理解決方案。 Tenant 用來分組或隔離資訊的容器，tenant會分組或者隔離身份對象。根據不同的服務操作者，tenant可以映射到一個客戶(customer)、帳號(account)、組織(organization)或者專案(Project)。 Service 一個OpenStack的服務，如運算（nova），物件儲存（swift），或映像檔服務（glance）。它提供了一個或多個Endpoint，使用者可以訪問的資源和執行操作。 Endpoint 當一個使用者存取服務時，所有可存取的網路網址，通常是一個URL網址。如果使用者是為板模的擴展而使用，一個Endpoint是可以被建立的，用來表示板模是所有可用的跨Region的可消費服務。 Role 一個定義使用者權限和特權，可賦予其執行某些特定的操作。管理者可以根據不同的 role 給定不同的權限，再將 role 指定給 user，每個 user 可以同時被指定為多個 role 藉以授予系統存取權限在身份驗證服務中，一個token會帶有使用者訊息，其包含了角色列表。服務在被呼叫時，會看使用者是什麼角色，而這個角色賦予的權限能夠操作哪些資源。 Keystone Client OpenStack 身份驗證服務API提供了一套指令介面。例如，使用者可以執行keystone service-create與keystone endpoint-create指令，在OpenStack中註冊服務。下面示意圖展示了OpenStack身份驗證的流程： 了解概念后，我们开始安装keystone。 创建数据库及数据结构 登录数据库 1# mysql -u root -p123456 创建keystone数据库 1mysql&gt; CREATE DATABASE keystone; 数据库权限 1234mysql&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \ IDENTIFIED BY '123456';mysql&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \ IDENTIFIED BY '123456'; 123456是密码(KEYSTONE_DBPASS)。 安装配置keystone 安装包 1# yum install openstack-keystone httpd mod_wsgi 编辑 /etc/keystone/keystone.conf 如下: 1234567[database]...connection = mysql+pymysql://keystone:123456@controller/keystone[token]...provider = fernet 123456是密码(KEYSTONE_DBPASS)。 填充数据 1su -s /bin/sh -c "keystone-manage db_sync" keystone 初始化Fernet 12# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 启动身份服务 12345keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://controller:35357/v3/ \ --bootstrap-internal-url http://controller:35357/v3/ \ --bootstrap-public-url http://controller:5000/v3/ \ --bootstrap-region-id RegionOne 123456是密码(ADMIN_PASS)。 配置Apache HTTP 编辑 /etc/httpd/conf/httpd.conf 配置ServerName项: 1ServerName controller 创建链接文件 /usr/share/keystone/wsgi-keystone.conf: 1ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ keystone命令 开机启动及启动Apache HTTP服务 12# systemctl enable httpd.service# systemctl start httpd.service 查看占用端口 12345678910111213141516# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.70.2:3306 0.0.0.0:* LISTEN 2700/mysqld tcp 0 0 0.0.0.0:11211 0.0.0.0:* LISTEN 3957/memcached tcp 0 0 0.0.0.0:4369 0.0.0.0:* LISTEN 1/systemd tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1097/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1423/master tcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 2917/beam tcp6 0 0 :::11211 :::* LISTEN 3957/memcached tcp6 0 0 :::80 :::* LISTEN 26461/httpd tcp6 0 0 :::22 :::* LISTEN 1097/sshd tcp6 0 0 ::1:25 :::* LISTEN 1423/master tcp6 0 0 :::35357 :::* LISTEN 26461/httpd tcp6 0 0 :::5000 :::* LISTEN 26461/httpd tcp6 0 0 :::5672 :::* LISTEN 2917/beam 35357 5000 端口 配置administrative账号环境变量 1234567# export OS_USERNAME=admin# export OS_PASSWORD=123456# export OS_PROJECT_NAME=admin# export OS_USER_DOMAIN_NAME=default# export OS_PROJECT_DOMAIN_NAME=default# export OS_AUTH_URL=http://controller:35357/v3# export OS_IDENTITY_API_VERSION=3 123456是密码(ADMIN_PASS)。 keystone用户 创建service项目: 12345678910111213# openstack project create --domain default \ --description "Service Project" service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Service Project || domain_id | default || enabled | True || id | b6e39648297f496d8bd351f91c5dbc01 || is_domain | False || name | service || parent_id | default |+-------------+----------------------------------+ 创建demo项目: 12345678910111213 openstack project create --domain default \ --description "Demo Project" demo+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Demo Project || domain_id | default || enabled | True || id | acada8b2e88943aaa094dc0617ffd266 || is_domain | False || name | demo || parent_id | default |+-------------+----------------------------------+ 创建demo用户: 12345678910111213# openstack user create --domain default \ --password-prompt demoUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | 53c9811a2a7940a1a2d89d641c1f7b2a || name | demo || password_expires_at | None |+---------------------+----------------------------------+ 创建用户角色: 12345678# openstack role create user+-----------+----------------------------------+| Field | Value |+-----------+----------------------------------+| domain_id | None || id | 5e2f64124ff54868b63fdf042671dbfa || name | user |+-----------+----------------------------------+ 添加用户角色到demo项目和demo用户: 1# openstack role add --project demo --user demo user 验证keystone 安全性考虑，禁止临时认证token: 编辑 /etc/keystone/keystone-paste.ini 文件从 [pipeline:public_api] [pipeline:admin_api] [pipeline:api_v3] 中删除 admin_token_auth 选项 Unset OS_URL 环境变量: 1# unset OS_URL 请求一个admin用户认证token: 1234567891011121314# openstack --os-auth-url http://controller:35357/v3 \ --os-project-domain-name default --os-user-domain-name default \ --os-project-name admin --os-username admin token issuePassword:+------------+-----------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------+| expires | 2016-02-12T20:14:07.056119Z || id | gAAAAABWvi7_B8kKQD9wdXac8MoZiQldmjEO643d-e_j-XXq9AmIegIbA7UHGPv || | atnN21qtOMjCFWX7BReJEQnVOAj3nclRQgAYRsfSU_MrsuWb4EDtnjU7HEpoBb4 || | o6ozsA_NmFWEpLeKy0uNn_WeKbAhYygrsmQGA49dclHVnz-OMVLiyM9ws || project_id | 343d245e850143a096806dfaefa9afdc || user_id | ac3377633149401296f6c0d92d79dc16 |+------------+-----------------------------------------------------------------+ 请求一个demo用户认证token: 1234567891011121314# openstack --os-auth-url http://controller:5000/v3 \ --os-project-domain-name default --os-user-domain-name default \ --os-project-name demo --os-username demo token issuePassword:+------------+-----------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------+| expires | 2016-02-12T20:15:39.014479Z || id | gAAAAABWvi9bsh7vkiby5BpCCnc-JkbGhm9wH3fabS_cY7uabOubesi-Me6IGWW || | yQqNegDDZ5jw7grI26vvgy1J5nCVwZ_zFRqPiz_qhbq29mgbQLglbkq6FQvzBRQ || | JcOzq3uwhzNxszJWmzGC7rJE_H0A_a3UFhqv8M4zMRYSbS2YF0MyFmp_U || project_id | ed0b60bf607743088218b0a533d5943f || user_id | 58126687cbcc4888bfa9ab73a2256f27 |+------------+-----------------------------------------------------------------+ 创建OpenStack client环境脚本 编辑 admin-openrc 文件内容如下: 12345678export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 123456是密码(ADMIN_PASS)。 编辑 demo-openrc 文件内容如下: 12345678export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 123456是密码(DEMO_PASS)。 脚本使用，使用admin用户环境: 1# . admin-openrc 请求认证token: 1234567891011# openstack token issue+------------+-----------------------------------------------------------------+| Field | Value |+------------+-----------------------------------------------------------------+| expires | 2016-02-12T20:44:35.659723Z || id | gAAAAABWvjYj-Zjfg8WXFaQnUd1DMYTBVrKw4h3fIagi5NoEmh21U72SrRv2trl || | JWFYhLi2_uPR31Igf6A8mH2Rw9kv_bxNo1jbLNPLGzW_u5FC7InFqx0yYtTwa1e || | eq2b0f6-18KZyQhs7F3teAta143kJEWuNEYET-y7u29y0be1_64KYkM7E || project_id | 343d245e850143a096806dfaefa9afdc || user_id | ac3377633149401296f6c0d92d79dc16 |+------------+-----------------------------------------------------------------+ Glance 镜像组件Glance作為OpenStack的Image service，提供使用者可以去尋找、註冊、取得虛擬機的Image。並提供了一個REST API，使你能夠查詢虛擬機Image的metadata與取得實際的Image。你可以透過Image service儲存在不同的地點所提供虛擬機Image，從簡單的檔案系統到像是物件儲存系統的OpenStack Object Storage（Swift）。除了可以讓使用者新增 image 之外，也可以從正在運作的 server 上取得 snapshop 來作為 image 的備份或者是其他虛擬磁碟的 image。OpenStack的映像檔服務(Image service)包含了以下幾個元件： glance-api：接受來至其他服務的API呼叫，諸如Image尋找、取得、儲存。 glance-registry：儲存、處理以及取得Image的metadata，metadata（包含諸如檔案大小、類型等資訊）。 Database：存放Images的metadata資訊，使用者可以根據個人喜好選擇資料庫，大多數選擇MySQL或SQLite。 Image的Storage Repository：支援多種類型的Repository，可以從一般檔案系統、Object Storage（Swift）、RADOS Block device、HTTP、Amazon S3等。但要注意，其中一些Repository只支援讀取。 從Openstack架構圖，可以看到Glance的定位： 可以將 image 存於 Swift 中。 提供 image 給 Nova 作為執行 VM 之用。 使用者可以透過 Horizon 呼叫 Glance API 來管理 image。 在使用 Glance API 之前，都需要通過 Keystone 的認證。 创建数据库及数据结构 登录数据库 1# mysql -u root -p123456 创建keystone数据库 1mysql&gt; CREATE DATABASE glance; 数据库权限 1234mysql&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \ IDENTIFIED BY '123456';mysql&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \ IDENTIFIED BY '123456'; 123456是密码(GLANCE_DBPASS)。 获取 admin 命令行证书权限 1# . admin-openrc 创建glance用户 123456789101112# openstack user create --domain default --password-prompt glanceUser Password:Repeat User Password:+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | bf72dd3d5806479ab175be3b630bda0d || name | glance || password_expires_at | None |+---------------------+----------------------------------+ 用户glance添加角色 1# openstack role add --project service --user glance admin 添加glance服务 1234567891011# openstack service create --name glance \ --description "OpenStack Image" image+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | OpenStack Image || enabled | True || id | 385dcc7f756a4dd8801063dcc4f75a35 || name | glance || type | image |+-------------+----------------------------------+ 创建镜像服务API点: 123456789101112131415# openstack endpoint create --region RegionOne \ image public http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | bd89aaca7ab34c2fa5e3ab96b42b69af || interface | public || region | RegionOne || region_id | RegionOne || service_id | 385dcc7f756a4dd8801063dcc4f75a35 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 123456789101112131415# openstack endpoint create --region RegionOne \ image internal http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | 65af6f8c900242f9bb4a9c8dec9b4ba0 || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 385dcc7f756a4dd8801063dcc4f75a35 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 123456789101112131415# openstack endpoint create --region RegionOne \ image admin http://controller:9292+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | bde313f29d4745cd87da35e63608fce7 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 385dcc7f756a4dd8801063dcc4f75a35 || service_name | glance || service_type | image || url | http://controller:9292 |+--------------+----------------------------------+ 安装配置Glance 安装包: 1# yum install openstack-glance 编辑 /etc/glance/glance-api.conf 文件内容: 12345678910111213141516171819202122232425[database]...connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken]...auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]...flavor = keystone[glance_store]...stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ 123456是密码(GLANCE_DBPASS)。 编辑 /etc/glance/glance-registry.conf 文件内容: 12345678910111213141516171819[database]...connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken]...auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]...flavor = keystone 123456是密码(GLANCE_DBPASS)。 填充数据 1# su -s /bin/sh -c "glance-manage db_sync" glance 终端打印过时信息 开机启动及启动glance 1234# systemctl enable openstack-glance-api.service \ openstack-glance-registry.service# systemctl start openstack-glance-api.service \ openstack-glance-registry.service 校验glance 获取 admin 命令行证书权限 1# . admin-openrc 下载cirros镜像: 12# yum install wget# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img 上传QCOW2格式镜像文件: 1234567891011121314151617181920212223242526# openstack image create "cirros" \ --file cirros-0.3.4-x86_64-disk.img \ --disk-format qcow2 --container-format bare \ --public+------------------+------------------------------------------------------+| Field | Value |+------------------+------------------------------------------------------+| checksum | 133eae9fb1c98f45894a4e60d8736619 || container_format | bare || created_at | 2015-03-26T16:52:10Z || disk_format | qcow2 || file | /v2/images/cc5c6982-4910-471e-b864-1098015901b5/file || id | cc5c6982-4910-471e-b864-1098015901b5 || min_disk | 0 || min_ram | 0 || name | cirros || owner | ae7a98326b9c455588edd2656d723b9d || protected | False || schema | /v2/schemas/image || size | 13200896 || status | active || tags | || updated_at | 2015-03-26T16:52:10Z || virtual_size | None || visibility | public |+------------------+------------------------------------------------------+ 查看镜像: 123456# openstack image list+--------------------------------------+--------+--------+| ID | Name | Status |+--------------------------------------+--------+--------+| 38047887-61a7-41ea-9b49-27987d5e8bb9 | cirros | active |+--------------------------------------+--------+--------+]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>虚拟化</tag>
        <tag>openstack</tag>
        <tag>newton</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack 模块介紹（入门篇）]]></title>
    <url>%2Fopenstack%2Fopenstack-simple-intro-20161013.html</url>
    <content type="text"><![CDATA[OpenStack是美國國家航空暨太空總署和Rackspace共同打造的雲端開源軟體，以Apache許可證授權，並且是一個自由軟體和開放原始碼項目，來打造基礎設施即服務(Infrastructure as a Service)。OpenStack擁有三大模組運算模組、網通模組和儲存模組，加上一套集中式管理的儀表板模組，來組合成一套OpenStack共享服務，並且以提供虛擬機方式，對外帶來運算資源，以便利彈性擴充或調度。 從2010年10月到現今已歷經12個版本，來到了Liberty與下一個版本Mitaka，專案數也從A版的2個發展到現今超過10個以上專案，許多大廠也紛紛加入該行列，打造一套自己的雲端平台。值得一提的是 OpenStack 已在 Liberty 加入了 Big Tent 模型。讓管理人員只需要更新核心的專案，其餘專案可以隨自己需求選擇是否要更新。也將在 2016年第二季推出第一個OpenStack認證管理員（COA）認證。 套件介紹Keystone 身分識別套件 (Identity service)Keystone套件作為OpenStack的身份驗證服務，具有中央目錄能查看哪位使用者可存取哪些服務，並且提供了多種驗證方式，包括使用者帳號密碼、Token以及類似AWS的登入機制。另外，Keystone可以整合現有的中央控管系統，像是LDAP（輕型目錄訪問協議）。 類似 Amazon AWS 的 IAM。 Nova 運算套件 (Compute)Nova 主要擔任著部署與管理虛擬機角色。Nova提供了一套API來開發額外的應用程式，IT人員可以透過網頁介面來查看與管理資源狀態，且可以控制啟動、停止、調整虛擬機。IT人員可將Nova套件部署在多家廠商的虛擬化平臺上，目前來說以KVM和Xen虛擬化平臺最為穩定。除了支援不同的虛擬化平臺之外，在硬體架構的部份，OpenStack支援x86架構、ARM架構等。另外Nova套件還支援Linux羽量級的虛擬化技術LXC，能夠在切割虛擬機時，分出更多的虛擬化執行環境。此外，Nova套件還具有管理LAN網路的功能，可程式化的分配IP位址與VLAN，快速部署網路與資安功能。Nova套件還可將某幾臺虛擬機器設為群組，和不同群組作隔離，並有基於角色的訪問控制（RBAC）功能，可根據使用者的角色確保可存取的資源為何。+ 類似 Amazon AWS 的 EC2。 Glance 映象檔管理套件 (Image Service)Glance套件提供了硬碟或伺服器的Image尋找、註冊以及服務交付等功能。儲存的Image可作為新伺服器部署所需的範本，加快服務上線速度。若是有多臺伺服器需要配置新服務，就不需要額外花費時間單獨設定，也可做為備份時所用。 類似 Amazon AWS 的 VM Import／Export。 Horizon 儀表板套件 (Dashboard)Horizon套件提供IT人員一套圖形化的網頁介面，讓IT人員可以綜觀雲端服務目前的規模與狀態，並且能夠統一存取、部署與管理所有雲端服務所使用到的資源。Horizon套件是個可擴展的網頁式Application。所以Horizon套件可以整合第三方的服務或是產品，像是計費、監控或是額外的管理工具。 類似 Amazon AWS 的 Console。 Neutron 網通套件 (Networking)Neutron套件為其它OpenStack服務提供網路連接即服務（Network-Connectivity-as-a-Service）功能。比如OpenStack運算，為租戶提供API定義網路和使用。基於插件式的架構，使其支援眾多的網路供應商和技術，，IT人員可分配IP位址、靜態IP或是動態IP。且IT人員也可以使用SDN技術，像是OpenFlow協定來打造更大規模或是多租戶的網路環境。此外，允許部署和管理其他網路服務，像是入侵偵測系統（IDS）、負載平衡、防火牆、VPN等。 類似 Amazon AWS 的 VPC。 Swift 物件儲存套件 (Object Storage)Swift套件提供可擴展的分散式儲存平臺，以防止單點故障的情況發生。使用者可透過API進行存取，可存放非結構化的資料，像是圖片、網頁、網誌等，並可作為應用程式資料備份、歸檔以及保留之用。透過Swift套件，可讓業界標準的設備存放PB等級的資料量。而且，當新增伺服器後，儲存群集可輕易的橫向擴充。此外，因為Swift套件是透過軟體的邏輯，確保資料被複製與分布在不同設備上，這可讓企業使用較便宜的設備，節省成本。 類似 Amazon AWS 的 S3。 Cinder 區塊儲存套件 (Block Storage)Cinder套件允許區塊儲存設備能夠整合商業化的企業儲存平臺，像是NetApp、Nexenta、SolidFire等。區塊儲存系統可讓IT人員設置伺服器和區塊儲存設備的各項指令，包括建立、連接和分離等，並整合了運算套件，可讓IT人員查看儲存設備的容量使用狀態。Cinder套件並提供快照管理功能，可保護虛擬機器上的資料，作為系統回復時所用，快照甚至可用來建立一個新的區塊儲存容量。 類似 Amazon AWS 的 EBS。 Ceilometer 資料監控計量套件(Telemetry)Ceilometer提供OpenStack雲端服務可藉由監控與量測OpenStack的使用，來收集CPU與網路的使用資料，以提供收費計價（Billing）、評測（Benchmarking）等使用，或是使用這些資料當作評估系統延展性以及進行系統相關統計之用。 Heat 編排模板套件 (Orchestration)Heat主要提供一個以模板（Templeate）為基礎的架構來描述雲端的應用，模板中可以讓使用者建立如虛擬映像實體（Instance）、浮動IP位址、安全群組（Security Group）或是使用者等OpenStack各種資源，也就是說，Heat讓使用者可以設定一個雲端應用模板，來串連建立設定相關所需的OpenStack服務資源，而不必一個個分別去建立設定。 Sahara 資料處理套件 (Data Processing)Sahara 目的是提供給搭建Haddoop 分散式叢集的工程師能用簡單的概念， 就能在 OpenStack 上面部署和管理「Haddoop 分散式叢集」。Sahara 也提供了MapR Distribution、Spark、Cloudera、Hortonworks插件，替IT人員打造一系列Hadoop ecosystem。 Trove 資料庫服務套件 (Database as a Service)Trove主要負責銜接與簡化實際資料庫的使用，提供OpenStack各個服務一個具延展性且可靠的雲端資料庫服務（Cloud Database-as-a-Service），Database服務包含了銜接傳統關聯式資料庫與新興非關聯式資料庫。 Ironic 裸機部署套件 (Bare Metal )Ironic裸機部署功能，在Kilo版中釋出，IT人員可以在實體伺服器自動化部署OpenStack，等於能用管理虛擬機器的方式，來管理實體伺服器，有助於一次部署大量OpenStack主機來滿足大型IaaS環境的需要。 Zaqar 雲端訊息佇列服務(Message service)Zaqar 是對 Web 開發人員提供了多租戶（Multi tenant）的雲端訊息服務。它結合開創了 Amazon 的 SQS 產品與附加的語義來支援事件的廣播想法。本服務擁有一個完全基於 RESTful 的 API，開發人員可使用他們的 Saas 與行動應用程式的各種元件之間的訊息發送，透過使用多種通訊模式。這個 API 是一種高效的訊息傳送引擎設計，充分的考慮可擴展性與安全性。然而其他 OpenStack 的套件可以與 Zaqar 的表面事件 End users 進行整合以及與訪客的Agent運作於 『Over-cloud』層。雲端公司可以利用Zaqar提供如同 SQS 與 SNS給他們的客戶。 Barbican 金鑰管理服務(Key management)Barbican 是一個以 REST API 設計來進行安全儲存、配置以及機密的管理，如密碼、加密金鑰以及 X.509 憑證。其目的是為了適用於所有環境，包含大型短暫性雲端。 Designate DNS管理服務 (DNS)Designate 提供了 DNSaaS 服務於 OpenStack 上，包含以下幾項功能： 使用 REST API 管理 domain/record 多租戶 整合 Keystone 驗證 以框架來整合 Nova 與 Neutrion 的通知（自動產生記錄） 支援立即可用的 PowerDNS 與 Bind9 Manila 共享式檔案系統服務 (Shared Filesystems)Manila 提供 OpenStack 共享的檔案系統，核心概念有共享目錄、ACL、共享網路、快照與後端驅動程式，目前支援有 GPFS、GlusterFS、EMCVNX等。在雲端平台上，所有服務必須要考慮多租戶資源隔離，目前 Manila 的多租戶資源隔離依賴於 Neutron 的私有網路隔離。 Magnum 容器即服務 (Containers service)Magnum 是一個 OpenStack API 服務，是由 OpenStack Containers Team 開發作為Container orchestration 的引擎，諸如 Docker、Kubernetes 這一類別可以在 Openstack 上作為資源。Magnum 使用 Heat 來編排一個 OS Image，其中包含 Docker 以及 Kubernetes，並執行 Image 於任何的虛擬機或 Bare Metal 叢集配置。 Murano 應用程式目錄服務(Application Catalog)Murano 專案引入一個 Application Catalog 於 OpenStack，使應用程式開發人員與雲端管理人員，可以快速的發布各種已就緒的雲端應用程式，並以目錄方式進行分類。雲端使用者、包括沒經驗的人可以通過統一的框架與 API 實現應用程式的快速部署與應用程式的生命週期管理，來降低應用程式對底層平台（IaaS 層）的依賴。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>虚拟化</tag>
        <tag>openstack</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析监控项]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code-metrics-20160927.html</url>
    <content type="text"><![CDATA[主要讨论常见的性能指标，cpu,memory,network，filesystem 看下几种比较好的工具是如何搜集这些性能数据的，并且结合cadvisor进行具体的分析，有些内容比较琐碎，只能慢慢完善了。 想起来初中生物老师说过的话，微观的问题往往通过宏观的现象表现出来。当时感觉不明觉厉。运用到性能监控这里，代码级别的性能问题，反应出来，就是各个性能指标得到的信息了。具体的指标都有哪些，这些指标的含义是什么，应该算是基本功了，对这一部分的学习，要是基于此，并不停留于此。 关于CPUuptimeload average 这个指标可以用来查看系统负载，所谓系统负载，即是在特定时间间隔内，运行队列中的平均进程数。 如果一个进程满足以下条件则其就会位于运行队列中： 它没有在等待I/O操作的结果 它没有主动进入等待状态(也就是没有调用’wait’) 没有被停止(例如：等待终止) 一般来说，每个CPU内核当前活动进程数不大于3，则系统运行表现良好！ 当然这里说的是每个cpu内核，也就是如果你的主机是四核cpu的话，那么只要uptime最后输出的一串字符数值的和小于12，即表示系统负载不是很严重.当然如果达到20，那就表示当前系统负载非常严重，估计打开执行web脚本非常缓慢。这个指标还是与一定程度上还是与机器上的进程相关的，因为衡量的是等待队列的长度。 一般常用的工具中（比如top工具）的load average是统计 1分钟 5分钟 15分钟 时间段内的load average。 vmstatvmstat是输出的信息比较全面，memory,swamp,io,cpu,system 可以从不同的角度对系统进行衡量，还有一些更细节的参数，可以用于信息的输出。 vmstat 1 可以每隔1s中打印出来一些信息（后面的信息是这1s之内的信息？？？），其中最后的几行是跟cpu有关的信息。先只看最后几行的信息，其实就是各种cpu时间的占比，其实这几个值是对cpu时间的进一步细分。 us os在用户模式下所消耗的cpu时间占比。 sy os在系统模式下所消耗的cpu时间的占比，在内核模式下执行的时间的占比。 id 空闲的时候的cpu占比 wa 实际上是iowait time 等待时间 比如线程在I/O处理的时候阻塞，这个时候的cpu时间就被算到了id里面 就是等待着去为线程服务的时间 st（不是太常用） cpu steal time 在虚拟化的环境下，cpu被调用起来用于服务其他虚拟化资源的时间，注意这个并不是指虚拟机运行时候所占用的cpu时间，而是说hypervisor调用一些资源来进行，这个调用操作所占用的这部分时间，参考这里,通常解释就是 Time stolen from a virtual machine，这个里面有好多例子，介绍不同情况下，各个数据看起来是什么样子的。 其实这个还是通过cpu的各种不同时间，从比较宏观的角度，反映出机器上进程的状态是什么样的，比如系统调用过多，RAM瓶颈，high IO Read/Write，以及cpu wait IO 等等。 mpstatmpstat从更细的粒度上对cpu指标进行测量，比如mpstat -P ALL 1可以每隔1s的时间打印出本机上所有的cpu状态信息。具体测量方法也是对cpu的时间进行分类，名字不同，本质上是与vmstat是类似的，类别分的也更细一点，具体在需要的时候可以更细致地查询，这里就不再一一罗列。采用sar -P ALL 1也可以实现类似的结果。 其他工具再细化的话，可以通过ps -aux或者 ps -ef查看每个进程所占用的cpu时间，如果就在终端查看的话，top工具应该比较实用，上面介绍的相关参数信息，在top工具中都有具体的体现。 由于top工具展示的数据比较多，因此就可以比较好的发现进系统出现的异常，比如哪个进程占用了过多的资源等等。当然类似的工具还有很多，可以从不同粒度对cpu的时间进行衡量，比如time,ptime,pidstat，要是粒度更细的话，可以从代码的层级进行衡量，比如使用DTrace，这里暂不讨论。 还有一个使用很广泛的工具,sysdig，可以达到很细的粒度。 在cadvisor中 cpu相关指标测量 CpuStats 以及 TaskStatsstats api的具体输出如下 结果是ns。其中cpu_inst表示的是In nanocores per second (instantaneous)的cpu使用情况，可以认为是瞬间的使用率，表示cpu的瞬间使用率，即是在1s钟的时间内，cpu的使用的绝对时间。 1234567891011121314151617181920212223"cpu": &#123; "load_average": 0, "usage": &#123; "per_cpu_usage": [ 9866509286700, 9889084073920 ], "system": 12599470000000, "total": 19755593360620, "user": 6738860000000 &#125;&#125;,"cpu_inst": &#123; "usage": &#123; "per_cpu_usage": [ 61482158, 71646320 ], "system": 60023536, "total": 133128478, "user": 50019613 &#125;&#125;, summary api的相关部分具体输出如下 (又细分为 day usage，hour usage，minute _ usage) 12345678"cpu": &#123; "fifty": 70, "max": 232, "mean": 96, "ninety": 174, "ninetyfive": 213, "present": true&#125;, 先看下 summary api 这部分的含义，首先确定搜集的数据是什么，这里补充下百分比的这种表示方式，可以参考这里，在95%的时间之内，使用量低于这个值，其他的类似。剩下的几个是平均值，最大值等等。这里测量的值是 1Mean, Max, and 90p cpu rate value in milliCpus/seconds. Converted to milliCpus to avoid floats. 即是1s之内的cpu的使用时间（多少毫秒），这个是针对于每个容器而言的。比如这里，就是95%的采样时间都低于213ms。Instant sample 会在1s内更新一次。如果second数据足够多，就会产生minute数据。具体的这些指标又可以从 day usage，hour usage，minute _ usage 几个角度进行了细分。 关于summary stats 目前仅仅是追踪 cpu 以及 memory 的信息 关于TaskStats 具体cpuloader（manager中的一个字段）是由linux中的netlink实现的，cadvisor对netlink实现了一个封装，具体在utils/cpuload/netlink文件夹中，里面有个example.go的文件，介绍了主要的使用方式，从cgroups文件夹中获取不同的对应的信息，cpuloader主要用来获取TaskStats的相关信息。 netlink是一个用于在用户空间和内核空间进行通讯的工具，也是一种socket可以参考这个 http://blog.csdn.net/bingqingsuimeng/article/details/8470029 通过netlink可以得到的具体的信息，这部分信息实际上最后被放在TaskStats LoadStats字段当中。 1234567891011121314// Number of sleeping tasks.NrSleeping uint64 `json:"nr_sleeping"`// Number of running tasks.NrRunning uint64 `json:"nr_running"`// Number of tasks in stopped stateNrStopped uint64 `json:"nr_stopped"`// Number of tasks in uninterruptible stateNrUninterruptible uint64 `json:"nr_uninterruptible"`// Number of tasks waiting on IONrIoWait uint64 `json:"nr_io_wait"` 在containerStats中的cpu usage中包含的信息可以根据libcontaner中的相关操作得出，其中显示的单位是ns，明显这种显示不是太友好，百分比的形式会更好点，percpuusage显然是每个cpu使用的时间。 在updateStats的时候，会通过manager查看一下其cpuloader是否为nil之后get相关的信息,可以看到最后的结果被存在了stats.TaskStats字段中。这部分的信息目前是单独获取的，还没有被缓存起来。 12345678910111213if c.loadReader != nil &#123; // TODO(vmarmol): Cache this path. path, err := c.handler.GetCgroupPath("cpu") if err == nil &#123; loadStats, err := c.loadReader.GetCpuLoad(c.info.Name, path) if err != nil &#123; return fmt.Errorf("failed to get load stat for %q - path %q, error %s", c.info.Name, path, err) &#125; stats.TaskStats = loadStats c.updateLoad(loadStats.NrRunning) // convert to 'milliLoad' to avoid floats and preserve precision. stats.Cpu.LoadAverage = int32(c.loadAvg * 1000) &#125; 按理说在/sys/fs/cgroup/cpuacct这里面都可以找到cpu相关的信息，为何还要使用netlink socket的方式？具体原因可以参考 cpuacct gives us CPU usage, but the netlink code will get us load which we define as the number of threads waiting on CPU. This is not provided by cpuacct today. (在cadvisor中所定义的load包含等待cpu的task的数目(task load的相关信息) 单纯的cpuacct无法提供相关信息 这个与vmstat有点类似了) 在cadvisor中，这部分自己定义的信息叫做LoadStats，于CpuStats（包含相对普通的cpu信息）有所区别。 关于Memory背景知识补充 catched memroy 读写文件的时候，一些文件会在cache中缓存，以便提高读写速度，这部分内容就存在cache中。缓存内存(Cache Memory)在你需要使用内存的时候会自动释放，所以你不必担心没有内存可用。 page cache 以及 buffer cache 两种cache都是 disk到memory中的中间结构，只不多存储的内容不同。 free通过free命令查看内存的使用情况 第一行 total 总共的物理内存 used 应用程序所占用的内存+cache做占用的内存 free 完全没有被使用的内存 free+used=total shared 应用程序共享的内存 buffers 缓存，主要用于目录方面，inode值等（ls大目录可看到这个值增加） cached 缓存，用于已打开的文件 其他注意 total=free+used used &gt; buffer+cached (所谓应用程序占用实际上就体现在这两个方面) 第二行描述应用程序的内存使用： 前个值表示-buffers/cache —— 应用程序本身可以使用的内存大小，used减去缓存值 后个值表示+buffers/cache —— 所有可供应用程序使用的内存大小，free加上缓存值 注意-buffers/cache=used-buffers-cached+buffers/cache=free+buffers+cached 第三行表示swap的使用： used——已使用 free——未使用 实际场景又一次进行监控，发现top时候，%MEM中显示的容器的内存占用百分比与通过cgroup的时候进行操作的结果是不一致的。在docker stats中显示出来的结果明显大于通过top命令显示出来的结果，这就是所谓的“度量不一致“导致的原因吧。 通过ps -aux可以看到 RSS 以及 %MEM 这里的RSS表示的是（resident set size）表示的是系统的常驻内存，而这里的%MEM表示的是RSS所站的内存总量的百分比。 通过top命令显示出来的字段被称为RES这个实际上也表示的是常驻内存，就是不同工具带来的拼写有差异。 在docker stats &lt;容器id&gt;中显示出来的当前容器所占用的内存，实际上是cgroups文件系统中的memory.stat文件下，cache+rss两部分的总和，所以通过docker stats或得到的memory会稍微大一些。 vmstat首先还是看vmstat中的数据，这个还是从比较宏观的层面上显示了一些具体的指标信息。其中涉及内存健康状况的信息包括以下方面： memory角度这里的单位都是 KB （注意buffer与cache的区别） swpd 从内存中换出的容量（从memory换出到disk上的容量） free 当前可以使用的memory的大小 buff 在buffer cache中的内存的大小 cache 在page cache中的内存大小 swap角度 si 被换入的内存的大小 so 被换出的内存的大小 top通过top工具的Mem那一行可以比较清楚地看出mem的占用情况，其中 total=used+free used=程序实际使用的+buffer(buffer cache)+cache(page cache) cadvisor中的实现cadvisor中的内存指标数据 12345678910111213141516type MemoryStats struct &#123; // Current memory usage, this includes all memory regardless of when it was // accessed. // Units: Bytes. Usage uint64 `json:"usage"` // The amount of working set memory, this includes recently accessed memory, // dirty memory, and kernel memory. Working set is &lt;= "usage". // Units: Bytes.（实际上是一些cache page） WorkingSet uint64 `json:"working_set"` Failcnt uint64 `json:"failcnt"` ContainerData MemoryStatsMemoryData `json:"container_data,omitempty"` HierarchicalData MemoryStatsMemoryData `json:"hierarchical_data,omitempty"`&#125; 具体的信息还是通过runc/libcontainer获得的。libcontainer可以获得的信息包括： 1234567type Stats struct &#123; CpuStats CpuStats `json:"cpu_stats,omitempty"` MemoryStats MemoryStats `json:"memory_stats,omitempty"` BlkioStats BlkioStats `json:"blkio_stats,omitempty"` // the map is in the format "size of hugepage: stats of the hugepage" HugetlbStats map[string]HugetlbStats `json:"hugetlb_stats,omitempty"`&#125; 其他的信息就需要通过别的渠道获取了。获取的方式也就是打开对应的文件，之后得到相应的数值。在/sys/fs/cgroup/memory中包含大量的了memory相关的信息。具体含义可以参考cgroup的相关说明。之后通过相关的函数把从cgroups中获得到的信息即sgroup.stats转化成为cadvisor可以使用的containerStats usage字段指的是 cgropup中进程当前使用的总得内存量 （实际上cgroup还可以对内存用量使用量的上限进行限制，因此还可以设置内存上限的使用量以及超过这个使用量的次数） workingset字段指的是 cgroup中的total inactive anon ？？？of bytes of anonymous and swap cache memory on inactive LRU list（需要把LRU Page再看看） Failcnt show the number of memory usage hits limits 指的是缺页的次数 (可能达到了limit还发生swamp交换？？？) 关于cgroup在memory方面的使用可以参考，里面对于memrory的参数解释的比较清楚，比如memsw的含义，上限控制（含有soft时候的区别），oom killer等等。对于cgroup更全面的解释，最全的地方就是参考官方文档了，其他的比较好的资源比如这个。里面对page的分类说的很好。 关于networkcgroup本身没有对network进行什么限制。 12345678type NetworkStats struct &#123; InterfaceStats `json:",inline"` Interfaces []InterfaceStats `json:"interfaces,omitempty"` // TCP connection stats (Established, Listen...) Tcp TcpStat `json:"tcp"` // TCP6 connection stats (Established, Listen...) Tcp6 TcpStat `json:"tcp6"`&#125; terfaceStats就是网络interface的信息 容器没有自己的网络占的时候，就不搜集对应的信息（比如运行在k8s的 pod中的容器） 具体的信息也都是从/proc//net/dev文件当中获取的，主要就是TCP链接的各种状态。 在./cadvisor/contianer/docker/handler.go GetStats（被housekeeping调用） 函数以及 ./cadvisor/libcontainer/helper.go 文件中的GetStats函数比较关键，它们两个是互相调用的关系，通过这个可以看出哪些信息是从libcontainer中搜集过来的，以及哪些信息是从host的其他地方搜集来的（比如 Filesystem 的信息以及 Network 的信息） 关于Filesystem单独使用的一系列函数得到的相关信息。 iskIoStats这个信息是在cgroup信息转化为cadvisor中的container信息的时候进行的操作，具体可以参考这个函数toContainerStats1 信息来自于BlkioStats,主要之cgroup对于blkio方面的限制。可以参考这个http://www.elmerzhang.com/2012/12/cgroups-learning-6-blkio-subsystem/ 具体cadvisor中的结果 12345678910111213141516171819202122232425262728"diskio": &#123; "io_service_bytes": [ &#123; "major": 8, "minor": 0, "stats": &#123; "Async": 1646592, "Read": 1536000, "Sync": 0, "Total": 1646592, "Write": 110592 &#125; &#125; ], "io_serviced": [ &#123; "major": 8, "minor": 0, "stats": &#123; "Async": 54, "Read": 27, "Sync": 0, "Total": 54, "Write": 27 &#125; &#125; ]&#125;, cgroup中关于block io system 的表述 https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt 主要是理解 : 这种表述的含义。可以参考下这个，设备号被记录在/proc/devices 文件中，the major number identifies the driver associated with the device，The minor number is used by the kernel to determine exactly which device is being referred to， the minor number is used by the kernel to determine exactly which device is being referred to. 说的直接一点就是主设备代表的是驱动的大的类别，从设备代表的是驱动的具体的实体，主要是对主从设备号有一些理解。 用户自定义Metric可以自定义一些metrics添加到manager的对应字段中 相关参考资料cgroup各种参数中文版解释（由于好多信息都来自libcontainer而libcontainer中又是对cgroups的封装 直接查看文档比较快 这个资源很不错） https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch-Subsystems_and_Tunable_Parameters.html#sec-blkio 关于linux io的监控： http://www.cnblogs.com/york-hust/p/3793064.html network信息获取： 关键需要计算每秒的值 http://xmodulo.com/measure-packets-per-second-throughput-high-speed-network-interface.html proc中的字段详细信息 http://linux.die.net/man/5/proc]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析总结篇（五）]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code5-20160927.html</url>
    <content type="text"><![CDATA[这一部分主要是对与cadvisor项目一些实现的整理和体会，逐步完善中。 关于Watch部分这一部分有一点技巧性，还是回到start函数上，主要是用来watch新启动的容器。 首先是检验root container是否存在，就是Name为“/”的容器。 生成eventchannel，channel中传递数据的类型是container.SubcontainerEvent类型。 执行root container的WatchSubcontainers方法，把之前生成的eventchannel传入。 之后使用for+select操作，一直进行如下循环：如果eventchannel传来的是event实例，则进行进一步判断，若event的原因是add，则自动执行createContainer的操作，若是delete，则执行destroyContainer的操作。如果传递进来的是quit信号，执行StopWatchingSubcontainers的操作。 大致流程还是比较清晰的，再看下具体watchsubcontainer的实现，所有类似watch的核心思想都是：只要有变化，就收到相关的通知。 首先要补充下inotify的相关内容：inotify是内核提供的用于文件系统监控的一套机制，具体网上的参考资料也比较多，这里只要熟悉在golang中对其的封装即可。这里的例子也比较通俗易懂。主要是注意一下各种event的类型。 由于root容器使用的是rawcontainerhandler的实现，可以看下watchfornewcontainer函数中这一步的实现：err := root.handler.WatchSubcontainers(eventsChannel)其中包含了对于传入进来的eventchannel的处理，之后后面就是通过判断eventchannel的返回值来决定继续添加或者删除容器。 在rawcontainerHandler对于WatchSubcontainers的实现中，先是把所有的cgroup path放在watch实例的监控范围内（相当于所有cgroups的层级结构都受到了watch的监控 当然其中也有一些同步的操作 从watch的列表中添加已有的path删除已经过期的path）之后就是for+select的形式，收到watcher.event，watcher.error以及stopWatcher不同信号时候的处理。 收到watcher.event之后，主要的操作是把watcher传递过来的event转化成container.SubcontainerEvent，因为通过watcher直接传递过来的原生event的信息还是很多的，具体的Type类型也有多种，实际并用不了这么多，只需要SubcontainerAdd 以及 SubcontainerDelete两种类型即可，之后进行转化并且执行一些watch的更新操作，最后把新生成的SubcontainerEvent对象赋值给之前的那个eventChannel。 再回到start函数的地方，可以看到，最后会新启动一个goroutine，来运行manager的globalHousekeeping设置定时器，每次隔interval的时间就detectSubcontainers，或者接受到quit信号退出。 在Start操作的时候，最后两步生成了两个quitechannel它们用于实现退出的操作。]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析API篇（四）]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code4-20160927.html</url>
    <content type="text"><![CDATA[这一部分主要是从API的角度进行分析，cadvisor提供的API是怎样暴露的，怎样注册上来的，以及具体功能是怎样的，由于内容比较琐碎，也是一点一点逐步再完善。 具体API分析现在从另外一个维度进行分析，看cadvisor究竟提供了哪些服务出来。 具体api的实现在./cadvisor/api文件中，可以看到目前有newversion1_1 1_2 1_3 2_0几种版本，我们以2_0版本分析。 从start函数中的cadvisorhttp.RegisterHandlers执行具体注册的功能，可以看到除了注册普通的api，还注册了用于性能调优的一些api。这里注册api也没有使用什么特别的框架，直接使用golang自带的serverMux的相关操作，比较容易理解，不再赘述。 这里基本上把所有的request的情况写在了一个函数里，显的比较low，比起kube-apiserver来说，注册api这部分的复杂程度上，简直是差了好几个档次，别的不知道，就扩展性来说，显然不太行，不过考虑cadvisor也都仅仅是需要处理一些get请求，估计不会用到太复杂的api的参数，这样也算是满足需求，不需要杀鸡用牛刀了。 在handleRequest函数中，对传来的request做了第一层处理，即截取出用户输入的api版本，并且交给对应的APIVersion的HandleRequest函数来处理，还要注意，这里每次都要把manager传进去，因为实际的取信息的操作都是通过manager来进行的。 我们这里直接以2.0版本的api为例，分析下对外都暴露出了哪些信息，这些信息是怎样获取到的： v2.0版本有对应的option通过URL的query传递过来： IdType string 指定通过哪种方式识别容器名称，可以是name dockerid 或者 dockeralias Count int指定返回的stats的数目 Recursive bool 是否递归地返回childrencontainer的信息 下面分析下几种不同的requestType: /api/v2.0/stats 具体的每个容器的特别详细的信息可以通过这个API得到。 containerStats的信息比较丰富，包含每个子系统中的具体的信息。ContainerInfo中包含ContainerSpec以及ContainerStats数组，每隔一段时间就会记录一次ContainerStat信息。通过cout的参数控制，可以输出最新的几条containerstats信息。 通过管理篇的分析，可以知道，在createContainer的最后一步，是通过houskeeping的操作不断地updateStats然后存到memoryCache中，这里关键是看下updateStates的时候各部分信息是如何获取到的。 这部分的信息比较复杂，应该是容器主要搜集的信息来源，可以通过下面的图大致看一下具体那部分信息在程序中是怎么得到的。关于每一部分的具体含义来源，以及搜集时候的具体实现可以参考这一篇 /api/v2.0/version 注意下使用golang自带的system package进行系统调用的方式（之前每次都是自己写一个命令之后去system.run），注意一些获取信息的技巧，可以避免很多繁琐的操作。虽然这一步内部manager得到的信息比较多，但是实际返回回来的之后cadvisor的version信息 指标 来源 主机的os版本信息 uname系统调用 容器所运行的os版本 /etc/os-release 文件中 读取PRETTY_NAME dockerdeamon的版本信息 dockerdaemon get version cadvisor本身的版本信息 gobuild的时候从ldflags参数传入 /api/v2.0/attributes 指标 来源 machine info manager中的machine info结构体 主要是/proc/文件系统 version info /api/v2.0/version中所提到的操作 具体实例是在new manager的时候生成的 1234567891011121314151617181920212223242526272829303132333435363738394041type MachineInfo struct &#123; // The number of cores in this machine. NumCores int `json:"num_cores"` // Maximum clock speed for the cores, in KHz. CpuFrequency uint64 `json:"cpu_frequency_khz"` // The amount of memory (in bytes) in this machine MemoryCapacity uint64 `json:"memory_capacity"` // The machine id MachineID string `json:"machine_id"` // The system uuid SystemUUID string `json:"system_uuid"` // The boot id BootID string `json:"boot_id"` // Filesystems on this machine. Filesystems []FsInfo `json:"filesystems"` // Disk map DiskMap map[string]DiskInfo `json:"disk_map"` // Network devices NetworkDevices []NetInfo `json:"network_devices"` // Machine Topology // Describes cpu/memory layout and hierarchy. Topology []Node `json:"topology"` // Cloud provider the machine belongs to. CloudProvider CloudProvider `json:"cloud_provider"` // Type of cloud instance (e.g. GCE standard) the machine is. InstanceType InstanceType `json:"instance_type"` // ID of cloud instance (e.g. instance-1) given to it by the cloud provider. InstanceID InstanceID `json:"instance_id"`&#125; /api/v2.0/machine 这些信息包含在attribute中，直接返回machineinfo。 /api/v2.0/ps 这个会返回所有cgroup中的容器的信息，这个容器作为一个进程会显示出哪些信息，后面可以添加容器id信息，显示对应容器的ps信息(从manager存储的map中取出对应的containerData之后从中再进行筛选)： 比如： 123456789101112131415curl 127.0.0.1:8080/api/v2.0/ps/docker/4f61cb209b685085d5b575173bfa7a5bca822233ae47131ed43033e41fe6505d |python -m json.tool[&#123; "cgroup_path": "", "cmd": "sh", "parent_pid": 13823, "percent_cpu": 0, "percent_mem": 0, "pid": 18957, "rss": 671744, "running_time": "00:00:00", "start_time": "20:18", "status": "Ss+", "user": "root", "virtual_size": 4546560 &#125;] /api/v2.0/spec/ 返回对应的ContainerSpec,比如,这个显示的是容器的spec的信息，显然比起machine的信息要少了好多，就相当于是一个统计清单，看哪些指标包含，哪些指标不包含，比较宏观的一个统计结果： 1234567891011121314151617181920212223curl 127.0.0.1:8080/api/v2.0/spec/docker/4f61cb209b685085d5b575173bfa7a5bca822233ae47131ed43033e41fe6505d |python -m json.tool&#123; "/docker/4f61cb209b685085d5b575173bfa7a5bca822233ae47131ed43033e41fe6505d": &#123; "aliases":["serene_panini","4f61cb209b685085d5b575173bfa7a5bca822233ae47131ed43033e41fe6505d"], "cpu": &#123; "limit": 1024, "mask": "0-1", "max_limit": 0 &#125;, "creation_time": "2016-01-24T12:18:20.067581725Z", "has_cpu": true, "has_custom_metrics": false, "has_diskio": true, "has_filesystem": true, "has_memory": true, "has_network": true, "image": "ubuntu:14.04", "memory": &#123; "limit": 18446744073709551615, "swap_limit": 18446744073709551615 &#125;, "namespace": "docker" &#125;&#125; /api/v2.0/storage 主要是文件系统的信息，比如下面结果 1234567891011121314curl 127.0.0.1:8080/api/v2.0/storage/ |python -m json.tool[ &#123; "available": 321901092864, "capacity": 483753484288, "device": "/dev/disk/by-uuid/ab42c0eb-a891-4261-90cf-557f75f61f15", "labels": [ "root", "docker-images" ], "mountpoint": "/", "usage": 137255526400 &#125;] /api/v2.0/summary 通过containerData中的summaryreader来获取某个cgroups下面容器的某段时间的摘要信息。目前主要追踪的是cpu以及memory的信息。在statsSummary结构中有具体计算每个属性的方式，包括小时的平均信息，分钟的平均信息，等等。 /api/v2.0/appmetrics 可以自定义metrics信息。 从对外暴露的api的角度进行分析info结构得到的信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&#123; "BridgeNfIp6tables": true, "BridgeNfIptables": true, "Containers": 36, "CpuCfsPeriod": true, "CpuCfsQuota": true, "Debug": false, "DockerRootDir": "/var/lib/docker", "Driver": "aufs", "DriverStatus": [ [ "Root Dir", "/var/lib/docker/aufs" ], [ "Backing Filesystem", "extfs" ], [ "Dirs", "244" ], [ "Dirperm1 Supported", "false" ] ], "ExecutionDriver": "native-0.2", "ExperimentalBuild": false, "HttpProxy": "", "HttpsProxy": "", "ID": "ZL2B:AQMX:2S7E:H3PG:V7P6:ITIE:AFEO:P6OL:HPAJ:QFCW:PR6D:PCPG", "IPv4Forwarding": true, "Images": 172, "IndexServerAddress": "https://index.docker.io/v1/", "InitPath": "/usr/lib/docker/dockerinit", "InitSha1": "1f4a3c648015cae3b3d76c5ba2980d8c1f88f388", "KernelVersion": "3.13.0-24-generic", "Labels": null, "LoggingDriver": "json-file", "MemTotal": 8373075968, "MemoryLimit": true, "NCPU": 4, "NEventsListener": 0, "NFd": 90, "NGoroutines": 157, "Name": "ubuntu", "NoProxy": "", "OomKillDisable": true, "OperatingSystem": "Ubuntu 14.04 LTS", "RegistryConfig": &#123; "IndexConfigs": &#123; "docker.io": &#123; "Mirrors": null, "Name": "docker.io", "Official": true, "Secure": true &#125;, "k8stestreg:5000": &#123; "Mirrors": [], "Name": "k8stestreg:5000", "Official": false, "Secure": false &#125; &#125;, "InsecureRegistryCIDRs": [ "127.0.0.0/8" ], "Mirrors": null &#125;, "SwapLimit": false, "SystemTime": "2015-11-24T20:02:48.562597431+08:00"&#125;]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析管理篇（三）]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code3-20160927.html</url>
    <content type="text"><![CDATA[这一部分主要是对manager的相关组件进行分析，manager组件应该是整个cadvisor的核心功能，应该是诸多核心逻辑实现的部分，怎么注册信息，怎么提取数据，何时开始搜集数据，间隔时间怎样，何时停止等等，由于manager部分本身比较抽象，所以理解起来还是比较困难，这里也是大概记录，好多细节也理解的不太好，只能一点一点修改了。 在manager.go文件中可以看到，manager是一个interface类型，里面定义了诸多方法，于是要看下，在main函数中调用start方法的是哪个manager，定位到/manager/manager.go文件中。大致浏览interface中定义的方法，可以发现，manage的主要功能就是负责一些具体的逻辑的操作，怎么样启动，怎么样开始搜集信息等等，之后再调用上面提到之前part2存储部分中提到的cache接口和storage接口来把数据存到合适的地方。 这一块需要从两个角度来分析，一方面manager通过start启动，会开始搜集数据，另一方面，manager也会被注册到api逻辑的实现部分，因为并不是所有数据的都一次存好的，有些数据要等到真正发送api请求的时候，才会去搜集，进行真正的取数操作，相当于是动态进行的，这些动态取数的操作，在part4 中具体介绍api的时候再细分析。 首先从manager自身的角度看一下manager.start的实现。在cadvisor.go文件中，相关的有两个操作，一个manager.New操作，即生成一个新的containerManager组件，另一个是start，即启动manager，启动各种goroutine。 首先来看下manager.New都做了哪些事情: manager.New首先是检验该进程是否运行在容器中。 检验的方式是通过读取/proc/self/cgroup文件的某个子系统（这里是用cpu子系统）来获取容器id。/proc/self/cgroup文件的含义可以查看这里访问/proc/self和使用/proc/&lt;本进程id&gt;的效果是一样的，采用/proc/self会直接通过软链接的方式链接到当前进程所在的对应的目录下。可以参考。 目前在本机上直接运行的时候，这一步似乎有些问题。比如在我的本机上，获得到如下的信息3:cpu:/user/1000.user/c3.session，这似乎与通常形式docker容器的名称不一样。其实这里的容器是广泛意义上的容器，并不仅仅局限于docker容器，只要是被cgroup文件系统所监控的目录，在cadvisor看来，都是一个“容器”，因此直接在宿主机上运行cadvisor的时候（比如在自己的虚拟机上）得到的selfcontianer的id就是这个/user/1000.user/c3.session。??? 毕竟绝大部分还是基于docker容器的，之后docker客户端给本地的docker daemon发请求，得到docker的信息，dockerInfo，之后生成一个叫做context结构的实例，其中包括dockerRoot以及dockerInfo，获取这个context的目的是根据这个信息生成RealFsInfo结构的一个实例，RealFsInfo是对FsInfo interface 的实现，并且还包含partition和labels两个字段cd，具体流程如下： 关于chroot可以参考下相关的资料以及namespace in operation通过/proc/pid/mountinfo文件，可以查看到当前这个进程所在的mount namespace下所有的mount point，具体的文件中每个字段的信息，可以查看这里。 通过对mountinfo文件进行解析之后，每一条信息都会生成一个info结构体，包含对应的那几个字段。对每一条进行解析，首先是检验是否为支持的文件系统（Fstype包含ext的，或者btrfs或者xfs的）在docker context的文件类型为devicemapper的时候，需要单独进行一些额外的操作。最后通过这些从mountinfo文件中提取出来的信息，生成partition map。存储到FsInfo中，并返回，这一块还是有些不理解，主要是对文件系统相关的这块还不是很理解，下面是本机上运行cadvisor，生成RealFsInfo的内容，可以参考下这个(莫非是说docker image所在的设备号？？root那个是什么意思？？)： 1234&amp;&#123;partitions:map[/dev/disk/by-uuid/eb63c885-a9a8-4cc8-aad7-25357b10b3b3:&#123;mountpoint:/ major:8 minor:1 fsType: blockSize:0&#125;] labels:map[root:/dev/disk/by-uuid/eb63c885-a9a8-4cc8-aad7-25357b10b3b3 docker-images:/dev/disk/by-uuid/eb63c885-a9a8-4cc8-aad7-25357b10b3b3]&#125; 之后再回到New函数中，下面一步是判断容器是否存在于hostnamespace(主要判断/rootfs/proc是否存在) 最后根据之前的信息生成manager结构体： 123456789101112131415newManager := &amp;manager&#123; containers: make(map[namespacedContainerName]*containerData), quitChannels: make([]chan error, 0, 2), memoryCache: memoryCache, fsInfo: fsInfo, cadvisorContainer: selfContainer, inHostNamespace: inHostNamespace, startupTime: time.Now(), maxHousekeepingInterval: maxHousekeepingInterval, allowDynamicHousekeeping: allowDynamicHousekeeping, ignoreMetrics: ignoreMetricsSet, containerWatchers: []watcher.ContainerWatcher&#123;&#125;, eventsChannel: eventsChannel, collectorHttpClient: collectorHttpClient,&#125; 之后还要获得machine的信息（主要是查询各种文件）并且进行eventhandler(主要是封装了一些对event进行处理的操作，相当于是一个event manager)的注册，生成最后的manager返回。 关于FsInfo的几个问题还要再补充说明下。 manager的Start()方法start方法就是启动一些一直要运行的go routine来用于实现各种监控操作。 首先是注册factory，facotory实质上是对容器的一些操作的方法的封装，具体可以看./cadvisor/container的package,其中有ContainerHandlerFactory以及ContainerHandler两个主要的接口，ContainerHandler主要是对容器的一些操作的实现，ContainerHandlerFactory是更上层的抽象，比如创建一个containerhandle或者判断当前的containerhandler可否使用。具体对这个两个接口的实现有方式也有两种，一个是在docker 文件夹下的实现，一个是在raw文件夹下的实现。 注册factory的操作就是调用docker.Register以及raw.Register方法将对应的Factory注册到所生成的manager实例中。之后根据参数判断是否启动cpuloadreader。（因为cpuload的值是不断变化的，而且变换速度很快，所以要单独设置一个loaderreader来不断地读取信息） 之后通过manager的watchForNewOoms方法，启动对于OOM的watch操作，这里主要是读取内核的日志文件，并进行解析，看是否捕获OOM信息。 后面的两个操作比较重要：createContainer(&quot;/&quot;)以及detectSubcontainers(&quot;/&quot;) 在manager中的具体操作如下，它们的功能主要就是注册cadvisor文件系统中名字为”/“的容器以及文件目录层次中的其它容器，稍后再具体分析这两个函数。 之后通过manager来watchNewContianer，即是watch cgroup的文件系统，如果其中有新的容器（目录）添加或者被删除，则需要动态地对内存中所存储的信息进行更新删除操作，保持存储的信息与实际的信息一致。 最后开启一个新的goroutine，其中主要执行的是一个定时器的操作，如果没有接收到quite信号，就继续执行detectSubcontainers(“/”)的操作，可以看到，具体的添加注册容器的操作应该都是在createContainer和detectSubcontainers中进行的，先看一下这一部分的结构图： 详细分析下涉及到的主要的函数 func (m *manager) createContainer(containerName string) 1234567891011121314151617创建containerHandler,主要是通过遍历factories，根据containerName看是否能该factories处理，如果可以处理，就调用对应factory的NewContainerHandler方法。创建containerManager (拥有更上一层的方法的抽象) 实例 m，返回的是一个GenericCollectorManager的结构，里面有两个字段，一个是`[]*collectorData&#123;&#125;`数组，另外一个是下次开始搜集的时间。创建newContainerData通过containerHandler的GetCollectorConfig得到collectorConfigs(一个map)通过containerManager的registerCollectors添加CollectorsAdd colletors 这里还有点问题？？？检验container是否存在（即之前提到的m.containers字段），不存在则把新生成containerData添加进来。生成event对象，通过m的eventHandler把newEvent添加进来。之后开启containerData对象的start方法。 前面已经列出了manager对象的主要结构，其中与这部分相关的是这几个： 123456789101112131415161718type manager struct &#123; containers map[namespacedContainerName]*containerData containersLock sync.RWMutex memoryCache *memory.InMemoryCache fsInfo fs.FsInfo machineInfo info.MachineInfo quitChannels []chan error cadvisorContainer string inHostNamespace bool eventHandler events.EventManager startupTime time.Time maxHousekeepingInterval time.Duration allowDynamicHousekeeping bool ignoreMetrics container.MetricSet containerWatchers []watcher.ContainerWatcher eventsChannel chan watcher.ContainerEvent collectorHttpClient *http.Client&#125; 可以看到，对容器进行实际操作的应该是这个containerData结构。 1234567891011121314151617181920212223242526type containerData struct &#123; handler container.ContainerHandler info containerInfo memoryCache *memory.InMemoryCache lock sync.Mutex loadReader cpuload.CpuLoadReader summaryReader *summary.StatsSummary loadAvg float64 // smoothed load average seen so far. housekeepingInterval time.Duration maxHousekeepingInterval time.Duration allowDynamicHousekeeping bool lastUpdatedTime time.Time lastErrorTime time.Time // Decay value used for load average smoothing. Interval length of 10 seconds is used. loadDecay float64 // Whether to log the usage of this container when it is updated. logUsage bool // Tells the container to stop. stop chan bool // Runs custom metric collectors. collectorManager collector.CollectorManager&#125; 实际上这个containerData也是一个中间层，再详细看下一个主要的几个部分： 具体对容器操作的实现通过container.ContainerHandler进行,通过其方法大致就可以了解，比如像是ListContainer,GetStats这些操作，这是一个接口，rawContainerHandler以及dockContainerHandler对其进行了实现。 containerInfo包含三部分: 12345type containerInfo struct &#123; info.ContainerReference Subcontainers []info.ContainerReference Spec info.ContainerSpec&#125; 以看到，这里也维护了一个containerCache?既然manager已经有了containerCache,这里为何还要维护 ？注意看下创建containerData时候的newContainerData方法，可以发现其中的memoryCache *memory.InMemoryCache参数，这个是从manager那里传进去的，也就是说，对于manager的InMemoryCache进行实际操作的步骤是在containerData这一层完成的，所有新创建的containerData使用的是同一个manager中的InMemoryCache。 SummayReader是一个summary信息 CollectorManager可以注册collector组件，从collector中搜集信息。 EventManager主要是用于监控event信息，稍后再分析。 看到在createContainer的最后一步是执行containerData的Start方法，实际上是用一个goroutine来执行go c.housekeeping()，这个housekeeping的主要部分是一个for循环，主要执行以下操作： 确定housekeeping实际时间，通过select判断，如果接收到stop信号，释放相关用于监控的资源。否则执行housekeepingTick()这里主要的作用是updateStats(),最后执行的是memoryCache的AddStats操作，相关方法在前面存储的部分已经分析过，还有些细节信息，比如containerData实例中的相关信息的更新，就不再赘述，要注意的是，这里存放数据的实际位置是containerData中的memoryCache，containerData中的memoryCache与manager中的InMemoryCache的区别？？？。注意这里的RecentStats函数中的maxStats的选取这里取的是60个。 再回到之前的housekeeping方法，之后是判断在update的时候，相关信息是否要输出，通过lasthousekeepingtime返回下一次housekeeping的时间（allowDynamicHousekeeping以及HousekeepingInterval两个参数就在这里派上用场）。之后就检验时间是否达到下一次，要是没到，就sleep，否则就更新lasthousekeepingtime，重新开始下一次的循环。 注意一下handler的选择问题？？ 实际选择的时候，通过CanHandleAndAccept函数的返回值来判断，是否这个handler可以处理。 rawFactory的CanHandleAndAccept逻辑： 很直接了，如果dockeronly参数被设置为false，或者容器的name为”/“，则CanHandleAndAccept都会返回true。factory注册的顺序先是dockerfactory其次是rawfactory，在检测的时候是遍历factory,执行它们的CanHandleAndAccept方法，那个先返回true，就先把那个factory注册进去，所以以”/“命名的容器应该被rawfactory处理，后面的应该被dockerfactory处理。 dockerFactory的CanHandleAndAccept的逻辑： 检验名称是否match docker的id: 1var dockerCgroupRegexp = regexp.MustCompile(`.+-([a-z0-9]&#123;64&#125;)\.scope$`)` 如果不是则不能处理。 传入的container名称可能是/docker/&lt;dockerid&gt;的形式，之后将containername转化为dockername，就是取出后面一段的容器id。 之后通过dockerclient给docker daemon发请求，获取到这个容器的信息。 以上操作都成功，则认为这个容器可以被dockerFactory来处理。 func (m *manager) detectSubcontainers(containerName string) error 首先是执行getContainersDiff，这个操作会从manager存储的containers(一个map)字段中检测上一步中已经注册进去的containerName为”/“的containerData，之后通过其handler调用ListContainers方法，得到所有的container，之后添加新加入的container，以及移除已经不在list中的container。 通过前面的分析，清楚了createContainer(“/”)，注册进来的容器实际操作的时候，使用的hadler是rawContainerFactory，看下其ListContainer的实现:首先是通过cgroup的绝对路径来查看出都有哪些容器，在递归遍历每个容器的子容器，之后把这些信息存储在map中，之后遍历map，生成info.ContainerReference数组，并返回。 存在其中的cgroups路径就像这样： 12map[memory:/sys/fs/cgroup/memory cpuacct:/sys/fs/cgroup/cpuacct blkio:/sys/fs/cgroup/blkio cpuset:/sys/fs/cgroup/cpuset cpu:/sys/fs/cgroup/cpu] 这个信息是在每次createContainer的时候，通过NewCongainerHandler函数生成的，这个信息是生成registry的时候通过cgroupSubsystems传递进来的。之后list的时候，完全是利用cgroup的层次结构的性质，如果某个容器存在，则对应的cgroups路径，比如：/sys/fs/cgroup/memory/docker/&lt;容器id&gt;就是一个目录，在实际实现的时候，在每个cgroups子系统下，也是只抓取目录，之后会按照cgroups树的结构往下拼，比如/docker/&lt;容器id&gt;，之后进行递归操作，这一块有一点技巧性，本质上是目录的遍历操作。 可以这样理解，每次ListContainer操作实际上就是把对应的cgroups整个文件系统遍历了一次，之后按照对应的路径，吧文件系统的叶子节点存到map中返回。 这里有个问题，不加筛选的把cgroups下面的内容都遍历了一遍？在ubuntu上，容器的相关信息被放在对应的/docker目录下，在其他系统，比如centos上，容器被放在对应的/system.slice目录下。很有可能其他的某些使用cgroups文件系统的进程和cadvisor需要监控的进程发生混淆？还是说只要是cadvisor下面的进程都是cadvisor的监控对象？ 这应该就是所谓的rawContainerHandler，就是从cgroups的视角去看，把受到cgroups目录树结构控制的进程都看做是一个raw的container。 有了上面的分析，对getContainerDiff的操作就更容易理解了。首先是按照上面的操作将所有的cgroups受控进程信息取出来。读取manager中已经记录的container信息和实际从cgroups中获取到的容器信息，二者进行比较，得到新add进来的容器以及已经删除掉的容器，以containerRefference数组的形式分别返回，这个就是getContainerDiff的全部操作。 显而易见，getContainersDiff应该会被多次调用，因为经常会需要检测有哪些容器被新加入或者删除。 总结可以看到manage部分都是围绕manager组件进行操作的，主要的逻辑就是先将root容器注册进去之后，之后以root容器为起点，进行detectsubcontainer的操作，更新删除容器信息。具体的存储在内存中的信息就是containerStates的结构。这也只是cadvisor能提供的所有信息中的一部分，之后定期进行housekeeping操作，更新容器的状态。 在入门篇中也提到，在生成containerManager的时候要把inMemoryCache对象传入进去，之后存储manager就会通过inMemoryCache来对存储相关的操作进行进一步的控制。更详细地说，是所用生成的containerData实例持有一个对inMemoryCache的指针，进行具体操作。]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析存储篇（二）]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code2-20160927.html</url>
    <content type="text"><![CDATA[storage模块从逻辑上讲相对比较基本，但同时又是很重要的一块，因为上层搜集到的数据，如果想要分析，首先需要存储起来，而存储的操作都要通过这一层来实现进一步的处理。 Storage Component分析storage部分主要有两个功能 把从宿主机上搜集出来的数据存储在内存中，每隔一段时间，还需要把内存中的数据更新一次。 把数据推送到 storage backend中，就像前面分析的那样，比如把数据发送到elastic search或者influxdb的backend中。 先大致分析下涉及到的package： ./cache ./storage ./info 几个package。 info./info 中的内容比较直接，是各种metrics的实际struct构成，大致看下其中的文件，可以了解到cadvisor从 machine 与 container 两个角度对资源进行描述，目前使用的资源描述结构都是v1版本，这里仅仅是说明一下，用到的时候再针对性地具体查看。 storage./storage package通过提供storagedriver接口（定义在storage.go中）对外暴露服务，其中的几个函数定义的比较直接：AddStats 将信息添加到对应的后端中；Close 停止存储后端的操作，不同后端的具体实现方式可能有区别；New 生成对应的StorageDriver，具体不同的实现分别在./storage下的几个不同的文件夹中，目前有bigquery,elasticsearch,indluxdb,redis几种backend实现。还可以直接将结果输出到标准输出（默认输出）或者将结果发送个某个daemon（通过host:port生成net.Conn之后把数据写进去），具体使用的时候，可以对应着某一个实现进行进一步分析。 12345678type StorageDriver interface &#123; AddStats(ref info.ContainerReference, stats *info.ContainerStats) error // Close will clear the state of the storage driver. The elements // stored in the underlying storage may or may not be deleted depending // on the implementation of the storage driver. Close() error&#125; storage部分的启动参数被放在common_flags.go文件中，可以具体查看，每一部分的backend的相关参启动参数被放在每一部分具体实现的package中，相关参数比较多也比较零碎，可以在有需要的时候针对性地查看。 cache这部分做了许多重要操作，最上层是cache接口： 12345678910111213141516171819type Cache interface &#123; // Add a ContainerStats for the specified container. AddStats(ref info.ContainerReference, stats *info.ContainerStats) error // Remove all cached information for the specified container. RemoveContainer(containerName string) error // Read most recent stats. numStats indicates max number of stats // returned. The returned stats must be consecutive observed stats. If // numStats &lt; 0, then return all stats stored in the storage. The // returned stats should be sorted in time increasing order, i.e. Most // recent stats should be the last. RecentStats(containerName string, numStats int) ([]*info.ContainerStats, error) // Close will clear the state of the storage driver. The elements // stored in the underlying storage may or may not be deleted depending // on the implementation of the storage driver. Close() error&#125; 有必要大致浏览下containerstats即cadvisor搜集回来的到底是容器的哪些信息，具体每个指标的含义这里不作为重点介绍： 1234567891011121314151617type ContainerStats struct &#123; // The time of this stat point. Timestamp time.Time `json:"timestamp"` Cpu CpuStats `json:"cpu,omitempty"` DiskIo DiskIoStats `json:"diskio,omitempty"` Memory MemoryStats `json:"memory,omitempty"` Network NetworkStats `json:"network,omitempty"` // Filesystem statistics Filesystem []FsStats `json:"filesystem,omitempty"` // Task load stats TaskStats LoadStats `json:"task_stats,omitempty"` //Custom metrics from all collectors CustomMetrics map[string][]MetricVal `json:"custom_metrics,omitempty"`&#125; 再看下AddStats中的第一个参数，containerReference的信息，相当于是一个容器的元信息，在map中扮演一个index的角色，通过这个信息来定位容器： 1234567891011121314151617type ContainerReference struct &#123; // The container id Id string `json:"id,omitempty"` // The absolute name of the container. This is unique on the machine. Name string `json:"name"` // Other names by which the container is known within a certain namespace. // This is unique within that namespace. Aliases []string `json:"aliases,omitempty"` // Namespace under which the aliases of a container are unique. // An example of a namespace is "docker" for Docker containers. Namespace string `json:"namespace,omitempty"` Labels map[string]string `json:"labels,omitempty"`&#125; 通过以上的分析，其实cache这块的结构比比较清晰了，输入是什么，输出是什么，具体进行了哪些操作。下面是这部分的结构图： inMemoryCache对cache interface的具体实现在memory.go文件中，有两个struct即inMemoryCache以及containerCache其中。 inMemoryCache是上面所介绍的Cache接口的一个具体实现，其中包含了一个containerCacheMap map[string]*containerCache字段，可以看到，这个map的value值是文件中定义的另一个结构containerCache。这个结构用于执行具体的存储操作，同时也是内存中存放数据的最根本的地方（实质上是一个interface{}组成的slice，具体在utils中实现即utils.TimedStore），就是上图中的recentStats *utils.TimedStore字段。 inMemoryCache中还有一个字段是backend storage.StorageDriver这个是存储后端的实际内容，表示要将什么样的数据放到存储后端，比如influxdb，就像前面所介绍的那样。 下面看下inMemoryCache所实现的AddStats操作，逻辑比较简单: 新生成一个containerCache 用于将stats数据存放在内存中，每个containerCache还包含一个有效期，后面会用到这个有效期。根据传进来的ref参数找到容器的名字，在containerCacheMap中进行检索 若不存在，则生成新的containercache对象 执行backend的AddStats操作，将stats数据推送到后端数据库 执行containercache的AddStats操作，将stats数据存储在本地内存中 在代码实现上，检索containercache是否存在并生成新对象的操作是用goroutine并行处理的。 再进一步，看下containerCache的AddStats操作： 首先考虑当前元素加入进来是否超过了buffer限制的最大长度（生成buffer的时候里面带一个maxitem的字段，默认的是-1即没有限制）若是添加了限制，就会循环使用内存资源，这样会覆盖掉旧的数据，占用的空间会减少。 append操作，把新得到的数据放在slice后面。 对slice进行排序，排序的规则是按时间先后排序，最近发生的会放在最前面。 进行截取操作，更新slice去掉过期数据。 这里细说下更新过期数据的操作，比如当前时间是t，数据过期时间是d，由于slice中存储的是这个容器在某个时间序列上的数据，在这个序列中，如果数据存入的时间是在t-d之前，那么这些数据都属于过期数据。首先通过sort.Search操作找到临界点之后的第一个index,之后进行截取操作self.buffer = self.buffer[index:]这一部分代码相对灵活一些，但是也不难理解。 在看下inMemoryCache的其他操作： RecentStats主要是返回在某段时间之内，某个容器的stats信息，主要实现思路就是根据起止时间确定sclice中的startindex以及endindex之后将数据返回。 Removecontainer主要是从containerCacheMap中移除存储某个容器的slice。 Close操作会将containerCacheMap中的内容清空，即清空了当前存储在内存中的数据。 contanerData字段查看源码，可以看到manager中有一个containers的字段containers: make(map[namespacedContainerName]*containerData)，这是一个map，其中的value值为指向containerData的指针。 根据map的定义也可以看出来，这个结构的主要功能是提供对container的实际操作，也就是被cadvisor识别过来的container都需要在这里注册一下，当然其中还包含相关的handler等等，注意每个containerData中都持有一个对manager的InMemoryCache的指针，这个InMemoryCache实际上是被所有的containerData实例共享的。具体对于InMemoryCache中内容的实际操作也是通过一个个的containerData实例进行的。 containers这个字段的key值也是namespacedContainerName，其value值是*containerData，这个结构的实际作用是对容器进行实际的处理和信息搜集、存储等操作，可以看到其中包含的各种handler，功能上来讲，应该是属于manager的范围，由于这个结构实际上也比较重要，这里就是提一下，具体的相关细节在manager相关的部分进行介绍。 总结根据前面的架构图可以看到这一部分各个组件的具体关系，在part1中也提到，在生成containerManager的时候要把inMemoryCache对象传入进去，之后存储manager就会通过inMemoryCache来对存储相关的操作进行进一步的控制。]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cadvisor源码分析入门篇（一）]]></title>
    <url>%2Fcadvisor%2Fcadvisor-source-code1-20160927.html</url>
    <content type="text"><![CDATA[这几篇文章主要是以cadvisor为例，对类似的资源搜集的agent的实现机理进行一个学习，采用的cadvisor版本是v0.20.0。主要是希望能通过分析cadvisor，对这一类agent所搜集的数据以及搜集方法，都有一个比较深入的理解。希望在梳理完成之后，对一些看似比较浅显的问题，比如cadvisor到底搜集了哪些数据，怎么搜集的，能有很好的回答，在文章整理方面，应该尽量通过 graph driving 的方式来呈现。 主要是梳理下代码的各个模块以及其相关的功能，后面的几篇是对每个模块的较细致分析。 主要是对存储模块进行分析。 主要是对manager模块，也是最核心的模块进行分析。 主要是从user interface的角度进行分析，也就是api的角度，看cadvisor到底对外向用户暴露了哪些功能，也就是从功能的角度上分析，具体都包含了哪些指标等等。 其他方面的整理和收获，比如退出机制，event机制，还有整体上的体会，以及cadvisor在k8s中的使用，如何与heapster结合，等等，从中得到的一些所谓的insights。 主要模块及流程 这个结构图其实也是main函数的主要执行流程。 首先是根据传入的storage参数生成inMemoryCache的实例（这一部分在part2中具体介绍），其中还包含了backendStorage实例，这个backendStorage实例主要是决定除了内存之外，数据会被存放在哪个后端中，实质上是一个storageDriver接口类型，在storage的package中，对于storageDriver接口做了不同的具体实现，memoryStorage实例的初始化的相关操作放在main package的另外一个文件：storagedriver.go中。 之后生成realSysFs结构的实例，其中涉及到的相关函数，是对系统的filesystem所进行的一些操作，realSysFs结构中实际上没有具体的字段，主要是对一系列的方法进行了封装，有点类似于一个工具类，就是通过传入不同的系统文件之后，再从中提取出来不同文件系统的信息。 之后，通过前面生成的memoryStorage以及sysfs实例，创建一个manager实例，这实际上通过一个接口来返回，manager接口中定义了许多用于获取容器和machine信息的函数，生成manager实例的时候，还需要传递两个额外的参数，分别是maxHousekeepingInterval(time.Durattion)以及allowDynamicHousekeeping(bool)分别表示信息存在内存的时间以及是否允许动态配置housekeeping的时间，也就是下一次开始搜集容器信息的时间，默认值分别为60s以及true。可以粗略浏览下manager结构的字段以及相关功能： 123456789101112131415161718192021222324252627282930type manager struct &#123; //当前受到监控的容器存在一个map中 containerData结构中包括了对容器的各种具体操作方式和相关信息 containers map[namespacedContainerName]*containerData //对map中数据存取时采用的Lock containersLock sync.RWMutex //缓存在内存中的数据 主要是容器的相关信息 memoryCache *memory.InMemoryCache //host上的实际文件系统的相关信息 fsInfo fs.FsInfo //machine的相关信息 cpu memory network system信息等等 machineInfo info.MachineInfo // 用于存放退出信号的channel manager关闭的时候会给其中的channel发送退出信号 quitChannels []chan error //cadvisor本身所运行的那个容器(如果cadvisor运行在容器中) cadvisorContainer string // 是否在hostnamespace中？ inHostNamespace bool // dockerid的正则表达式匹配 dockerContainersRegexp *regexp.Regexp // 用于获取cpu信息 loadReader cpuload.CpuLoadReader // 对event相关操作进行的封装 eventHandler events.EventManager //manager的启动时间 startupTime time.Time //在内存中保留数据的时间 也就是下次开始搜集容器相关信息并且更新内存信息的时间 maxHousekeepingInterval time.Duration //是否允许动态设置dynamic housekeeping time allowDynamicHousekeeping bool&#125; 由于还要把服务暴露给外部，所以还要提供一个server的功能来注册api，api可以看成是从另一个维度对程序进行分析，也就是从功能的维度。比起kube-apiserver，真是要简单多了，具体使用上也添加了证书的方式，把上面生成的containerManager注册进去，具体实现在cadvisor/http/handler.go中，可以看到目前已经实现了version1 0,1 1,1 2,1 3,2 0几种，以2 0为例，具体的路由类别主要是以下三种： 一种是通过自带默认界面简单看一下机器上的容器信息，就是/containers/路由，这个目前支持的api版本比较低。 另一种是/api/路由，具体在/cadvisor/api/handler.go中实现，这一部分路由已经设置的比较友好，会提示出当期支持的子路由都有哪些。可以看到在2.0版本中，支持的查询信息已经相当丰富。 另外一个不错的功能就是支持通过页面的方式使用golang的pprof工具，在使用了profiling=true的参数之后，可以通过“net/http/pprof”package来获取应用层面的信息，可以为应用性能调优提供帮助。 之后就是启动manager，运行其Start方法，开始搜集信息，存储信息的循环操作，这之后还为containerManager注册了singlehandler，如果收到了系统发来的kill信号，程序就会捕获到，就直接执行manager的stop函数，manager停止工作。 可以看到，代码的意图在这里表现的很明确，就是生成后端，生成manager，注册api，启动server。还有其他的一些套路化的操作，比如在main函数开始的时候设置MaxProcs,设置runtime.GOMAXPROCS为当前cpu的个数，使得并发性能较高。 后面几篇对每一个部分进行一些相对深入的分析。]]></content>
      <categories>
        <category>cadvisor</category>
      </categories>
      <tags>
        <tag>cadvisor</tag>
        <tag>golang</tag>
        <tag>docker</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转) 什么是NFV?它与SDN有什么关系?]]></title>
    <url>%2F%E7%AC%94%E8%AE%B0%2Fnfv-and-sdn-20160908.html</url>
    <content type="text"><![CDATA[什么叫网络虚拟化？先上两张简单粗暴的图。 所有的通信应用无非就是两部分组成：计算和网络。这两者关系密不可分，但两者关系严重缺乏对称性，网络一直拖累着计算。 4G网络RAN将会不断进化，据说，未来的4G网络空口速率将是现在的10倍。随着智能硬件的爆发，大量的应用接入4G网络，人们的流量需求如同海啸般汹涌而至，我们似乎赶上了好时代。不过，传统的通信网络里，每一类服务对应基于专用集成电路的带有专用处理器的专用服务器。网络里的设备很多，但是，这些家伙就像霸道总裁的办公室，仅供总裁专用，不管他在与不在，位置都占在那里，谁也不敢冒然挪用。 这样下去，网络会严重拖计算的后腿。于是，有人想到了虚拟化。 或者，可以这么理解，服务器觉得老这样拖着网络跑不是办法，迟早会被累死，就开始向网络扔砖头了！ 专用的硬件对应专用的服务，这样的花费是昂贵的，为了节省成本，我们得想想物尽其用。 想想你有一栋超级大的空置的公寓，你想出租，但是每一位租客的需求是不一样的。这个时候，你想到了“虚拟化”。根据租客的需求，把这栋公寓虚拟化成不同的建筑风格。于是，你对租客1说，“我有两栋公寓要出租，你喜欢红色那一栋还是蓝色那一栋？”。然后你对租客2说，“我有三栋公寓要出租，大、中、小，你喜欢哪一栋？”。接着租客3。。。 继续忽悠。。。 然后，所有的租客都住进了你的虚拟公寓，你按住宿时间和使用空间来收租金。 2012年10月，13家运营商发起在ETSI组织下正式成立网络功能虚拟化工作组，即ETSI ISG NFV，致力于实现网络虚拟化的需求定义和系统架构制定。 谈到虚拟化，首先得了解什么是虚拟化技术？最常用的虚拟化技术有操作系统中内存的虚拟化，实际运行时用户需要的内存空间可能远远大于物理机器的内存大小，利用内存的虚拟化技术，用户可以将一部分硬盘虚拟化为内存，而这对用户是透明的。又如，可以利用虚拟专用网技术（VPN）在公共网络中虚拟化一条安全，稳定的“隧道”，用户感觉像是使用私有网络一样。 NFV就是基于大型共享的OTS（Off-The-Shelf，成品）服务器，通过软件定义的方式，探索网络实体的虚拟化使用。在NFV中使用的虚拟机（Virtual Machines，VMs）技术是虚拟化技术的一种。 基于软件定义的虚拟机部署成本低，而且可以快速适应网络需求变化。虚拟机就像是将所有能想到的东西都放在一台物理服务器（physical server）上，有了云计算和虚拟化，那些冗余的服务器都可以部署在独立的物理服务器上，不但可以并行处理，满足网络峰值需求，还可以根据网络需求随时释放资源，方便部署，利于故障管理，快速升级，快速满足市场需求。 NFV技术颠覆了传统电信封闭专用平台的思想，同时引入灵活的弹性资源管理理念，因此，ETSI NFV提出了突破传统网元功能限制、全新通用的NFV架构下图所示。 NFV技术主要由3个部分构成：VNF（虚拟网络层，Virtualized Network Function）、NFVI（网络功能虚拟化基础设施NFVI，NFV Infrastructure)和MANO（NFV管理与编排，Management and Orchestration)。 （1）虚拟网络层是共享同一物理OTS服务器的VNF集。对应的就是各个网元功能的软件实现，比如EPC网元、IMS网元等的逻辑实现。 （2）NFVI，你可以将它理解为基础设施层，从云计算的角度看，就是一个资源池。NFVI需要将物理计算/存储/交换资源通过虚拟化转换为虚拟的计算/存储/交换资源池。NFVI映射到物理基础设施就是多个地理上分散的数据中心，通过高速通信网连接起来。 （3）NFVMANO。基于不同的服务等级协议（Service Level Agreements ，SLAs），NFVMANO运营支撑层负责“公平”的分配物理资源，同时还负责冗余管理、错误管理和弹性调整等，相当于目前目前的OSS/BSS系统。 这样一来，现在的移动通信网络结构就变成了这样： 上图顶部的VNF对应了网元功能的逻辑实现，比如，由多个VNF组成的VNF-FG（VNF Forwarding Graph）定义了LTE网络服务。 那么软件定义网络（SDN）和NFV又有什么关系呢？ NFV负责各种网元的虚拟化，而SDN负责网络本身的虚拟化（比如，网络节点和节点之间的相互连接）。我们先来看看一个典型的网络结构图。 如上图，一个网络由网络节点和节点间的链路组成。每一个节点都有一个控制面和与其它节点交换的网络信息。在上图中，右边的H节点获知一个新的网络（10.2.3.x/24）存在，现在它需要将这一信息告诉给网络中的其它节点。然而，节点H只和节点F和G直接相连，节点H通过链路状态通告（Link State Advertisements ，LSAs）通知节点F和G，F和G再将信息传递给它们的邻近节点，最终该消息传达到整个网络。这样，网络内每个节点都会更新自己的路由表，以确保数据可以传送到网络10.2.3.x/24。 如果节点C和E之间的链路中断，尽管C和E知道C-E链路中断，但节点A并不知道，节点A会继续通过C-E链路向网络10.2.3.x/24传送数据包。由于节点的“近视”，导致数据堆积在该节点，这需要花一些时间来向整个网络传送网络状态更新信息和完成纠错。网络越大，这种情况就越容易发生。 我们再来看看节点G，它由控制面（control plane）和数据面（data plane）组成。为了适应快速更新，控制面基本上是基于软件的，这实际上意味着控制消息的处理时长比基于硬件的逻辑单元（比如，数据面）要慢5到10倍。一直以来，我们仅要求控制面能够灵活更新，但对时延要求并不是太高。相对于数据面来讲，时延要求较高，我们希望数据包能够传送得越快越好，所以它必须是基于硬件来实现的。尽管基于软件实现的控制面能满足目前的要求，但随着设备的大量接入，特别是物联网的应用，控制面的时延也需要进一步提升。 软件定义网络（SDN）负责分离控制面和数据面，将网络控制面整合于一体。这样，网络控制面对网络数据面就有一个宏观的全面的视野。路由协议交换、路由表生成等路由功能均在统一的控制面完成。实现控制平面与数据平面分离的协议叫OpenFlow，OpenFlow是SDN一个网络协议。如下图所示： 从上图中，我们可以看到，首先需要通过OpenFlow将网络拓扑镜像到控制面，控制面初始化网络拓扑，初始化完成后，控制面会实时更新网络拓扑。 控制面完成初始化后，会向每个转发节点发送转发表，根据转发表用户数据在网络内传送。假设现在节点H获知新的网络 (10.2.3.x/24)。节点H将通过OpenFlow告知控制面，因为控制面统领全局，它可以快速的为每一个转发节点创建新的路由表，这样用户数据就可以传送到这个新网络。 转载自：tantexian博客]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网监控, Open-falcon多节点用户配置]]></title>
    <url>%2Fopen-falcon%2Fopen-falcon-deploy-muti-nodes-20160807.html</url>
    <content type="text"><![CDATA[Open-Falcon 是小米运维部开源的一款互联网企业级监控系统解决方案。监控系统是整个运维环节，乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供翔实的数据用于追查定位问题。监控系统作为一个成熟的运维产品，业界有很多开源的实现可供选择。本文档对多节点负载open-falcon部署配置说明。请阅读官方参考文档 机器部署系统：Centos7 主机名 主机IP 备注 falconpoc01 10.128.31.136 open-falcon模块测试机 falconpoc02 10.128.31.137 open-falcon模块测试机 falconpoc03 10.128.31.138 open-falcon数据测试机 模块部署展示 模块 主机名 主机IP 备注 hbs falconpoc01 10.128.31.136 心跳服务 hbs falconpoc02 10.128.31.137 心跳服务 judge falconpoc01 10.128.31.136 告警判断 judge falconpoc02 10.128.31.137 告警判断 graph falconpoc01 10.128.31.136 存储绘图 graph falconpoc02 10.128.31.137 存储绘图 transfer falconpoc01 10.128.31.136 数据转发 transfer falconpoc02 10.128.31.137 数据转发 query falconpoc01 10.128.31.136 绘图查询 query falconpoc02 10.128.31.137 绘图查询 dashboard falconpoc01 10.128.31.136 用户查询 dashboard falconpoc02 10.128.31.137 用户查询 uic falconpoc01 10.128.31.136 用户管理 uic falconpoc02 10.128.31.137 用户管理 portal falconpoc01 10.128.31.136 策略配置 portal falconpoc02 10.128.31.137 策略配置 alarm falconpoc01 10.128.31.136 报警事件 sender falconpoc01 10.128.31.136 报警通知 task falconpoc01 10.128.31.136 定时任务 主机名 主机IP 备注 mysql falconpoc03 10.128.31.138 mysql数据库 redis falconpoc03 10.128.31.138 redis数据库 sms falconpoc03 10.128.31.138 短信接口 模块 主机名 主机IP 备注 agent falconpoc01 10.128.31.136 指标采集 agent falconpoc02 10.128.31.137 指标采集 agent falconpoc03 10.128.31.138 指标采集 环境部署更新centos7源123mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum makecache 安装MySQL1234567yum install mysqlwget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum install mysql-community-serverservice mysqld restart 或者123456yum install mariadb-server mariadbsystemctl start mariadb #启动MariaDBsystemctl stop mariadb #停止MariaDBsystemctl restart mariadb #重启MariaDBsystemctl enable mariadb #设置开机启动 远程访问123mysql -u root GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION;flush privileges; 安装Redis1yum install gcc 下载1234567891011wget http://download.redis.io/releases/redis-3.0.0.tar.gztar zxvf redis-3.0.0.tar.gzcd redis-3.0.0#如果不加参数,linux下会报错make MALLOC=libccp src/redis-server /usr/bin/cp src/redis-cli /usr/bin/mkdir -p /etc/rediscp redis.conf /etc/redis/ 配置redis123vim /etc/redis/ redis.confbind 0.0.0.0daemonize yes 启动redis1/usr/bin/redis-server /etc/redis/redis.conf 登录redis1/usr/bin/redis-cli 关闭redis1/usr/bin/redis-cli shutdown 初始化MySQL数据123456789#下载初始化脚步git clone https://github.com/open-falcon/scripts.gitcd ./scripts/mysql -h localhost -u root --password="" &lt; db_schema/graph-db-schema.sqlmysql -h localhost -u root --password="" &lt; db_schema/dashboard-db-schema.sqlmysql -h localhost -u root --password="" &lt; db_schema/portal-db-schema.sqlmysql -h localhost -u root --password="" &lt; db_schema/links-db-schema.sqlmysql -h localhost -u root --password="" &lt; db_schema/uic-db-schema.sql Python环境123yum install mysqlyum install mysql-develyum install -y python-virtualenv pip换源12345vim ~/.pip/pip.conf[global]index-url = http://pypi.douban.com/simpletrusted-host = pypi.douban.com 模块单点部署 先从模块单点开始部署，完成后再部署多点。前期条件有限，先将open-falcon模块部署falconpoc01服务器，之后再部署相同的模块到falconpoc02。 内部模块相互访问使用host名称，方便于维护。falconpoc01配置 /etc/hosts;添加1234567810.128.31.138 falcon-mysql10.128.31.138 falcon-redis10.128.31.136 falcon-hbs10.128.31.137 falcon-hbs210.128.31.136 falcon-graph10.128.31.137 falcon-graph210.128.31.136 falcon-judge10.128.31.137 falcon-judge2 部署heartbeat心跳模块 模块部署的打包、上传不会做介绍说明，官方文档上已经写明。 123456789101112131415#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "database": "root:password@tcp(falcon-mysql:3306)/falcon_portal?loc=Local&amp;parseTime=true", "hosts": "", "maxIdle": 100, "listen": ":6030", "trustable": [""], "http": &#123; "enabled": true, "listen": "0.0.0.0:6031" &#125;&#125; 部署judge报警判断模块123456789101112131415161718192021222324252627282930313233#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "debugHost": "nil", "remain": 11, "http": &#123; "enabled": true, "listen": "0.0.0.0:6081" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:6080" &#125;, "hbs": &#123; "servers": ["falcon-hbs:6030"], "timeout": 300, "interval": 60 &#125;, "alarm": &#123; "enabled": true, "minInterval": 300, "queuePattern": "event:p%v", "redis": &#123; "dsn": "falcon-redis:6379", "maxIdle": 5, "connTimeout": 5000, "readTimeout": 5000, "writeTimeout": 5000 &#125; &#125;&#125; 部署graph存储绘图模块12345678910111213141516171819202122232425262728293031#配置cp cfg.example.json cfg.jsonmkdir -p /root/data/6070vim cfg.json&#123; "debug": false, "http": &#123; "enabled": true, "listen": "0.0.0.0:6071" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:6070" &#125;, "rrd": &#123; "storage": "/root/data/6070" &#125;, "db": &#123; "dsn": "root:password@tcp(falcon-mysql:3306)/graph?loc=Local&amp;parseTime=true", "maxIdle": 4 &#125;, "callTimeout": 5000, "migrate": &#123; "enabled": false, "concurrency": 2, "replicas": 500, "cluster": &#123; "graph-00" : "falcon-graph:6070" &#125; &#125;&#125; 部署transfer数据转发模块123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "minStep": 30, "http": &#123; "enabled": true, "listen": "0.0.0.0:6060" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:8433" &#125;, "socket": &#123; "enabled": true, "listen": "0.0.0.0:4444", "timeout": 3600 &#125;, "judge": &#123; "enabled": true, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "judge-00" : "falcon-judge:6080" &#125; &#125;, "graph": &#123; "enabled": true, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "graph-00" : "falcon-graph:6070" &#125; &#125;, "tsdb": &#123; "enabled": false, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "retry": 3, "address": "127.0.0.1:8088" &#125;&#125; 部署query绘图查询模块12345678910111213141516171819202122232425#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": "false", "http": &#123; "enabled": true, "listen": "0.0.0.0:9966" &#125;, "graph": &#123; "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "graph-00": "falcon-graph:6070" &#125; &#125;, "api": &#123; "query": "http://10.128.31.136:9966", "dashboard": "http://10.128.31.136:8081", "max": 500 &#125;&#125; 部署dashboard用户查询模块123#初始化virtualenv ./env./env/bin/pip install -r pip_requirements.txt 123456789101112131415161718192021222324252627282930313233343536#配置vim rrd/config.py#-*-coding:utf8-*-import os#-- dashboard db config --DASHBOARD_DB_HOST = "falcon-mysql"DASHBOARD_DB_PORT = 3306DASHBOARD_DB_USER = "root"DASHBOARD_DB_PASSWD = "password"DASHBOARD_DB_NAME = "dashboard"#-- graph db config --GRAPH_DB_HOST = "falcon-mysql"GRAPH_DB_PORT = 3306GRAPH_DB_USER = "root"GRAPH_DB_PASSWD = "password"GRAPH_DB_NAME = "graph"#-- app config --DEBUG = TrueSECRET_KEY = "secret-key"SESSION_COOKIE_NAME = "open-falcon"PERMANENT_SESSION_LIFETIME = 3600 * 24 * 30SITE_COOKIE = "open-falcon-ck"#-- query config --QUERY_ADDR = "http://10.128.31.136:9966"BASE_DIR = "/root/deploy/dashboard"LOG_PATH = os.path.join(BASE_DIR,"log/")try: from rrd.local_config import *except: pass 部署uic用户管理模块12345678910111213141516171819202122232425262728293031323334353637383940414243#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "log": "debug", "company": "MI", "http": &#123; "enabled": true, "listen": "0.0.0.0:1234" &#125;, "cache": &#123; "enabled": true, "redis": "127.0.0.1:6379", "idle": 10, "max": 1000, "timeout": &#123; "conn": 10000, "read": 5000, "write": 5000 &#125; &#125;, "salt": "", "canRegister": true, "ldap": &#123; "enabled": false, "addr": "ldap.example.com:389", "baseDN": "dc=example,dc=com", "bindDN": "cn=mananger,dc=example,dc=com", "bindPasswd": "12345678", "userField": "uid", "attributes": ["sn","mail","telephoneNumber"] &#125;, "uic": &#123; "addr": "root:password@tcp(falcon-mysql:3306)/uic?charset=utf8&amp;loc=Asia%2FChongqing", "idle": 10, "max": 100 &#125;, "shortcut": &#123; "falconPortal": "http://10.128.31.136:5050/", "falconDashboard": "http://10.128.31.136:7070/", "falconAlarm": "http://10.128.31.136:9912/" &#125;&#125; 部署portal报警策略模块123456789101112131415161718192021222324252627282930313233343536373839#初始化virtualenv ./env./env/bin/pip install -r pip_requirements.txt#配置vim rrd/config.py# -*- coding:utf-8 -*-__author__ = 'Ulric Qin'# -- app config --DEBUG = True# -- db config --DB_HOST = "falcon-mysql"DB_PORT = 3306DB_USER = "root"DB_PASS = "password"DB_NAME = "falcon_portal"# -- cookie config --SECRET_KEY = "4e.5tyg8-u9ioj"SESSION_COOKIE_NAME = "falcon-portal"PERMANENT_SESSION_LIFETIME = 3600 * 24 * 30UIC_ADDRESS = &#123; 'internal': 'http://10.128.31.136:1234', 'external': 'http://10.128.31.136:1234',&#125;UIC_TOKEN = ''MAINTAINERS = ['root']CONTACT = 'CDXXJCPT@gome.cn'COMMUNITY = Truetry: from frame.local_config import *except Exception, e: print "[warning] %s" % e 部署alarm报警处理模块12345678910111213141516171819202122232425262728293031323334353637#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "uicToken": "", "http": &#123; "enabled": true, "listen": "0.0.0.0:9912" &#125;, "queue": &#123; "sms": "/sms", "mail": "/mail" &#125;, "redis": &#123; "addr": "falcon-redis:6379", "maxIdle": 5, "highQueues": [ "event:p0", "event:p1", "event:p2", "event:p3", "event:p4", "event:p5" ], "lowQueues": [ "event:p6" ], "userSmsQueue": "/queue/user/sms", "userMailQueue": "/queue/user/mail" &#125;, "api": &#123; "portal": "http://10.128.31.136:5050", "uic": "http://10.128.31.136:1234", "links": "http://10.128.31.136:5090" &#125;&#125; 部署sender报警发送模块1234567891011121314151617181920212223242526#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "http": &#123; "enabled": true, "listen": "0.0.0.0:6066" &#125;, "redis": &#123; "addr": "falcon-redis:6379", "maxIdle": 5 &#125;, "queue": &#123; "sms": "/sms", "mail": "/mail" &#125;, "worker": &#123; "sms": 10, "mail": 50 &#125;, "api": &#123; "sms": "http://10.128.31.138:8000/sms", "mail": "http://10.128.31.138:9000/mail" &#125;&#125; 部署task定时任务模块1234567891011121314151617181920212223242526272829#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": false, "http": &#123; "enable": true, "listen": "0.0.0.0:8002" &#125;, "index": &#123; "enable": true, "dsn": "root:password@tcp(falcon-mysql:3306)/graph?loc=Local&amp;parseTime=true", "maxIdle": 4, "autoDelete": false, "cluster":&#123; "falcon-graph:6071" : "0 0 0 ? * 0-5" &#125; &#125;, "collector" : &#123; "enable": false, "destUrl" : "http://127.0.0.1:1988/v1/push", "srcUrlFmt" : "http://%s/statistics/all", "cluster" : [ "transfer,test.hostname:6060", "graph,test.hostname:6071", "task,test.hostname:8001" ] &#125;&#125; 部署agent监控采集模块123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "hostname": "", "ip": "", "plugin": &#123; "enabled": false, "dir": "./plugin", "git": "https://github.com/open-falcon/plugin.git", "logs": "./logs" &#125;, "heartbeat": &#123; "enabled": true, "addr": "10.128.31.136:6030", "interval": 60, "timeout": 1000 &#125;, "transfer": &#123; "enabled": true, "addrs": [ "10.128.31.136:8433" ], "interval": 60, "timeout": 1000 &#125;, "http": &#123; "enabled": true, "listen": ":1988", "backdoor": false &#125;, "collector": &#123; "ifacePrefix": ["eth", "ens"] &#125;, "ignore": &#123; "cpu.busy": true, "df.bytes.free": true, "df.bytes.total": true, "df.bytes.used": true, "df.bytes.used.percent": true, "df.inodes.total": true, "df.inodes.free": true, "df.inodes.used": true, "df.inodes.used.percent": true, "mem.memtotal": true, "mem.memused": true, "mem.memused.percent": true, "mem.memfree": true, "mem.swaptotal": true, "mem.swapused": true, "mem.swapfree": true &#125;&#125; 配置网络相关的collector选项，使用ifconfig查看系统网卡命名。collector选项采集网卡名称前缀。如是配置多节点则做后启动agent。 多节点模块部署 将open-falcon模块部署falconpoc02服务器 内部模块相互访问使用host名称，方便于维护。falconpoc02配置 /etc/hosts;添加1234567810.128.31.138 falcon-mysql10.128.31.138 falcon-redis10.128.31.136 falcon-hbs10.128.31.137 falcon-hbs210.128.31.136 falcon-graph10.128.31.137 falcon-graph210.128.31.136 falcon-judge10.128.31.137 falcon-judge2 部署heartbeat心跳模块 模块部署的打包、上传不会做介绍说明，官方文档上已经写明。 123456789101112131415#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "database": "root:password@tcp(falcon-mysql:3306)/falcon_portal?loc=Local&amp;parseTime=true", "hosts": "", "maxIdle": 100, "listen": ":6030", "trustable": [""], "http": &#123; "enabled": true, "listen": "0.0.0.0:6031" &#125;&#125; 部署judge报警判断模块12345678910111213141516171819202122232425262728293031323334#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "debugHost": "nil", "remain": 11, "http": &#123; "enabled": true, "listen": "0.0.0.0:6081" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:6080" &#125;, "hbs": &#123; "servers": ["falcon-hbs:6030", "falcon-hbs2:6030"], "timeout": 300, "interval": 60 &#125;, "alarm": &#123; "enabled": true, "minInterval": 300, "queuePattern": "event:p%v", "redis": &#123; "dsn": "falcon-redis:6379", "maxIdle": 5, "connTimeout": 5000, "readTimeout": 5000, "writeTimeout": 5000 &#125; &#125;&#125;#falcon01也需要修改 部署graph存储绘图模块12345678910111213141516171819202122232425262728293031#配置cp cfg.example.json cfg.jsonmkdir -p /root/data/6070vim cfg.json&#123; "debug": false, "http": &#123; "enabled": true, "listen": "0.0.0.0:6071" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:6070" &#125;, "rrd": &#123; "storage": "/root/data/6070" &#125;, "db": &#123; "dsn": "root:password@tcp(falcon-mysql:3306)/graph?loc=Local&amp;parseTime=true", "maxIdle": 4 &#125;, "callTimeout": 5000, "migrate": &#123; "enabled": false, "concurrency": 2, "replicas": 500, "cluster": &#123; "graph-00" : "falcon-graph:6070" &#125; &#125;&#125; 部署transfer数据转发模块123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "minStep": 30, "http": &#123; "enabled": true, "listen": "0.0.0.0:6060" &#125;, "rpc": &#123; "enabled": true, "listen": "0.0.0.0:8433" &#125;, "socket": &#123; "enabled": true, "listen": "0.0.0.0:4444", "timeout": 3600 &#125;, "judge": &#123; "enabled": true, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "judge-00" : "falcon-judge:6080", "judge-01" : "falcon-judge2:6080" &#125; &#125;, "graph": &#123; "enabled": true, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "graph-00" : "falcon-graph:6070", "graph-01" : "falcon-graph2:6070" &#125; &#125;, "tsdb": &#123; "enabled": false, "batch": 200, "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "retry": 3, "address": "127.0.0.1:8088" &#125;&#125;#falcon01也需要修改 部署query绘图查询模块1234567891011121314151617181920212223242526#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": "false", "http": &#123; "enabled": true, "listen": "0.0.0.0:9966" &#125;, "graph": &#123; "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "graph-00": "falcon-graph:6070", "graph-01": "falcon-graph2:6070" &#125; &#125;, "api": &#123; "query": "http://10.128.31.137:9966", "dashboard": "http://10.128.31.137:8081", "max": 500 &#125;&#125; 部署dashboard用户查询模块123456789101112131415161718192021222324252627282930313233343536373839#初始化virtualenv ./env./env/bin/pip install -r pip_requirements.txt#配置vim rrd/config.py#-*-coding:utf8-*-import os#-- dashboard db config --DASHBOARD_DB_HOST = "falcon-mysql"DASHBOARD_DB_PORT = 3306DASHBOARD_DB_USER = "root"DASHBOARD_DB_PASSWD = "password"DASHBOARD_DB_NAME = "dashboard"#-- graph db config --GRAPH_DB_HOST = "falcon-mysql"GRAPH_DB_PORT = 3306GRAPH_DB_USER = "root"GRAPH_DB_PASSWD = "password"GRAPH_DB_NAME = "graph"#-- app config --DEBUG = TrueSECRET_KEY = "secret-key"SESSION_COOKIE_NAME = "open-falcon"PERMANENT_SESSION_LIFETIME = 3600 * 24 * 30SITE_COOKIE = "open-falcon-ck"#-- query config --QUERY_ADDR = "http://10.128.31.137:9966"BASE_DIR = "/root/deploy/dashboard"LOG_PATH = os.path.join(BASE_DIR,"log/")try: from rrd.local_config import *except: pass 部署uic用户管理模块1234567891011121314151617181920212223242526272829303132333435363738394041424344#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "log": "debug", "company": "MI", "http": &#123; "enabled": true, "listen": "0.0.0.0:1234" &#125;, "cache": &#123; "enabled": true, "redis": "falcon:6379", "idle": 10, "max": 1000, "timeout": &#123; "conn": 10000, "read": 5000, "write": 5000 &#125; &#125;, "salt": "", "canRegister": true, "ldap": &#123; "enabled": false, "addr": "ldap.example.com:389", "baseDN": "dc=example,dc=com", "bindDN": "cn=mananger,dc=example,dc=com", "bindPasswd": "12345678", "userField": "uid", "attributes": ["sn","mail","telephoneNumber"] &#125;, "uic": &#123; "addr": "root:password@tcp(falcon-mysql:3306)/uic?charset=utf8&amp;loc=Asia%2FChongqing", "idle": 10, "max": 100 &#125;, "shortcut": &#123; "falconPortal": "http://10.128.31.137:5050/", "falconDashboard": "http://10.128.31.137:7070/", "falconAlarm": "http://10.128.31.136:9912/" &#125;&#125;#所有uic(fe)节点的salt配置要相同。 部署portal报警策略模块123456789101112131415161718192021222324252627282930313233343536373839#初始化virtualenv ./env./env/bin/pip install -r pip_requirements.txt#配置vim rrd/config.py# -*- coding:utf-8 -*-__author__ = 'Ulric Qin'# -- app config --DEBUG = True# -- db config --DB_HOST = "falcon-mysql"DB_PORT = 3306DB_USER = "root"DB_PASS = "password"DB_NAME = "falcon_portal"# -- cookie config --SECRET_KEY = "4e.5tyg8-u9ioj"SESSION_COOKIE_NAME = "falcon-portal"PERMANENT_SESSION_LIFETIME = 3600 * 24 * 30UIC_ADDRESS = &#123; 'internal': 'http://10.128.31.137:1234', 'external': 'http://10.128.31.137:1234',&#125;UIC_TOKEN = ''MAINTAINERS = ['root']CONTACT = 'CDXXJCPT@gome.cn'COMMUNITY = Truetry: from frame.local_config import *except Exception, e: print "[warning] %s" % e 部署agent监控采集模块12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#配置cp cfg.example.json cfg.jsonvim cfg.json&#123; "debug": true, "hostname": "", "ip": "", "plugin": &#123; "enabled": false, "dir": "./plugin", "git": "https://github.com/open-falcon/plugin.git", "logs": "./logs" &#125;, "heartbeat": &#123; "enabled": true, "addr": "10.128.31.137:6030", "interval": 60, "timeout": 1000 &#125;, "transfer": &#123; "enabled": true, "addrs": [ "10.128.31.136:8433", "10.128.31.137:8433" ], "interval": 60, "timeout": 1000 &#125;, "http": &#123; "enabled": true, "listen": ":1988", "backdoor": false &#125;, "collector": &#123; "ifacePrefix": ["eth", "ens"] &#125;, "ignore": &#123; "cpu.busy": true, "df.bytes.free": true, "df.bytes.total": true, "df.bytes.used": true, "df.bytes.used.percent": true, "df.inodes.total": true, "df.inodes.free": true, "df.inodes.used": true, "df.inodes.used.percent": true, "mem.memtotal": true, "mem.memused": true, "mem.memused.percent": true, "mem.memfree": true, "mem.swaptotal": true, "mem.swapused": true, "mem.swapfree": true &#125;&#125; 用户接口 描述 访问接口 功能 备注 dashborad http://10.128.31.136:8081/ 监控主机数据查询 uic http://10.128.31.136:1234/ 用户组管理 portal http://10.128.31.136:5050/ 监控策略配置 dashborad http://10.128.31.137:8081/ 监控主机数据查询 uic http://10.128.31.137:1234/ 用户组管理 portal http://10.128.31.137:5050/ 监控策略配置 alarm http://10.128.31.136:9912/ 报警查询]]></content>
      <categories>
        <category>open-falcon</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>监控</tag>
        <tag>linux</tag>
        <tag>分布式</tag>
        <tag>open-falcon</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网监控, Open-falcon用户配置]]></title>
    <url>%2Fopen-falcon%2Fopen-falcon-deploy-20160806.html</url>
    <content type="text"><![CDATA[Open-Falcon 是小米运维部开源的一款互联网企业级监控系统解决方案。监控系统是整个运维环节，乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供翔实的数据用于追查定位问题。监控系统作为一个成熟的运维产品，业界有很多开源的实现可供选择。本文档对多节点负载open-falcon部署配置说明。请阅读官方参考文档 机器部署系统：Centos7 主机名 主机IP 备注 falconpoc01 10.128.31.136 open-falcon模块测试机 falconpoc02 10.128.31.137 open-falcon模块测试机 falconpoc03 10.128.31.138 open-falcon数据测试机 基于&lt;&lt;多节点Open-falcon部署&gt;&gt;文章的环境进行部署 Nginx安装12345yum install epel-releaseyum install nginxsystemctl enable nginxsystemctl start nginx Nginx配置1234567891011121314151617181920212223242526272829303132333435vim /etc/nginx/nginx.conf# For more information on configuration, see:# * Official English Documentation: http://nginx.org/en/docs/# * Official Russian Documentation: http://nginx.org/ru/docs/user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970vim /etc/nginx/conf.d/falcon.confupstream dashboard &#123; ip_hash; server 10.128.31.136:8081; server 10.128.31.137:8081;&#125;upstream uic &#123; ip_hash; server 10.128.31.136:1234; server 10.128.31.137:1234;&#125;upstream portal &#123; ip_hash; server 10.128.31.136:5050; server 10.128.31.137:5050;&#125;upstream alarm &#123; server 10.128.31.136:9912;&#125;server &#123; listen 8081; server_name 10.128.31.138; location / &#123; proxy_pass http://dashboard; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; server &#123; listen 1234; server_name 10.128.31.138; location / &#123; proxy_pass http://uic; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; server &#123; listen 5050; server_name 10.128.31.138; location / &#123; proxy_pass http://portal; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; server &#123; listen 9912; server_name 10.128.31.138; location / &#123; proxy_pass http://alarm; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; Open-falcon用户接口配置uic用户管理模块123456789101112131415161718192021222324252627282930313233343536373839404142#配置vim cfg.json&#123; "log": "debug", "company": "MI", "http": &#123; "enabled": true, "listen": "0.0.0.0:1234" &#125;, "cache": &#123; "enabled": true, "redis": "falcon:6379", "idle": 10, "max": 1000, "timeout": &#123; "conn": 10000, "read": 5000, "write": 5000 &#125; &#125;, "salt": "", "canRegister": true, "ldap": &#123; "enabled": false, "addr": "ldap.example.com:389", "baseDN": "dc=example,dc=com", "bindDN": "cn=mananger,dc=example,dc=com", "bindPasswd": "12345678", "userField": "uid", "attributes": ["sn","mail","telephoneNumber"] &#125;, "uic": &#123; "addr": "root:password@tcp(falcon-mysql:3306)/uic?charset=utf8&amp;loc=Asia%2FChongqing", "idle": 10, "max": 100 &#125;, "shortcut": &#123; "falconPortal": "http://10.128.31.138:5050/", "falconDashboard": "http://10.128.31.138:8081/", "falconAlarm": "http://10.128.31.138:9912/" &#125;&#125; query绘图查询模块12345678910111213141516171819202122232425#配置vim cfg.json&#123; "debug": "false", "http": &#123; "enabled": true, "listen": "0.0.0.0:9966" &#125;, "graph": &#123; "connTimeout": 1000, "callTimeout": 5000, "maxConns": 32, "maxIdle": 32, "replicas": 500, "cluster": &#123; "graph-00": "falcon-graph:6070", "graph-01": "falcon-graph2:6070" &#125; &#125;, "api": &#123; "query": "http://10.128.31.137:9966", "dashboard": "http://10.128.31.138:8081", "max": 500 &#125;&#125; alarm报警处理模块123456789101112131415161718192021222324252627282930313233343536#配置vim cfg.json&#123; "debug": true, "uicToken": "", "http": &#123; "enabled": true, "listen": "0.0.0.0:9912" &#125;, "queue": &#123; "sms": "/sms", "mail": "/mail" &#125;, "redis": &#123; "addr": "falcon-redis:6379", "maxIdle": 5, "highQueues": [ "event:p0", "event:p1", "event:p2", "event:p3", "event:p4", "event:p5" ], "lowQueues": [ "event:p6" ], "userSmsQueue": "/queue/user/sms", "userMailQueue": "/queue/user/mail" &#125;, "api": &#123; "portal": "http://10.128.31.138:5050", "uic": "http://10.128.31.138:1234", "links": "http://10.128.31.138:5090" &#125;&#125; 用户接口 描述 访问接口 功能 备注 dashborad http://10.128.31.138:8081/ 监控主机数据查询 uic http://10.128.31.138:1234/ 用户组管理 portal http://10.128.31.138:5050/ 监控策略配置 alarm http://10.128.31.138:9912/ 报警查询]]></content>
      <categories>
        <category>open-falcon</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>监控</tag>
        <tag>linux</tag>
        <tag>分布式</tag>
        <tag>open-falcon</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 Min系统 网络设备名问题]]></title>
    <url>%2Flinux%2Fcentos7-min-net-problem-20150429.html</url>
    <content type="text"><![CDATA[下载安装min Centos7到virtual box, 启动linux后网络命令是不可用。需要系统启动选项加载网络参数。其次ifconfig常见命令也无法找到，需要手动安装net-tools。 加载网络 修改grub启动参数vim /etc/sysconfig/grub 添加net.ifnames=0 biosdevname=0, 效果如下 1234567GRUB_TIMEOUT=5GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT="console"GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0 biosdevname=0"GRUB_DISABLE_RECOVERY="true" 加载 grub2-mkconfig -o /boot/grub2/grub.cfg 修改网卡名如:网卡名en03修改成eth0 修改网络文件名mv /etc/sysconfig/network-scripts/ifcfg-en03 /etc/sysconfig/network-scripts/ifcfg-eth0 修改网络设备名vim /etc/sysconfig/network-scripts/ifcfg-eth0 1name=en03修改为name=eth0 重启systemctl reboot now 换源使用国内aliyun源，加快下载速度。 123mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum makecache 安装网络工具yum install net-tools]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>linux</tag>
        <tag>虚拟机</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker初体验 应用容器引擎]]></title>
    <url>%2Fdocker%2Fdocker-practice-20150429.html</url>
    <content type="text"><![CDATA[Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。本篇文章是实践中使用docker，遇到到的问题并记录下。 首先推荐docker的学习资料 Docker —— 从入门到实践 Docker安装系统Ubuntu 14.04 内核3.13.0-65-generic不多说直接给出安装命令，如果想源码编译的请上官网下载编译。 123apt-get updatewget -qO- https://get.docker.com/ | sh Ok, 安装完成。就是这么简单。检测安装的docker版本。 1docker --version 常用命令docker安装成功，了解下常用命令。 123456789101112#启动服务/bin/systemctl restart docker.service#登录docker login registry.rd.fyec.cn#创建docker build -t registry.rd.fyec.cn/redis-server .#运行docker run -t -i registry.rd.fyec.cn/centos:centos7 /bin/bash#提交docker push registry.rd.fyec.cn/virtual_ccb_server#SSHdocker exec -i -t 8ec02d6b7234 bash Dockerfile如何创建自己的docker镜像，当然是写dockerfile了。现在就来写个简单的dockerfile吧。service是一个linux系统下的可运行文件 1234567891011#基础镜像库from registry.rd.fyec.cn/base:latest#将service文件添加到docker镜像的/opt目录ADD service /opt/service#运行时暴露80端口EXPOSE 80#启动后运行/opt/serviceCMD /opt/service 简单的Dockerfile写好了，当然是创建docker image了。1docker build -t &lt;docker image name&gt; &lt;dockerfile path&gt; 最后，当然是吧docker镜像启动起来 12#启动并bashdocker run -t -i &lt;docker image name&gt; /bin/bash Docker SwarmSwarm是Docker公司在2014年12月初发布的一套较为简单的工具，用来管理Docker集群，它将一群Docker宿主机变成一个单一的，虚拟的主机。Swarm使用标准的Docker API接口作为其前端访问入口，换言之，各种形式的Docker Client(docker client in go, docker_py, docker等)均可以直接与Swarm通信。Swarm几乎全部用Go语言来完成开发，上周五，4月17号，Swarm0.2发布，相比0.1版本，0.2版本增加了一个新的策略来调度集群中的容器，使得在可用的节点上传播它们，以及支持更多的Docker命令以及集群驱动。 Swarm deamon只是一个调度器（Scheduler）加路由器(router)，Swarm自己不运行容器，它只是接受docker客户端发送过来的请求，调度适合的节点来运行容器，这意味着，即使Swarm由于某些原因挂掉了，集群中的节点也会照常运行，当Swarm重新恢复运行之后，它会收集重建集群信息。 安装Docker-swarm有三台服务器, 这三台机器创建一个Docker集群 其中 i-238242qix (10.253.101.25) 同时充当swarm manager管理集群 机器名 Ip 描述 i-238242qix 10.253.101.25 swarm manager i-239z31k69 10.253.100.20 &nbsp; i-238etvs5t 10.253.100.229 &nbsp; Docker deamon 的监听端口 vim /etc/default/docker 在文件后面添加1DOCKER_OPTS="-H 0.0.0.0:2375 -H unix:///var/run/docker.sock --graph /mnt/datadisk/docker --storage-driver btrfs" --graph指定docker使用磁盘 --storage-driver指定文件系统 如果是使用consul配置Docker1DOCKER_OPTS="-H 0.0.0.0:2375 -H unix:///var/run/docker.sock -D --cluster-advertise 10.139.52.27:2375 --cluster-store consul://10.139.52.27:8500/swarm --graph /mnt/data/docker --storage-driver btrfs" Docker命令重启 Docker deamon1service docker restart 安装Docker官方提供的Swarm镜像1docker pull swarm 运行swarm manager1docker run -d -p 2376:2376 swarm manage -H :2376 --replication --advertise 10.168.10.198:2376 consul://10.168.10.198:8500/swarm 加入 swarm1docker run -d swarm join --advertise=10.168.10.198:2375 consul://10.168.10.198:8500/swarm 查看集群信息1docker -H 0.0.0.0:2376 info 其他共享主机目录1docker run -d -v ~/nginxlogs:/var/log/nginx -p 5000:80 -i nginx 持久化1234docker create -v /tmp –name cmbdbf dev/file_agent docker run -t -i –volumes-from cmbdbf dev/file_agent /bin/bash echo “I’m not going anywhere” &gt; /tmp/hiexit 删除noneimage1docker rmi $(docker images | grep "^&lt;none&gt;" | awk "&#123;print $3&#125;") 这里只是列出常用命令，之后会将docker网络问题与docker-compose管理以例子列出 文章来源]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
        <tag>分布式</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[尝鲜, ubuntu 安装Docker]]></title>
    <url>%2Fdocker%2Fubuntu-docker-install-20150410.html</url>
    <content type="text"><![CDATA[尝鲜、尝鲜、尝鲜!!! 开始安装Docker吧，多余的话不说了。 安装要求 ubuntu 64 bit系统 系统内核3.10及以上 命令uname -r 安装步骤系统包更新 12sudo apt-get update$ sudo apt-get install apt-transport-https ca-certificates 增加GPG key 1sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 增加下载源 vim /etc/apt/sources.list.d/docker.list 添加以下内容： On Ubuntu Precise 12.04 (LTS) 1deb https://apt.dockerproject.org/repo ubuntu-precise main On Ubuntu Trusty 14.04 (LTS) 1deb https://apt.dockerproject.org/repo ubuntu-trusty main On Ubuntu Wily 15.10 1deb https://apt.dockerproject.org/repo ubuntu-wily main On Ubuntu Xenial 16.04 (LTS) 1deb https://apt.dockerproject.org/repo ubuntu-xenial main 系统包更新 1sudo apt-get update 删除旧Docker检查系统是否已经安装docker并移除老版本 1sudo apt-get purge lxc-docker 验证安装Docker 1apt-cache policy docker-engine 安装依赖包 12sudo apt-get updatesudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual 安装Docker 1sudo apt-get install docker-engine 启动Docker 1sudo service docker start 验证Docker安装下载并启动hello-world的docker镜像，终端打印hello world 1sudo docker run hello-world]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
        <tag>容器</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang学习笔记, 基础语法篇]]></title>
    <url>%2FGolang%2Fgolang-learning-note-20141210.html</url>
    <content type="text"><![CDATA[Go 是一个开源的编程语言，它能让构造简单、可靠且高效的软件变得容易。 Go是从2007年末由Robert Griesemer, Rob Pike, Ken Thompson主持开发，后来还加入了Ian Lance Taylor, Russ Cox等人，并最终于2009年11月开源，在2012年早些时候发布了Go 1稳定版本。现在Go的开发已经是完全开放的，并且拥有一个活跃的社区。 源码笔记源码github-Jerrylou 推荐书籍 https://github.com/gunsluo/Learning-Go-zh-cn https://www.gitbook.com/book/bingohuang/effective-go-zh-en/details 优势 并行Go 让函数很容易成为非常轻量的线程。这些线程在Go 中被叫做goroutines； Channel这些goroutines 之间的通讯由channel完成； 低依赖可直接编译成机器码，不依赖其他库，glibc的版本有一定要求，部署就是扔一个文件上去就完成 安全当转换一个类型到另一个类型的时候需要显式的转换并遵循严格的规则。Go 有垃圾收集，在Go 中无须free()，语言会处理这一切； 标准格式化Go 程序可以被格式化为程序员希望的（几乎）任何形式，但是官方格式是存在的。标准也非常简单：gofmt 的输出就是官方认可的格式； 编码统一任何地方都是UTF-8 的，包括字符串以及程序代码。 开源Go 的许可证是完全开源的，参阅Go 发布的源码中的LICENSE 文件； 静态语言编译很快，执行也很快。编译时间用秒计算；动态语言，缺少编译过程，低级错误频出;高效率（相对于其它静态语言C/C++的问题） 开发效率低，对开发者要求高 libc只向后兼容，运维难度偏大 跨平台windows linux osx 语法简单 缺点 Go的import包不支持版本，有时候升级容易导致项目不可运行，所以需要自己控制相应的版本信息 Go的goroutine一旦启动之后，不同的goroutine之间切换不是受程序控制，runtime调度的时候，需要严谨的逻辑，不然goroutine休眠 无异常处理机制 性能 https://benchmarksgame.alioth.debian.org/u64q/go.html http://mrcook.uk/golang-vs-java-performance 规范 gofmt大部分的格式问题可以通过gofmt解决，gofmt自动格式化代码，保证所有的go代码一致的格式。正常情况下，采用Sublime编写go代码时，插件GoSublilme已经调用gofmt对代码实现了格式化。命名应该采用有意义的字符，尽量做到见名识意。 包名包名统一采用小写单词，不得使用下划线或者混合大小写。 接口名接口名统一以大写字母 “I” 开头，后续采用驼峰结构。官方 - 单个函数的接口名以”er”作为后缀，如Reader,Writer接口的实现则去掉“er” 结构体名结构体名统一采用驼峰命名结构，不得出现下划线，结合是否可导出确定首字母大小写。 常量常量统一采用大写字母，单词之间使用下划线进行分隔。如果是包可见的常量，可在其名字前加上 “k_” 作为前缀。可导出常量与不可导出常量应该分开声明，不得出现在同一常量声明块内。 变量 全局变量采用驼峰结构命名，结合是否可导出确定首字母大小写，不得出现下划线。可导出变量与不可导出变量应该分开声明，不得出现在同一变量声明块内。 形参采用驼峰结构命名，首字母必须小写，不得出现下划线。 局部变量采用驼峰结构命名，首字母必须小写，不得出现下线线。 函数（方法 ）采用驼峰结构命名，结合是否可导出确定首字母大小写，不得出现下划线；返回值必须命名，采用驼峰结构命名，首字母必须小写，不得出现下划线；方法的接收者统一命名 this，接收者类型统一采用指针，特殊情况除外；如果接收者是 map, slice 或者 chan，则不要用指针传递； Goroutinegolang容许你创建成千上万的goroutine，和原生系统级线程不同，goroutine的调度并不是由系统内核来完成，而是golang自己的sched调度系统来完成。golang的sched调度系统采用比较著名的work-steel算法，大家都知道该算法里最核心的一个数据结构就是一个任务队列，如何保证高并发下该队列的正确性是该算法的重点，比较熟悉java的同学应该知道，大师doug lea威廉叔叔的fork-join并发框架采用一个64位 volatile long字段来保证队列的高并发不加锁的实现 http://morsmachine.dk/go-scheduler 框架 beego gin echo martini revel Ide sublime text 3 Visual Studio Code idea vim]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>linux</tag>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git原理介绍，优雅的使用Git]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fgit-principle-20140829.html</url>
    <content type="text"><![CDATA[git是软件开发版本控制系统，类似linux文件系统的实现(当然比linux文件系统简单)。 Git对象Git是如何将文件进行存储？如何将相同文件不同提交进行分割?都依赖与Git对象 repository(Tree) Tree对象(多commit) Commit对象(Blob) Blob对象(存储) 举例说明初始化Git123mkdir gittestcd gittestgit init 查看目录结构tree .git1234567891011121314151617181920212223.git├── branches├── config├── description├── HEAD├── hooks│ ├── applypatch-msg.sample│ ├── commit-msg.sample│ ├── post-update.sample│ ├── pre-applypatch.sample│ ├── pre-commit.sample│ ├── prepare-commit-msg.sample│ ├── pre-push.sample│ ├── pre-rebase.sample│ └── update.sample├── info│ └── exclude├── objects│ ├── info│ └── pack└── refs ├── heads └── tags branches - 新版本不在使用 config - Git项目特有的配置 description - GitWeb 程序使用 HEAD - 文件指向当前分支 hooks - 客户端或服务端钩子脚本 info - .gitignore 文件中管理的忽略模式 (ignored patterns) 的全局可执行文件 objects - Git对象存储目录 refs - 目录存储指向数据 (分支) 的提交对象的指针 添加文件到Git 添加： echo &#39;version 1&#39; &gt; test.txt git add test.txt 查看： find .git/objects -type f 结果如下： 1.git/objects/83/baae61804e65cc73a7201a7252750c76066a30 查看对象类型： git cat-file -t 83baae61804e65cc73a7201a7252750c76066a30 结果如下： 1blob 查看对象内容： git cat-file -p 83baae61804e65cc73a7201a7252750c76066a30 结果如下： 1version 1 提交文件到Git 提交： git commit -a -m &quot;firt commit&quot; 查看： find .git/objects -type f 结果如下： 123.git/objects/83/baae61804e65cc73a7201a7252750c76066a30.git/objects/d8/329fc1cc938780ffdd9f94e0d364e0ea74f579.git/objects/6d/c9727a5974bb610f81c22914081d7e6373ae77 objects目录新生成了两个对象，这两个对象类型内容是什么呢？我们来看下 查看类型：git cat-file -t d8329fc1cc938780ffdd9f94e0d364e0ea74f579 1tree 查看内容：git cat-file -p d8329fc1cc938780ffdd9f94e0d364e0ea74f579 1100644 blob 83baae61804e65cc73a7201a7252750c76066a30 test.txt 查看类型：git cat-file -t 6dc9727a5974bb610f81c22914081d7e6373ae77 1commit 查看内容：git cat-file -p 6dc9727a5974bb610f81c22914081d7e6373ae77 12345tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579author jerrylou &lt;gunsluo@gmail.com&gt; 1473652081 +0800committer jerrylou &lt;gunsluo@gmail.com&gt; 1473652081 +0800firt commit 综上git存储都是通过blob，commit，tree对象。blob存储文件提交内容，commit存储操作提交信息，tree存储指向blob的指针。 结构Git repository是很多不同commit的集合，是有向无环图。如下: 123A---B---C---D---E---F--- master \ / \ G------H I---J--- feature merge和rebase新的特性分支feature上有F,G两个commit。我们项将feature上的改动同步到master，可以feature分支merge到master。将得到如下: 123A---B---C---D--- master \ / F------G----- feature 我们会发现D节点有两个父节点C和G，经常做merge操作会导致无法得到正确的修改历史。 使用rebase，得到如下: 123A---B---C---D------ master \ F---G---feature 不用担心修改历史 命令 git clone - 下载git源码 git pull –rebase origin master - 同步远程master分支到本地 git push origin master - 提交当前分支到远程master分支 git stash [pop] - 换成修改commit git reset –hard {commit} - 恢复到commit提交点 git reflog - 查看修改log,可用于恢复数据 git commit –amend - 撤销最后一次提交 git checkout {文件名} - 撤销对文件的修改 解决冲突当做了新的功能准备提交代码时，第一件事同步要合入分支代码(如dev) git pull –rebase origin dev 提示冲突先修改冲突文件保证代码正确 git add 冲突文件 git rebase –contunie 如还有冲突，跳到第二步，直到无冲突 长时间不rebase主分支容易发生冲突。 我的.gitconfig配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[alias] st = status -sb br = branch -vv ds = diff --staged standup = log --since '1 day ago' --oneline --author sebastian@kusnier.net lastweek = log --since '1 week ago' --oneline ci = commit amend = commit --amend -C HEAD undo = reset --soft HEAD^ co = checkout df = diff dc = diff --cached lg = log -p lol = log --graph --decorate --pretty=oneline --abbrev-commit lola = log --graph --decorate --pretty=oneline --abbrev-commit --all l = log --pretty=oneline -n 20 --graph graph = log --graph --pretty=format':%C(yellow)%h%Cblue%d%Creset %s %C(white) %an, %ar%Creset' ls = ls-files g = grep -I vd = difftool -y -t gvimdiff p = !"git pull; git submodule foreach git pull origin master" undopush = push -f origin HEAD^:master # Credit an author on the latest commit credit = "!f() &#123; git commit --amend --author \"$1 &lt;$2&gt;\" -C HEAD; &#125;; f" # Show files ignored by git: ign = ls-files -o -i --exclude-standard[user] name = jerrylou email = gunsluo@gmail.com[core] editor = /usr/local/bin/vim excludesfile = /Users/jerrylou/.gitignore_global pager = cat autocrlf = input[difftool "Kaleidoscope"] cmd = ksdiff --partial-changeset --relative-path \"$MERGED\" -- \"$LOCAL\" \"$REMOTE\"[diff] tool = Kaleidoscope[difftool] prompt = false[mergetool "Kaleidoscope"] cmd = ksdiff --merge --output \"$MERGED\" --base \"$BASE\" -- \"$LOCAL\" --snapshot \"$REMOTE\" --snapshot trustExitCode = true[mergetool] prompt = false keepBackup = true[merge] tool = Kaleidoscope log = true summary = true[difftool "sourcetree"] cmd = opendiff \"$LOCAL\" \"$REMOTE\" path = [mergetool "sourcetree"] cmd = /Applications/SourceTree.app/Contents/Resources/opendiff-w.sh \"$LOCAL\" \"$REMOTE\" -ancestor \"$BASE\" -merge \"$MERGED\" trustExitCode = true 参考 Git内部原理]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 安装Gitlab CE]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2Fgitlab-ce-install-20140629.html</url>
    <content type="text"><![CDATA[买了阿里云服务器一段时间了，一直想装自己的代码服务器。事先了解gitlab，它类似类似github。官网上查看了安装流程，决定动手搭建自己的代码服务器。 安装依赖12345678sudo yum install curl policycoreutils openssh-server openssh-clientssudo systemctl enable sshdsudo systemctl start sshdsudo yum install postfixsudo systemctl enable postfixsudo systemctl start postfixsudo firewall-cmd --permanent --add-service=httpsudo systemctl reload firewalld 测试发送邮件1echo "Test mail from postfix" | mail -s "Test Postfix" xxx@xxx.com 添加安装源1curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 更新为国内源新建 /etc/yum.repos.d/gitlab-ce.repo，内容为 1234567[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key 1sudo yum makecache 安装1sudo yum install gitlab-ce 修改配置vim /etc/gitlab/gitlab.rb 123gitlab_rails['gitlab_email_from'] = 'gitlab@example.com'external_url 'http://10.128.31.109' 编译启动1sudo gitlab-ctl reconfigure 问题 在浏览器中访问GitLab出现502错误。原因：内存不足。解决办法：检查系统的虚拟内存是否随机启动了，如果系统无虚拟内存，则增加虚拟内存，再重新启动系统。 8080端口冲突。原因：由于unicorn默认使用的是8080端口。解决办法：打开/etc/gitlab/gitlab.rb,打开# unicorn[‘port’] = 8080 的注释，将8080修改为9090，保存后运行sudo gitlab-ctl reconfigure即可。 GitLab头像无法正常显示原因：gravatar被墙解决办法：编辑 /etc/gitlab/gitlab.rb，将 1#gitlab_rails['gravatar_plain_url'] = 'http://gravatar.duoshuo.com/avatar/%&#123;hash&#125;?s=%&#123;size&#125;&amp;d=identicon' 修改为： 1gitlab_rails['gravatar_plain_url'] = 'http://gravatar.duoshuo.com/avatar/%&#123;hash&#125;?s=%&#123;size&#125;&amp;d=identicon' 然后在命令行执行： 12sudo gitlab-ctl reconfigure sudo gitlab-rake cache:clear RAILS_ENV=production]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[玩转魔方，玩自己]]></title>
    <url>%2F%E5%85%B4%E8%B6%A3%2Frubik-cube-formulas-20140429.html</url>
    <content type="text"><![CDATA[看了菲神三阶魔方的出神入化，自己也开始蠢蠢欲动。闲来无事，了解下玩魔方入门方法，也不知道是玩魔方，还是玩自己。只以此记录下，以便以后查看。 ##魔方公式 @(兴趣) 第一步白色中心为底面，黄色中心为顶面 白 &nbsp; 白 黄 白 白 &nbsp; 顶面四边中块为白色，使用公式 1公式 FU`R 第二步 底面归位 顶层白色角块，观察另外两块颜色，与F和R面相同 公式 1公式 RUR`U` 将白色块放于右下角执行公式，会变成1 第三步 中棱归位 找一顶层一边中块无黄色模块（两面都无），移至与中心颜色相同面，使其在右边，使用公式 123顶面与F面颜色相同 R`U`R`U`R`URUR顶面与F面颜色不同 RURURU`R`U`R`特殊情况两块对换 R`U`R`U`R`URUR 第四步 顶面十字 顶端已有一个一字 已有黄色连成左上直角 只有一个黄色中心块 1公式 FRUR`U`F` 第五步 顶面归位 只有一个黄色角块 置于右下角 有2个两黄角块，转动顶面直到R面前上角有黄色角块 顶端无黄色角块，转动顶面直到F右上角有黄色角块​，执行公式 1公式 R`U2RUR`UR 第六步 顶角归位 找到有2个相同角块一面置于F面 若无则先应用公式变为1 1公式 IU`RD2R`URD2R`2 第七步 顶棱归位 有一面已经正确归位，置于左侧，执行公式 若无则先用公式变为1 12公式 R`2U`MF`U2MFU`R`2反方向公式 R`2U MF`U2MFU R`2]]></content>
      <categories>
        <category>兴趣</category>
      </categories>
      <tags>
        <tag>魔方</tag>
        <tag>层先法</tag>
        <tag>爱好</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Mac Window10 双系统，为发烧而生]]></title>
    <url>%2F%E7%94%B5%E8%84%91%E7%A1%AC%E4%BB%B6%2Fhackintosh-window-install-20130429.html</url>
    <content type="text"><![CDATA[本人使用mac pro 一段时间，深刻感受到mac pro对于开发者是如此的好用 - 爱不释手。同时也体会到pro硬件配置低引起的卡顿（图形渲染、virtual-box多开） 哎！由于经费和个人好奇心，于是… 开始吧 我的需求 装个hackintosh用于工作 最好再装个window10。看看电影、打打游戏, 想想都嗨的不行。 硬件选择osx系统对硬件有要求，在选择CPU、主板、显卡是要特别注意。选择一不小心可能会在之后无法安装osx, 也可能无法驱动显卡，还可能出现莫名其妙的问题。选择硬件之前请一定查是否有人成功安装。其他的就不说，直接给出参考网站: Mac硬件(可能需要翻墙) 我的硬件 硬件类型 硬件型号 ￥价格 CPU 英特尔（Intel）酷睿四核 i5-6500 1480.00 主板 技嘉（GIGABYTE）Z170-D3H主板 1000.00 显卡 EVGA GTX950 2G SC ACX2.0 cooler 1100.00 硬盘 三星(SAMSUNG) 850 EVO 250G SATA3 固态硬盘 580.00 硬盘 希捷(SEAGATE)1TB 7200转64M SATA3 台式机硬盘 330.00 内存 英睿达(Crucial)铂胜运动LT系列DDR4 2400 8G台式机内存 * 2 470.00 机箱 美商海盗船（USCorsair）SPEC-03 黑色红光 中塔机箱 350.00 水冷 美商海盗船（USCorsair）H55 CPU散热器 500.00 注意：硬件之间要相互支持。我选择的主板不支持蓝牙，也没有买无线网卡，osx的airdrop功能无法使用。你可以选择带蓝牙功能的主板并配上带蓝牙功能的无线网卡（单独蓝牙硬件） 组装主机自己看着办，我无能为力。 制作El Capitan UEFI USB安装启动盘 准备16G的USB盘和一台osx系统的电脑(如果只有window，请看后面) 通过 “Mac App Store” 更新下载，请进”Mac App Store下载” Install OS X El Capitan 下载可能有点慢（可能需要翻墙） 通过网盘下载，这里有小编亲手打包的”OS X El Capitan”镜像已上传到百度盘。提供下载 文件名：安装 OS X El Capitan 1.7.28 2015-10-01.dmg 百度盘 ：http://pan.baidu.com/s/1pLIMAD1 提取码：593r 格式话U盘，u盘名字为USB 打开「应用程序」-「实用工具」-「终端」，复制下面的命令，并粘贴到「终端」里，按回车运行： 1sudo /Applications/Install\ OS\ X\ El\ Capitan.app/Contents/Resources/createinstallmedia --volume /Volumes/USB --applicationpath /Applications/Install\ OS\ X\ El\ Capitan.app —nointeraction 下载Cloverefiboot, 右键「show package contents 」。找到Clover_v2.3k_r3423-UEFI-UB.pkg，双击安装UEFI启动到U盘。 windows系统制作osx启动盘。请参考: 教程 修改Bios,UEFI方式启动U盘 设置Bios 支持UEFI 安装el capitan 到固态硬盘 选择install OS X 选择语言 选择你的固体硬盘 格式化硬盘 install OS X OS X 安装好了。 Hackintosh驱动篇Hackintosh的驱动是最为麻烦的，显卡使用相对型号的web driver。驱动的详细安装介绍另起文章说明。 安装Window 10前准备 调整安装OSX固态盘大小，腾出硬盘空间安装Window 10 我的固态硬盘大小250G，170G安装Mac，80G安装Window 10 打开「应用程序」-「实用工具」-「终端」 1diskutil list 调整硬盘大小 1sudo diskutil resizeVolume /dev/disk0s2 170GB 调整后硬盘大小 制作Window 10 UEFI U盘 下载Window 10 iso安装文件，最好选择64bit的安装包。下载地址 格式化U盘，请务必选择「MS-DOS(FAT)」 在「launchpad」中打开「Boot Camp Assistant」 点「continue」继续 选择下载的Window 10.ISO文件，「continue」写入U盘 安装Window10 重启系统，U盘启动进入Window安装 选择没有使用的80G作为Window安装盘 点「install」安装 安装成功 修改Window10引导启动 重启系统，进入Mac系统 打开「应用程序」-「实用工具」-「终端」, 挂载EFI启动盘 1diskutil mount /dev/disk0s1 将Mac的EFI引导程序拷贝到microsoft 备份原有bootmgfw.efi 用mac的BOOTX64.efi替换到bootmgfw.efi 重启系统，选择你想进入的系统吧]]></content>
      <categories>
        <category>电脑硬件</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>hackintosh</tag>
        <tag>黑苹果</tag>
        <tag>安装</tag>
        <tag>UEFI</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>
